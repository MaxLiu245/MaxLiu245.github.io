<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>【论文阅读36】Dissecting NODEs</title>
    <url>/2021/03/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB36%E3%80%91Dissecting-NODEs/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>读了好久，不少地方困住了</p>
<p><em>Dissecting Neural ODEs</em></p>
</blockquote>
<a id="more"></a>
<p>这篇文章大概20年上半年就在<a href="https://arxiv.org/abs/2002.08071" target="_blank" rel="noopener">arXiv</a>上挂出来了，后来才知道中了<a href="https://proceedings.neurips.cc/" target="_blank" rel="noopener">NeurIPS</a>2020，作者日韩，文献地址：<a href="https://proceedings.neurips.cc/paper/2020/hash/293835c2cc75b585649498ee74b395f5-Abstract.html" target="_blank" rel="noopener"><em>Dissecting Neural ODEs</em></a></p>
<p>如题，本文试图解剖NODE，确实从很多角度给出了一个general的增广NODE框架，总结了一系列的相关模型！</p>
<h2 id="内容大纲"><a href="#内容大纲" class="headerlink" title="内容大纲"></a>内容大纲</h2><p>这篇文章的主要目的是分（解）析（剖）一下NODE，给出了一个general的NODE框架，叫<strong>general system-theoretic Neural ODE formulation</strong></p>
<p>文献的背景/idea：</p>
<ul>
<li>最近两年来比较有意思的连续深度学习模型NODE解剖分析的需求，需要简明解释NODE结构</li>
</ul>
<p>整体的框架是$(1)$式</p>
 <img src="/2021/03/01/【论文阅读36】Dissecting-NODEs/DNODEs-1.png" title="general system-theoretic Neural ODE formulation">
<p>进一步分析这个框架是如何general的。具体分了三大块：</p>
<ul>
<li>Depth-Variance<ul>
<li>基本的idea是NODE不能作为无限近似的连续模型，为什么？当结论看着吧，不过确实参数化更rich</li>
</ul>
</li>
<li>增广策略<ul>
<li>基于ANODE的增广策略，可以退化为ANODE</li>
<li>在框架下，分为：<ul>
<li>对输入层增广</li>
<li>高阶信息增广</li>
</ul>
</li>
</ul>
</li>
<li>增广之外<ul>
<li>动机是对$\varphi(x)=-x$或者同心环问题，不一定需要增广，由此提出了2种模型：<ul>
<li>data control</li>
<li>adaptive depth</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="基于NODE的连续系统"><a href="#基于NODE的连续系统" class="headerlink" title="基于NODE的连续系统"></a>基于NODE的连续系统</h2><p>标题即文中<strong>Continuous–Depth Models</strong>部分，这算是框架的一个背景模型，其实就是回顾NODE。只不过这里的说法指的是general形式的NODE，即添加了输入层$h_x$和输出层$h_y$</p>
<p>主要回顾了general的连续ODE系统、适定性即解的存在唯一性、训练方法Adjoint</p>
<h2 id="Depth-Variance"><a href="#Depth-Variance" class="headerlink" title="Depth-Variance"></a>Depth-Variance</h2><p>Depth-Variance指的是啥没读懂，depth和variance应该分别指每个深度（层）和其参数不同，后者应该不是方差的意思</p>
<p>它的动机是最原始的NODE，虽然说是连续系统模拟，但是即使网络足够深<strong>也不能说</strong>是ResNet的极限，注意这个说法。一个简单的理解是ResNet的每一层的残差模块都应该有自己的参数，记为$\theta_s$，$s$在这里特指为层数，或者说<strong>depth</strong>。这样的话，general的深度ResNet应该至少是$\dot{z}=f_{\theta_s}(s,z(s))$的形式，包括了所有层的变化规律</p>
<blockquote>
<p>注：文中表示试图达到ResNet的deep极限的最初尝试就是hypernetwork，日后请浏览！</p>
</blockquote>
<p>本文表示直接参数化的做法，在理论上存在弱点。本文藉此考虑<strong>泛函空间中的梯度下降</strong>，直接视$\theta_s$函数存在于一（较）般（大）的函数空间中，这样的空间又往往是无穷维的，所以函数空间中的梯度下降就要算<strong>G导数</strong>，然后拓展使用Adjoint方法的过程中发现无穷维的时候不会算这个导数。</p>
<p>因此需要进行有限维空间中的近似，具体的做法给了两种，一种是采用有限多个正交基展开$\theta_s$；另一种是暴力地让$\theta_s$分多段常值。这两种方法分别叫<em>Spectral discretization: Galërkin Neural ODEs</em>和<em>Depth discretization: Stacked Neural ODEs</em>。它们从不同的角度参数化了网络模型的参数，增强了表达的能力，对，应该是这样</p>
<h2 id="增广策略"><a href="#增广策略" class="headerlink" title="增广策略"></a>增广策略</h2><p>本文对NODE相关的增广研究都基于ANODE，要强调的一点是ANODE只是单纯增广了维度，这种增广称为<strong>0-增广</strong>。如果细化一点增广的是什么，可能能得到general的增广策略</p>
<h3 id="输入层增广"><a href="#输入层增广" class="headerlink" title="输入层增广"></a>输入层增广</h3><p>注意general框架中的$z(0)=h_x(x)$，这就是输入层增广，称为<strong>IL-NODE</strong>，IL表示输入层</p>
<p>0-增广看成把输入$x$增广为$[x,0]$，然后转化为隐变量（顺序可看成反过来），其中$x$和$0$都是向量；那么输入层增广就是把输入$x$增广为$h_x(x)$，$h_x$取普通网络即可，再在框架中和$s,x$合并为正式输入</p>
<p>这样的话0-增广就是IL-NODE的一个特例，即$h_x(x)\overset{def}{=}[I,0]x$，维度省略，意思是明显的</p>
<p>进一步，我在增广策略开头提到<code>细化一点增广的是什么</code>，举个例子，如果让$x$的前多少维度反映重要信息，比如$x$本身信息，后面的维度增广为高阶信息，那增广就能引入微分方程中的高阶导数信息</p>
<h3 id="高阶信息增广"><a href="#高阶信息增广" class="headerlink" title="高阶信息增广"></a>高阶信息增广</h3><p>高阶信息增广指的是<strong>Higher–order</strong> Neural ODEs，这样增广的<strong>动机</strong>是提高参数效率</p>
<p>本来只有隐变量$z_1=z_q$，用网络从$z_1$生成$z_2=z_p$，作为$\dot{z}_1$，然后加一个二阶信息（方程）$\dot{z}_2=\ddot{z}_1$，即文章中的$(6)$式，$(7)$式是高阶信息增广合并到一起的写法：</p>
<script type="math/tex; mode=display">\displaystyle \left\{ \begin{aligned} \dot{z}_1 &= z_2(s) \\ \dot{z}_2 &= f_{\theta (s)}(s,z_1(s),z_2(s)) \end{aligned} \right.,\ where\ z(s)=[z_1(s),z_2(s)]. \tag{6}</script><script type="math/tex; mode=display">\displaystyle \dfrac{d^nz^1}{ds^n}=f_{\theta (s)}\left(s,z,\dfrac{dz^1}{ds},\cdots,\dfrac{d^{n-1}z^1}{ds^{n-1}} \right),\ where\ z=[z^1,z^2,\cdots,z^n],\ z^i\in\mathbb{R}^{\frac{n_z}{n}}. \tag{7}</script><p>为什么这样合并的形式提高了参数效率呢？此时注意$(7)$式中关键的函数$f_{\theta (s)}$的输出只需要是原来的$\frac{1}{n}$即可，这就是原因了</p>
<p>进一步推广不太好弄，一直加高阶信息的话需要增加的维度也很多，而且意义也不大了。一个做法是选择出重要的维度来存储信息</p>
<h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h3><p>文中给出了一些实验，表明了这些增广模型都是有效的，一定程度上提高了计算效率</p>
<p>用了两个<strong>评价指标</strong>，分别是NFE（number of function evaluations），越小越好；和参数量</p>
<h2 id="增广之外"><a href="#增广之外" class="headerlink" title="增广之外"></a>增广之外</h2><p>这一部分看似有点奇怪，但动机良好，对$\varphi(x)=-x$（称为<strong>reflection</strong>）或者同心环问题，不一定需要增广策略，由此提出了2种模型，<strong>data control</strong> NODE和<strong>adaptive depth</strong> NODE</p>
<p>对于data control，文章先举了一个例子，对于reflection，控制$z(0)=x$，然后去学习$z(1)$，让$z(1)$随初值的变化而变化，这样学出了$-x$这个函数。这个例子是在框架下的</p>
<p>然后举了一个data control的归一化流的例子，没看懂。。。但是我觉得这个data control的思路是让初值变化，其实就是一种输入层增广，这样对多个初值能学习到很多条流，这些流被data control，学习到目标函数。这个时候不需要在意轨线可能相交的原因应该是<strong>哪里避免了流在同一时刻达到交点</strong>，这个精妙的地方没有读懂。。</p>
<p>第2种模型是adaptive depth，字面意思是控制网络深度。差不多，它控制的是积分的上下限，如文中$(5.2)$节的积分上限$g(w)$，把要积到什么时候用网络来参数化，同样<strong>避免了流在同一时刻达到交点</strong>，它能做到的原因应该是学习了一系列的流，但是各自的积分时长其实都是自适应学习的，时刻也有所变化🤔</p>
<hr>
<p>最后，文章提了一个东西，叫<strong>Mind your input networks</strong>，意思是说对输入层增广之后可能已经得到了一些重要信息，导致后面再积分的过程冗余，所以输入的处理要小心</p>
<h2 id="审稿意见"><a href="#审稿意见" class="headerlink" title="审稿意见"></a>审稿意见</h2><p><a href="https://proceedings.neurips.cc/paper/2020/file/293835c2cc75b585649498ee74b395f5-Review.html" target="_blank" rel="noopener">https://proceedings.neurips.cc/paper/2020/file/293835c2cc75b585649498ee74b395f5-Review.html</a></p>
<p>看得真累，叭看了</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Massaroli, S., Poli, M., Park, J., Yamashita, A., &amp; Asma, H. (2020). Dissecting Neural ODEs. 34th Conference on Neural Information Processing Systems, NeurIPS 2020.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>ODE</tag>
        <tag>Flow Model</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文略读33-35】染色体分割相关</title>
    <url>/2021/02/22/%E3%80%90%E8%AE%BA%E6%96%87%E7%95%A5%E8%AF%BB33-35%E3%80%91%E6%9F%93%E8%89%B2%E4%BD%93%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>读了点染色体分割的小文章，有点平衡不好暴力美学和解释性方法了</p>
</blockquote>
<a id="more"></a>
<h2 id="第一篇"><a href="#第一篇" class="headerlink" title="第一篇"></a>第一篇</h2><p>第一篇文章是专利：<a href="https://www.zhangqiaokeyan.com/patent-detail/06120101534115.html" target="_blank" rel="noopener">一种染色体核型图像切割方法</a></p>
<p>申请公布日19/12/03，公司写的专利</p>
<p>粗看了一遍流程，方法是<strong>基础，合理，工程</strong>的</p>
<p>好多方法都用了，形态学算子、MaskRCNN等等，大概思路就是</p>
<ol>
<li>不知道有没有预处理，好像不需要</li>
<li>连通域提取染色体区域</li>
<li>重叠染色体进一步提取，方式是阈值判断与置信选择</li>
<li>进一步提取骨架，找中心点，骨架其实就是染色体条像素宽度1</li>
<li>后面就是无趣的深度学习工程式套模型，数据增强等等，有多少方法套多少方法</li>
</ol>
<h2 id="第二篇"><a href="#第二篇" class="headerlink" title="第二篇"></a>第二篇</h2><p>第二篇文章<a href="https://ieeexplore.ieee.org/document/7163174" target="_blank" rel="noopener">A Geometric Approach To Fully Automatic Chromosome Segmentation</a></p>
<p>这个文章是2014 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)的</p>
<p>感觉讲得很结实实在，就是讲染色体图像自动分割，方法就是<code>a geometric method for segmentation of the touching and partially overlapping chromosomes</code></p>
<p>基本方法：</p>
<ol>
<li>去噪（预处理）</li>
<li>分离重叠或者touching的染色体（key）</li>
<li>正式分离</li>
</ol>
<p>本文的出发点（idea）是基于几何进行自动分割，主要贡献在于检测分离2中的情形。本文强调几何方法的优点（section I最后有三条优点）是不用考虑图片类型，通用。本文的数据也不多，database containing 62 touching and  partially overlapping chromosomes，要考虑减小过拟合。</p>
<p>上述的2有两大步骤</p>
<ol>
<li>第1是把所有染色体簇识别出来，方式是三种几何方法适当组合，虽然每个方法都有偏颇，但组合之后确实可能更有效分别是计算凸包像素比例、外接椭圆长短轴比、骨架端点数</li>
<li>第2就是把染色体簇分成单个的，方式是找出分割点，有VAMD和SDTP两个准则，都是几何判断，细节不考虑了</li>
<li>ps：如果有多条染色体成簇，就重复算法，每次分出来一条</li>
</ol>
<h2 id="第三篇"><a href="#第三篇" class="headerlink" title="第三篇"></a>第三篇</h2><p>第三篇文章很早之前看过，这次整理一下</p>
<p><a href="https://ieeexplore.ieee.org/document/8014843" target="_blank" rel="noopener">Crowdsourcing for Chromosome Segmentation and Deep Classification</a>是<a href="https://ieeexplore.ieee.org/xpl/conhome/8014302/proceeding" target="_blank" rel="noopener">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</a>的</p>
<p>原来是个workshop，怪不得觉得差味道</p>
<p>这个讲一整套流程，中期（ metaphase ）染色体分析，分割与分类，众包流程。</p>
<p>众包是人工注释边界。。然后伸直矫正+长度归一化。。再网络分类。。</p>
<p>哈哈这个文章也提到了染色体分割的难处，叽里呱啦介绍了一堆别人方法，然后表示我们还是深度吧。。</p>
<p>扫了一遍内容没啥，我觉得就是强调了一遍染色体分类应用落地的流程。整个流程十分工程：</p>
<ol>
<li>众包分割单个染色体图像</li>
<li>众包过程中有细节提高分割效率</li>
<li>单个染色体有诸如剪切、拼接、扭直、归一化长度的操作</li>
<li>普通的网络进行分类</li>
<li>交互式应用界面</li>
</ol>
<p>最后，它的数据集不大，准确率泛化性存疑</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>平衡不好暴力美学和解释性方法了</p>
<p>要是一点点地设计模型，效果是存疑的，但是可解释性会比较优美，所以希望分类的方法能直观，能看出来实际意义，同时保持准确性（安全性）</p>
<p>要是暴力工程处理，实在不美，效果泛化未必就好，这样来看还是处理general图像的方法好，预处理尽可能忽略图像类型的影响</p>
<p>大晚上的迷糊了，ヾ(•ω•`)o</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 宋宁, 池浩塬, 秦玉磊, 韩云鹏, 马伟旗, 沈晓明, 晏青, 吴朝玉, and 杨洁. “一种染色体核型图像切割方法.” 2019.</p>
<p>[2] Minaee, S., M. Fotouhi, and B. H. Khalaj. “A Geometric Approach to Fully Automatic Chromosome Segmentation.” Paper presented at the 2014 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), 13-13 Dec. 2014 2014.</p>
<p>[3] Sharma, M., O. Saha, A. Sriraman, R. Hebbalaguppe, L. Vig, and S. Karande. “Crowdsourcing for Chromosome Segmentation and Deep Classification.” Paper presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 21-26 July 2017 2017.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Medical Images</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读32】NF小综述</title>
    <url>/2021/01/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB32%E3%80%91NF%E5%B0%8F%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>阅读归一化流小综述</p>
<p><em>Normalizing Flows: An Introduction and Review of Current Methods</em></p>
</blockquote>
<a id="more"></a>
 <img src="/2021/01/28/【论文阅读32】NF小综述/lzgnyq.jpg" title="小拳拳出击">
<p>他喵的读文章读了好几天，日常很烦躁。</p>
<p>这篇文章是关于流模型的一个小综述，发在TPAMI(Early Access)上我是想不到的，截至2021/01/28影响因子17.8。去年5月刊出，现在有2次被引</p>
<p><a href="https://ieeexplore.ieee.org/document/9089305" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/9089305</a></p>
<h2 id="NF概述"><a href="#NF概述" class="headerlink" title="NF概述"></a>NF概述</h2><p>总结就是流这个东西属于生成模型建模的一部分，现在的<code>Normalizing流</code>在保证<strong>生成效果</strong>的同时要求<strong>生成过程可逆</strong>，也就是可以来回变。这个变来变去的过程一般包括采样，所以还需保证<strong>采样效率</strong>良好，这三个其实就是目前主要的<strong>模型改进方向</strong></p>
<p> <code>Normalizing流</code>没有正式翻译，我姑且称为归一化流。本来是<strong>从简单初始分布一步步到复杂数据分布的过程</strong>嘛，要求<strong>可逆可微</strong>（后者应该可以退化成连续）。那么<strong>流有两个方向</strong>，简单到复杂就是<strong>生成方向</strong>；反之称为<strong>normalizing方向</strong>，意思是复杂分布统统归一化为简单分布，就是这个意思</p>
<p>它的应用方向似乎不少，但是比较专，和生成模型相关的都可以，数据生成什么的也沾边</p>
<h2 id="NF基础理解"><a href="#NF基础理解" class="headerlink" title="NF基础理解"></a>NF基础理解</h2><p>基础的流理论，主要是概率分布之间的变化，那么自然有Jacobi的引入，后续针对它的结构、行列式计算也有很多改进。另外是基础理论，这个真的觉得难度一下子就上去了，涉及测度论</p>
<p>注意一个区别，微分同胚和双射不同！微分同胚是更强的，双射比较弱，原文是这么说的。自己感觉的话，微分同胚连续可微，双射不一定连续</p>
<p>流理论的两种主要应用，密度估计和变分推断，涉及流的两个方向的使用。正向即生成方向是直接计算复杂分布的概率密度的，因此这个过程要计算Jacobi。所以在密度估计问题中，由于这个密度指的是已知数据条件下，从复杂分布推导前面简单分布的概率密度，如果建模生成方向的话，则要计算其Jacobi的逆，这样复杂了，所以对密度估计一般建模归一化方向可能方便一点。</p>
<p>至于变分推断，就是一般的推断过程，需要重参数化技巧。不过我自己还没有见过有关这方面的流模型文章</p>
<h2 id="NF方法分类"><a href="#NF方法分类" class="headerlink" title="NF方法分类"></a>NF方法分类</h2><p>一图胜千言系列，文章也是按这个顺序介绍的，而且对应的方法相对前一种多有针对性的改进</p>
<img src="/2021/01/28/【论文阅读32】NF小综述/NF-1.png" title="方法分类">
<ul>
<li><p>逐点流，其实是逐维度的流，维度分开。显然表达能力比较欠缺，维度之间没有相关性</p>
</li>
<li><p>线性流，线性指线性变换。在逐点流的基础上维度之间线性组合，即进行线性变换，不妨设形式$Ax+b$。具体又分<strong>$A$对角，上下三角，置换，正交，$A$矩阵分解，$A$取卷积阵</strong>等，这些都算线性流！</p>
<p>其中的<strong>Periodic Convolutions可能和XQ师兄提到的变换基的卷积有所关系</strong>！</p>
<p>线性流的缺点还是表达能力，毕竟是线性（如果直接引入非线性特征变换，线性套非线性变换的话会很奇怪，为什么不直接非线性变换）</p>
</li>
<li><p>平面流&amp;径向流，试图突破线性流，取非线性变换</p>
<ul>
<li>平面流加了方向，见文中$(10)$式$x+uh(w^Tx+b)$，观察知，有一点残差的味道，前面弄出来个$x$好像是有利于Jacobi行列式估计的；另外，后面的$h$非线性，先线性再非线性，前面还有个所谓的方向$u$，这大概是平面流名称的由来</li>
<li>径向流用的是那种中心点的形式，有点像核函数的那种形式，见文中$(14)$式</li>
</ul>
</li>
<li><p>耦合（coupling）流，<strong>有技巧的非线性变换</strong>，<strong>耦合函数也叫conditioner</strong>。一般的操作是维度拆开，一部分id，一部分用耦合函数作用，以前看的NICE就是这个套路，还取了名字叫加性耦合函数。</p>
<ul>
<li>问题一：哪一部分变量取耦合作用，这些可能涉及到顺序的问题。常用的套路是每次非线性变换一部分变量，到后面，<strong>还能id的变量维度是减少的</strong>，因为先前非线性变换的信息已经够多</li>
<li>问题二：回去即归一化的方向，在问题一的假设中，此方向id的维度依次增多，语义信息保留，指的是高阶信息逐渐变成id，且还能<strong>在归一化的过程中保留</strong></li>
</ul>
<p>针对问题一，耦合往往使用分割维度的方式，其顺序可以随机置换，也有mask方式和选择Jacobi对角值更高的维度（信息更多吧）</p>
</li>
</ul>
<p>中间插一段<strong>各种耦合函数</strong>的介绍：</p>
<ol>
<li>直接仿射变换</li>
<li>非线性的二次变换</li>
<li>连续的混合密度</li>
<li>样条（插值？），又包括分段线性，分段二次，三次样条，etc</li>
<li>神经网络建模的自回归耦合方式，应该是依次套层</li>
<li>多项式平方和</li>
<li>分段双射耦合</li>
</ol>
<ul>
<li><p>自回归流，其实就是上述耦合函数<strong>作用对象变化</strong>，依次作用在先前所有的输入上</p>
<p>这样做的一个缺点是逆不好算；且由于是顺序的，难以并行计算</p>
<img src="/2021/01/28/【论文阅读32】NF小综述/NF-2.png" title="自回归流的一般结构">
</li>
<li><p>以残差网络为代表的流，正逆向建模的都有</p>
<ul>
<li><p>正向是自然理解，用了耦合coupling的手段，维度分开，用残差加性处理加性和前面说的还不是太一样，对每次的输出也有耦合作用，见$(30)$式</p>
</li>
<li><p>逆向需要所谓re-arrange，这个记不清了，暂时不太明白</p>
</li>
</ul>
<p>缺点是Jacobi难算，一个方案是采用det估计，由Lipschitz控制把det转换为计算迹，其中还有采样近似估计的trick。估计的方式不少，有偏无偏都有</p>
<p>对于这一类流，一个理解是ODE系统，这类网络可设计为可逆，有文中命题7保证，要求残差block被常数1，Lipschitz控制。但不好控制，一般需要控制网络的结构，由此又衍生出一些模型</p>
</li>
<li><p>连续流，字面上指离散得足够精细，也可依据上面ODE理解，中间状态足够精细就逼近连续系统。但是直接加深ResNet深度太SB了。基于此，这个分两类，ODE-based和SDE-based</p>
<ul>
<li><p>前者即熟知的NODE+adjoint方法</p>
<p>相关改进有陈天琦参与的FFJORD，估计迹，降低计算和存储（连续流）复杂度，还有个思路是正则限制</p>
<p>ODE方法类也有缺点，<strong>一条path上Jacobi行列式不变号</strong>，解决方式是维度增广ANODE，以前读ANODE的时候一直对这个path不交叉有点迷惑，现在又多了一个解释！</p>
<p>我看的DDNF他们也介绍了，路径由许多微分同胚构成</p>
</li>
<li><p>SDE-based没怎么看过（Langevin flows）：大概就是ODE与时间相关了，有随机项，也有固定的偏移项之类的东西</p>
</li>
</ul>
</li>
</ul>
<h2 id="NF常用数据"><a href="#NF常用数据" class="headerlink" title="NF常用数据"></a>NF常用数据</h2><p>实验数据方面比较固定。除了人工数据，参考文中表2、4即可。分别是UCI和常用实际数据</p>
<p>似乎UCI的数据都连续化了de-quantized，方式是对离散数据加均匀噪声，看成连续数据</p>
<h2 id="NF挑战"><a href="#NF挑战" class="headerlink" title="NF挑战"></a>NF挑战</h2><p>这个文章提的挑战不多，感觉很多核心的东西没有明说，我也没看出来太多</p>
<ol>
<li><p>从基础理论开始，自然是测度论太难做了，基测度，即初始分布的选取可以看成一种先验，也就是大家说取简单的好采样的就完事了。有生成的复杂度和效果之间的权衡？</p>
</li>
<li><p>另外，微分同胚的建模也很重要，上一篇看的<a href="https://maxliu245.github.io/2021/01/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB31%E3%80%91DDNF%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%BE%AE%E5%88%86%E5%90%8C%E8%83%9A%E7%9A%84Normalizing-Flow/">DNNF</a>就是</p>
</li>
<li><p>还有一点是损失函数的选取，现在普遍KL，也有其它距离介导的损失</p>
</li>
<li><p>再提一个理论上的东西，从欧氏空间到非欧空间哈哈。黎曼来袭，怎么把欧式转为黎曼。计算上必然复杂。这其中涉及到黎曼几何课上学习的东西，哈哈什么测地完备则测地线可以无限延伸，即空间中处处可以转换，也有切空间近似方式，亦或是考虑特定的黎曼空间，李群结构什么的😄</p>
</li>
<li><p>最后至于不连续流，个人觉得是特定应用场景下再使用就算了，而且一般也就是离散流de-quantized近似为连续流</p>
</li>
</ol>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Kobyzev, I., Prince, S., &amp; Brubaker, M. (2020). Normalizing Flows: An Introduction and Review of Current Methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1-1. <a href="https://doi.org/10.1109/TPAMI.2020.2992934" target="_blank" rel="noopener">https://doi.org/10.1109/TPAMI.2020.2992934</a> </p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Flow Model</tag>
      </tags>
  </entry>
  <entry>
    <title>【Windows技巧3】奇怪的网页广告统统走开</title>
    <url>/2021/01/26/%E3%80%90Windows%E6%8A%80%E5%B7%A73%E3%80%91%E5%A5%87%E6%80%AA%E7%9A%84%E7%BD%91%E9%A1%B5%E5%B9%BF%E5%91%8A%E7%BB%9F%E7%BB%9F%E8%B5%B0%E5%BC%80/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>大家吼！给大家安利去网页广告神器ADBlock！</p>
</blockquote>
<a id="more"></a>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>凡安利必有因，本来打算用学校的vpn下波文献，研究研究《傅雷家书》的版权问题。毕竟傅雷早已过世，为什么傅雷家书还不算公版书呢？</p>
<p>以下文献讲得还可以，我感觉我明白了，《傅雷家书》是傅聪整理的，而且有所创新，其<strong>著作权现在归属傅聪</strong>。get</p>
<blockquote>
<p><a href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFDLAST2017&amp;filename=FBZX201726164&amp;v=xscXH%25mmd2F5%25mmd2BzQVsz6bvAycyaGFAlQY3yZkHtJAdqbO7AuEXwv3Jgdg0xh2Bec06j25V" target="_blank" rel="noopener">浅论公版图书的版权问题——以《傅雷家书》版权案为例</a></p>
<p>许姗. (2017). 浅论公版图书的版权问题——以《傅雷家书》版权案为例. 法制博览(26), 252. </p>
</blockquote>
<h2 id="哈哈哈哪里都有奇怪的广告"><a href="#哈哈哈哪里都有奇怪的广告" class="headerlink" title="哈哈哈哪里都有奇怪的广告"></a>哈哈哈哪里都有奇怪的广告</h2><p>但是这波上了学校vpn之后出现的奇怪的广告，导致我<strong>点击不了文献</strong>，包括大半个网页屏幕区域</p>
<img src="/2021/01/26/【Windows技巧3】奇怪的网页广告统统走开/Webvpn-1.png" title="请问是Webvpn的锅么">
<p>刚开始尝试开发者工具删除element，但是直接找看不出来哪个是这个广告</p>
<p>突然惊醒，👴有<strong>去广告神器ADBlock</strong>嘻嘻🤭</p>
<img src="/2021/01/26/【Windows技巧3】奇怪的网页广告统统走开/Webvpn-2.png" title="懂我意思吧hxd">
<p>🆗</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>最后<strong>怒喷</strong>一波，学校这个服务还带广告的？？？这波是算合理外快还是又什么操作？？？搜文献的时候点击不了很毁人心情的</p>
]]></content>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title>【Hexo4】公式渲染出错更正——渲染位置失效问题</title>
    <url>/2021/01/13/%E3%80%90Hexo4%E3%80%91%E5%85%AC%E5%BC%8F%E6%B8%B2%E6%9F%93%E5%87%BA%E9%94%99%E6%9B%B4%E6%AD%A3%E2%80%94%E2%80%94%E6%B8%B2%E6%9F%93%E4%BD%8D%E7%BD%AE%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>记录一下长久以来困扰我的公式渲染问题</p>
</blockquote>
<a id="more"></a>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>很早就有这个问题了，<code>hexo</code>对公式的渲染莫名其妙会出现问题，自己在本地<code>Typora</code>编辑器中渲染得都非常漂亮，但是一旦发布或者在本地服务器上预览就会出问题，本来的<strong>应该被渲染的公式可能没有被渲染或者被渲染到了别的地方</strong>。这次好好写的笔记又炸了，如下图所示：</p>
<img src="/2021/01/13/【Hexo4】公式渲染出错更正——渲染位置失效问题/error.jpg" title="MathJax Error">
<p>主要的问题在于：</p>
<ol>
<li><p>用来生成$\{\}$的代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\&#123;\&#125;</span><br></pre></td></tr></table></figure>
<p>中花括号的转义字符<code>\</code>会消失导致渲染失败</p>
</li>
<li><p>下标转义字符<code>_</code>会偶尔消失，主要是行内公式</p>
</li>
<li><p>由以上问题导致非公式文本会被渲染</p>
</li>
</ol>
<p>太难看了，这次从网上找了解决方案，聊记于此，原因是<strong>原来的渲染包<code>hexo-renderer-marked</code>不太行</strong>。。。我先自己更新了一下，发现问题没有改观，然后卸载之并安装<code>hexo-renderer-kramed</code>，问题仍未改观。最后按照解决方案修改<code>js</code>规则，总算🆗啦！</p>
<h2 id="试错顺序"><a href="#试错顺序" class="headerlink" title="试错顺序"></a>试错顺序</h2><p>第一步观察是不是工具包未升级的问题：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g npm</span><br></pre></td></tr></table></figure>
<p>第二步观察是不是新的工具包就好了：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<p>第三步按链接找到<code>../node_modules/kramed/lib/rules/inline.js</code>中第11行和第20行并修改转义规则：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br><span class="line"></span><br><span class="line"><span class="comment">//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line">em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure>
<p>再重新渲染就🆗啦！</p>
<h2 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h2><p><a href="https://home.cnblogs.com/u/Ai-heng/" target="_blank" rel="noopener">VitaHeng</a>的博客<a href="https://www.cnblogs.com/Ai-heng/p/7282110.html" target="_blank" rel="noopener">hexo博客MathJax公式渲染问题</a>，顺便发现这位大兄弟的主页了，前两年还有两篇文章，加油！</p>
<p>自己确实试了错，加了自己的探索过程，除了代码，我觉得不算转载。如果的确需要授权，请联系我商议解决~</p>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读31】DDNF——深度微分同胚的Normalizing Flow</title>
    <url>/2021/01/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB31%E3%80%91DDNF%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%BE%AE%E5%88%86%E5%90%8C%E8%83%9A%E7%9A%84Normalizing-Flow/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>阅读文章<em>Deep Diffeomorphic Normalizing Flows</em></p>
<p>感觉写得不错，但是引用量很少，我觉得是代码没有开源的缘故</p>
</blockquote>
<a id="more"></a>
<h2 id="Deep-Diffeomorphic-Normalizing-Flows"><a href="#Deep-Diffeomorphic-Normalizing-Flows" class="headerlink" title="Deep Diffeomorphic Normalizing Flows"></a>Deep Diffeomorphic Normalizing Flows</h2><h2 id="心路历程"><a href="#心路历程" class="headerlink" title="心路历程"></a>心路历程</h2><p>很长时间没有看过流模型了，这次也看到了许多之前没有接触过的概念，包括很多细节，本来打算这些细节将在文末实验部分单独一一列出，但是太累了不写了，需要的时候自己看看吧</p>
<p>另外文章中介绍了不少以往的模型，这些也不介绍了，以后见到慢慢补充</p>
<h2 id="文献简介"><a href="#文献简介" class="headerlink" title="文献简介"></a>文献简介</h2><img src="/2021/01/13/【论文阅读31】DDNF——深度微分同胚的Normalizing-Flow/DDNF-1.png" title="DDNF">
<ul>
<li><p>作者信息</p>
<p>一作是微软的工程师Hadi Salman，看了看他的<a href="https://www.microsoft.com/en-us/research/people/hasalman/" target="_blank" rel="noopener">个人主页</a>和<a href="https://scholar.google.com/citations?user=Kr8JjF0AAAAJ&amp;hl=en&amp;oi=sra" target="_blank" rel="noopener">学术主页</a>，这个人主要是做对抗攻防的，带一点稳健动力系统。其余作者来自弗吉尼亚大学和匹兹堡大学</p>
</li>
<li><p>文献信息</p>
<p>文献链接：<a href="https://arxiv.org/abs/1810.03256" target="_blank" rel="noopener">https://arxiv.org/abs/1810.03256</a></p>
<p>这篇文章18年挂出来，截至2021/01/12谷歌学术上的引用量是11</p>
</li>
<li><p>文献简述</p>
<p>这个文章有些意思，它提出了一种新的对流模型的建模<strong><em>DDNF</em></strong>，主要的思想是借用ODE中每个state可以看成流模型中隐变量的多次映射过程，即 $z_0\rightarrow z_1\rightarrow\cdots\rightarrow z_n$，那么隐变量的每一步更新看成ODE的积分过程，这个过程可以进一步反应在抽象出来的流形上，如下图所示：</p>
<img src="/2021/01/13/【论文阅读31】DDNF——深度微分同胚的Normalizing-Flow/DDNF-2.png" title="隐变量如何多次映射">
</li>
</ul>
<p>我想把它对于流模型的建模步骤归纳为：</p>
<pre class="mermaid">graph LR
A(DDNF<br>Deep Diffeomorphic Normalizing Flows) -->B(Idea<br>介绍微分同胚和ODE之间的关联)
A -->C(DDNF的结构<br>主要是建模过程,模型设置)
A -->D(Jacobi计算<br>泰勒展开以及行列式的迹估计)
A -->E(正则引入<br>有两种方法改善模型效果,分别是测地正则和逆一致正则)</pre>

<h2 id="基本任务——对归纳偏执建模"><a href="#基本任务——对归纳偏执建模" class="headerlink" title="基本任务——对归纳偏执建模"></a>基本任务——对归纳偏执建模</h2><p>写总结的时候发现还是要写像target一样的东西放在前面</p>
<p>对流的建模基本不变，即数据 $X=\{x_1,\cdots,x_N\}$，引入生成模型及其隐变量 $z$，目标是最大似然</p>
<script type="math/tex; mode=display">\begin{align} \displaystyle \log p_{\theta}(X)&=\sum_{i=1}^N \log p_{\theta}x_i \tag{1}\\ &\overset{p_{\theta}(x)=\int p_{\theta}(z,x)dz}{\geq} \mathbb{E}_{q_{\lambda}(z|x)}[\log p_{\theta}(x|z)-KL(q_{\lambda}(z|x)|p(z))]. \tag{2}\end{align}</script><p>现在给出归纳偏置，由于主要是<strong>建模隐变量</strong> $z$，试图近似它的后验分布，那么本文假设的是<strong>最终的隐变量 $z$ 由一系列可逆变换 $\phi_k$ 作用在初始分布 $q_0$ 上得到</strong>，即</p>
<script type="math/tex; mode=display">\displaystyle final\ latent\ variable\ z_K = \phi_K\circ\cdots\circ\phi_2\circ\phi_1(z_0) \tag{3}</script><p>之后要写出对应的变分目标，具体的过程就是 $(3)$ 式<strong>概率密度函数分解</strong>，由</p>
<script type="math/tex; mode=display">\begin{align} \displaystyle q_{k+1}(z_{k+1})&=q_k(z_k)\bigg|det\dfrac{\partial \phi_{k+1}(z_k)}{\partial z_k}\bigg|^{-1} \tag{4}\\ &=q_k(\phi_{k+1}^{-1}(z_{k+1}))\bigg|det\dfrac{\partial \phi_{k+1}^{-1}(z_{k+1})}{\partial z_{k+1}}\bigg| \tag{5}\end{align}</script><p>可得分解后的变分目标（其实不难，就是要写半天。。。）为</p>
<script type="math/tex; mode=display">\begin{align} \displaystyle \mathcal{F}(\theta,\lambda)&=\mathbb{E}_{q_{\lambda}}[\log q_{\lambda}(z|x)-\log p_{\theta}(x,z)] \tag{6}\\ &=\mathbb{E}_{q_0}[\log q_0(z_0|x)-\log p_{\theta}(x,z_K)] - \mathbb{E}_{q_0}\left[\sum_{k=1}^K \log  \bigg|det\dfrac{\partial \phi_{k}(z_{k-1})}{\partial z_{k-1}}\bigg|\right] \tag{7}\end{align}</script><p>因此，DDNF的目标其实就是<strong>针对这一系列的可逆变换设计流模型</strong>，进一步引入微分同胚保持可逆和光滑性，而这恰好引入了ODE-Net的思想</p>
<h2 id="Idea——微分同胚如何引入"><a href="#Idea——微分同胚如何引入" class="headerlink" title="Idea——微分同胚如何引入"></a>Idea——微分同胚如何引入</h2><p>这要联系ODE中<strong>连续流</strong>的概念，并结合美妙的<strong>黎曼几何</strong>。如文首所述：</p>
<blockquote>
<p>主要的思想是借用ODE中每个state可以看成流模型中隐变量的<strong>多次映射</strong>过程，即 $z_0\rightarrow z_1\rightarrow\cdots\rightarrow z_n$，那么<strong>隐变量的每一步更新看成ODE的积分过程</strong>，这个过程可以进一步反应在抽象出来的流形上</p>
<img src="/2021/01/13/【论文阅读31】DDNF——深度微分同胚的Normalizing-Flow/DDNF-2.png" title="隐变量如何多次映射">
</blockquote>
<p>具体设置为（细节拜拜，反之就是Hilbert空间假设，然后随便推导），每个可逆变换 $\phi_k:\Omega\rightarrow\Omega$，其中 $\Omega\subset\mathbb{R}^d$，就是切向量之间的映射，设 $\Omega$ 的切丛（从这里看出它把 $\Omega$ 直接看成带有内积的流形了，不妨当成黎曼流形就好）的完备化空间为 $V\overset{def}{=}H^s(\mathcal{T}\Omega)$，$s$ 代表切向量的几阶导存在，且都默认二次可积。总之，完备化+黎曼流形就瞎推了</p>
<p>那么<strong>关键</strong>来了，之前说隐变量的前进过程看成ODE上的积分过程，这需要<strong>切向量在流形上的平行移动（其实就是积分）</strong>，所以先要模拟切向量，这里设其形式为一般的可随时间变化的切向量 $v(t,\cdot):[0,1]\rightarrow V$，应该是指流形上每一点处可以得到与时间相关的切向量。那么<strong>隐变量的每一步更新看成ODE的积分过程</strong>：</p>
<script type="math/tex; mode=display">\displaystyle \dfrac{d}{dt}\phi_k(t,z)=v(t,\phi_k(t,z)). \tag{8}</script><p>这样，如果能用像ResNet之类的网络模拟一阶导 $v$，那么ResNet输出的就相当于积分后的方程，输出的是 $\phi_k$；那么再配合上RNN之类的结构把这一系列变换 $\phi_k$ 串起来，那就是整个流的建模过程了</p>
<p>最后再声明，上述建模过程说得很轻巧，看起来也比较合理，那有没有理论保证，你这个就一定行呢？有的，参考原文第3页左下角部分，大概就是</p>
<ul>
<li>前人结论：<ul>
<li>若 $v$ 充分光滑，那 $\phi$ 就相当于微分同胚</li>
<li>这样的flow中的Jacobi行列式总是非负的（<strong>为0不就gg，估计有trick处理，阈值截断什么的</strong>）</li>
</ul>
</li>
<li>原作者总结的性质：<ul>
<li>一系列微分同胚 $\phi_k$ 保持代数运算，它们在代数群上✅</li>
<li>假定空间存在合适的内积，那就成为黎曼流形，进一步可以引入测地线概念，引入测地正则✅</li>
</ul>
</li>
</ul>
<h2 id="网络结构——RNN-ResNet-ODE-Net"><a href="#网络结构——RNN-ResNet-ODE-Net" class="headerlink" title="网络结构——RNN+ResNet(ODE-Net)"></a>网络结构——RNN+ResNet(ODE-Net)</h2><p>这个部分本来以为挺难，但是读完了发现文章中图2真是<strong>一图胜千言</strong>。我解读了三次此网络，前两次应该都错了！后一次就是总结出来的┭┮﹏┭┮</p>
<img src="/2021/01/13/【论文阅读31】DDNF——深度微分同胚的Normalizing-Flow/DDNF-3.png" title="网络设置">
<p>首先，观察<strong>最下面的图</strong>，这就是<strong>整个流的建模过程</strong>，即通过 $K$ <strong>个可逆微分同胚变换</strong>，$\{\phi_{k}\}_k$ ，它把初始化的隐变量 $z_0$，逐步映射到最终目标隐变量 $z_K$。因此，最下面的图中每个block都是在流形上某点$z_k$处切空间中进行积分的过程。且注意到在每个切空间  $T_{z_k}\Omega$中，切向量都是不同的，因此每步变换中的切向量或者说速率不同！</p>
<p>其次，如上图<strong>中间的图</strong>表示，中间的图整个是<strong>一个积分过程，表示一步可逆变换</strong> $z_{k+1}=\phi_{k+1}(z_k)$。<strong>整体是RNN</strong>模型，其中有 $T$ 个block，指的是 $T$ 个ResNet block，$T$ 指的是从 $z_k$ 到 $z_{k+1}$ 的积分过程中离散近似的小区间数目，RNN就是把这离散的过程叠加循环起来。且为了方便，建模时考虑<strong>使用不随时间变化的stationary向量场</strong>，即每个block使用相同的向量场，图中记为 $v^k$ 是很合理的。此时，整个一步积分是自治方程 $(8)$ 的求解过程，，解出来其实是指映射 $\phi^{v_k}=exp(v^k(t,\cdot))=exp(v^k(\cdot))$，如果考虑实验中的近似手段 $T$ 个block的结果叠加，那就是 $\phi^{v_k}=exp(v^k(\cdot)/T)^T$</p>
<p>最后是<strong>最上面的模型</strong>，即具体<strong>一步积分中一个小区间的近似</strong>过程，为<strong>一个ResNet block</strong>，其中使用多少层，每层宽度多少是可以调整的。我记的没错的话，本文实验多使用的结构只有两层，每层只有两个神经元，目的是验证即使结构简单，其表达能力也很强，这将会验证基于ODE的微分同胚流的模型是优异的</p>
<hr>
<p>这样模型就讲完了，下面介绍为什么使用ResNet的结构，计算Jacobi会用到，但是这<strong>和以前我看的不一样</strong>，我记得以前看过哪篇文章，<strong>用ResNet的结构会使Jacobi呈现行列式为1的形式</strong>，进一步微调使之变化（好像是这样，忘了）。这里计算Jacobi不太一样</p>
<p>本文的思路是借用ResNet中恒等映射的存在<strong>作泰勒展开</strong>近似估计Jacobi行列式：</p>
<script type="math/tex; mode=display">\begin{align} \displaystyle \log det(\mathcal{J}\phi^v(\Delta t,\cdot))&\overset{RestNet}{=}\log det(I+\Delta t\mathcal{J}v(\cdot)) \tag{9}\\ &\overset{A\overset{def}{=}I+\Delta t\mathcal{J}v(\cdot)}{=}\log det(A) \tag{10}\\ &=\log \left(det(A^2)\right)^\frac{1}{2} \tag{11}\\ &= \dfrac{1}{2}\log \left(det(A^TA)\right) \tag{12}\\ &= \dfrac{1}{2}\log \left(det(I+\Delta t(\mathcal{J}v(\cdot)+\mathcal{J}v(\cdot)^T+\Delta t\mathcal{J}v(\cdot)^T\mathcal{J}v(\cdot)))\right) \tag{13}\\ &\overset{def}{=} \dfrac{1}{2}\log \left(det(I+\Delta tB)\right) \tag{14}\\ &\approx \dfrac{1}{2}\Delta tTr(\mathcal{J}v(\cdot)+\mathcal{J}v(\cdot)^T)-\dfrac{1}{2}(\Delta t)^2Tr(\mathcal{J}v(\cdot)^T\mathcal{J}v(\cdot)). \tag{15}\end{align}</script><p>其中最后一步我<strong>没有推出来</strong>，现在不推了，日后需要再搞。只知道是在 $B=0$ 处二阶泰勒展开，三次及三次以上都近似忽略掉，然后用迹估计的方法即可</p>
<p>所以使用ResNet的结构并引入泰勒展开，<strong>目的是降低Jacobi计算的复杂度</strong>，一般来说根据模型结构，有 $T$ 个Jacobi的行列式需要计算，所以计算复杂度可看成 $\mathcal{O}(Td^3)$，而转化为 $(15)$ 式后，除了矩阵 $\mathcal{J}v$ 的存储外，迹的计算复杂度只有 $\mathcal{O}(d)$，而近似方法中需要 $M$ 常数次采样，因此最终复杂度由 $\mathcal{O}(Td^3)$ 降低到 $\mathcal{O}(Md)$，的确是比较大的进步！</p>
<h2 id="微分同胚流的其它亮点"><a href="#微分同胚流的其它亮点" class="headerlink" title="微分同胚流的其它亮点"></a>微分同胚流的其它亮点</h2><h3 id="流的逆简易计算"><a href="#流的逆简易计算" class="headerlink" title="流的逆简易计算"></a>流的逆简易计算</h3><p>流的逆一般需要好求，这往往需要对网络的结构进行限制，本文表示，由于限定每一个可逆映射中的切向量是stationary的，所以求解出来直接就是指数映射，所以逆回去的时候也指数逆回去就好了，即 $\phi^{v_k}=exp(v^k(\cdot)/T)^T$ 逆为 $(\phi^{v_k})^{-1}=exp(-v^k(\cdot)/T)^T$</p>
<h3 id="两种正则有效"><a href="#两种正则有效" class="headerlink" title="两种正则有效"></a>两种正则有效</h3><p>在最后，本文指出可以引入正则增强模型的性能，有两种：</p>
<ul>
<li><p>测地正则，即 $v_0$ 和 $v_K$ 的测地距离尽可能小，原理大概就是测地线喵喵咪</p>
<script type="math/tex; mode=display">\mathcal{R}(v)=\langle v_0,v_K\rangle_V=\mathbb{E}_{q_0}[v_0^TGv_K] \tag{16}</script><p>其中 $G$ 是黎曼流形的度规矩阵，实际操作的时候用欧氏空间的单位阵即可</p>
</li>
<li><p>逆正则，即逆回去得到的 $\phi^{-1}(z_K)$ 与初始的变量 $z_0$ 要恢复好</p>
<script type="math/tex; mode=display">\mathcal{R}(v)=||z_0-\phi^{-1}(z_K)||_2 \tag{17}</script></li>
</ul>
<h2 id="实验简介"><a href="#实验简介" class="headerlink" title="实验简介"></a>实验简介</h2><p>文章的一个不错的地方是实验很充足，我都看了，但是不想写细节了，细节也忘了，需要的时候再回顾吧，写累死了。。。</p>
<p>有哪些有意思的实验：</p>
<ul>
<li>文中图7，模拟数据验证性能可以逼近分布</li>
<li>文中图4和图6，分别是toy和真实数据中变量分布的逼近，与其它几个模型对比，优势体现在不需要ResNet太多层等</li>
<li>文中图5和图8，两种正则的效果，看起来不错</li>
<li>文中表1，MNIST和Omniglot数据上的实验，大模型套的VAE，DDNF用来建模隐变量，似乎效果不错，只是说的好少？</li>
</ul>
<h2 id="优缺点简析"><a href="#优缺点简析" class="headerlink" title="优缺点简析"></a>优缺点简析</h2><p>模型优点（根据原文所述）：</p>
<ul>
<li>所提的微分同胚流 $f$ 保持了微分同胚的性质，$f$ 可逆，且 $f$ 与 $f^{-1}$ 都光滑</li>
<li>流的建模引入ODE的概念，流的部分变化过程由可与时间相关的向量场 $v(t,\cdot)$ 完成，此向量场由ResNet模拟</li>
<li>流的建模还引入了RNN来近似整个流的变化，结构使得似然推断还行，流的逆易求，隐变量的概率密度也好算</li>
<li>流的建模没有对网络结构有特殊设计</li>
<li>实验充足，据文章说competitive with SOTA</li>
<li>黎曼几何有更多的认识么？</li>
</ul>
<p>模型缺点：</p>
<ul>
<li>文章说代码将会开源，这都2021了我搜不到，迷惑</li>
<li>其实还是有很多估计，现在还能记起来的有，ODE积分的离散估计；stationary切向量的使用（但是不用这个就没有指数性质）；泰勒展开中只近似到2阶项；Jacobi计算时迹的估计</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Hadi Salman, Payman Yadollahpour, Tom Fletcher, and Kayhan Batmanghelich. Deep diffeomorphic normalizing flows. arXiv e-prints, page arXiv:1810.03256, 2018.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>ODE</tag>
        <tag>Flow Model</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文略读30】AI Poincare</title>
    <url>/2021/01/11/%E3%80%90%E8%AE%BA%E6%96%87%E7%95%A5%E8%AF%BB30%E3%80%91AI-Poincare/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>阅读文章<em>AI Poincaré: Machine Learning Conservation Laws from Trajectories</em></p>
<p>名字挺炫酷，不过看了一遍，觉得可以借鉴的地方不多，见正文方法部分最后一句</p>
</blockquote>
<a id="more"></a>
<h1 id="AI-Poincare"><a href="#AI-Poincare" class="headerlink" title="AI Poincaré"></a>AI Poincaré</h1><p>还是偏物理的，个人觉得不太算CS的文章，有的格式都不一样；只有5页，不过符号不算少，就是证明没有，难道是保密技术？纯粹是兴趣看一看，<strong>作者MIT学物理的</strong>，不是我，狗头👍。而且可以学习借鉴的地方不多</p>
<p>文献链接：<a href="https://arxiv.org/abs/2011.04698" target="_blank" rel="noopener">https://arxiv.org/abs/2011.04698</a></p>
<p>提出的方法叫做 $AI\ Poincar\acute{e}$，是只利用未知动力系统轨线数据自动寻找守恒律的学习算法</p>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li>纯粹数据驱动，从轨线trajectories中学习守恒律</li>
<li>自动学习，不手动设计特征，且不加显式的偏置归纳</li>
<li>实验优点：哈密顿系统中发现了所有精确的守恒量，而且似乎有一定的解释性：发现了周期轨道，phase transitions和breakdown timescale</li>
</ul>
<h2 id="认为的缺点："><a href="#认为的缺点：" class="headerlink" title="认为的缺点："></a>认为的缺点：</h2><ul>
<li>方法在建模比赛中应该是非常有意思的，但是在学习领域，觉得不够味道</li>
<li>虽然走纯数据驱动的路子，但是可解释性不足，发现轨道等概念的解释性和我想的不一样</li>
<li>方法其实解释得不清楚，只是讲了一遍过程，合理性不足</li>
<li>实验结果似乎对不上？没看明白ERD图，难道不是中间有几条线下凸，低于阈值就是发现多少守恒律？文章表格和描述的似乎不一致，我很费解，可能是我菜。。。</li>
</ul>
<h2 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h2><p>文章自述一个宏伟的目标是<strong>智能学习物理系统中的规律，如符号回归得到的方程形式、守恒律、对称性，etc</strong>，我喜欢。本文的具体target是：<strong>从轨线trajectories中学习守恒律</strong>。</p>
<h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><p>一般求守恒律可能要限制方程形式？所以以往方法可能是模型驱动。现在的idea就是表示只用轨线，对方程形式没有任何假设，就是要<strong>纯粹数据驱动</strong>试试。</p>
<p>具体的思想见建模的过程，本质是概念的对应，实现的方式比较基础</p>
<h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><p>用于对比批判的方法：</p>
<ul>
<li>用AE、暹罗网络的正交方法（我自己未验证此方法），可以寻找对称性。缺点是要手动设计（对称性）特征</li>
<li>把守恒律作为偏执归纳先验，但这不是纯数据驱动</li>
</ul>
<p>重点参考的方法：</p>
<p>sampling manifolds方法和动力系统<strong>概念的对应</strong>（我一直在想这个问题但是想不通。。）看了这篇文章给的对应关系，也只是觉得可能有关系，但是还是没有说清楚概念之间的真正联系，可能<strong>还是要看更基础的文章</strong>！</p>
<img src="/2021/01/11/【论文略读30】AI-Poincare/AIPoin.png" title="概念对应关系">
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>看完方法觉得在机器学习的范畴内亮点不多。把方法流程大致过一遍。</p>
<p>首先是模型的基本假设，假定是<strong>带有守恒律</strong> $H_j(\mathbf{x})=h_j$ 的方程（真的是守恒律），每条守恒律都看作是一个高维空间（在可操作意义下都是 $\mathbb{R}^{l}$）中的超平面，满足一条守恒律就意味着有一个<strong>限制条件</strong>（凸分析学的好啊）。本文把所有可行轨线所在的空间看成与满足守恒律的空间一致，称之为可允许状态流形<code>permissible state manifold (PSM)</code> $\mathcal{M}$，且<strong>假设</strong>轨线数据集 $\mathcal{S}=\{\mathbf{x}(t)|t\geq 0\}$ 都分布在这个流形 $\mathcal{M}$ 上。它定义为</p>
<script type="math/tex; mode=display">\begin{equation} \mathcal{M} = \{\mathbf{x}\in\mathbb{R}^N|H_j(\mathbf{x})=h_j\} \tag{1}\end{equation}</script><p>那么每满足一条守恒律，此流形其实就有了一个的限制，相当于在 $\mathbb{R}^N$ 的基础上维度<strong>降低一维</strong>，如果观察到总维度（应该是提前已知）减去估计出的轨线数据集 $\mathcal{S}$ 的维度不为0，那就是找出了守恒律。</p>
<p>那么第二点是模型怎么算的。<strong>第1步是对轨线数据预处理</strong>，标准化，这样会有一个协方差矩阵，对角元对应特征值，有特征值就说明找到了重要的维度，初始时的非0特征值个数就应该是上一段说的总维度，然后一旦<strong>有特征值在迭代的过程中变得很小</strong>，那就是守恒律被发现了，这个维度就对应守恒律。第2步是对轨线数据加噪，干扰之，得到<strong>近似的流形</strong>（我认为是这样），再蒙特卡洛采样，学习恢复流形的函数，这一步的目的应该想把干扰后的轨线投影到切空间，如果守恒律存在，那么<strong>合适的干扰对守恒律会有较大的影响</strong>。那么第3步就是把投影后的轨线数据PCA降维处理得到最后的维度，如果降维的过程中发现<strong>维度变化很大</strong>，说明合适水平的噪声影响到了守恒律。这时根据<code>explained ratio diagram (ERD)</code>（我真不懂这个图怎么对应几条守恒律，因为好像和结果描述的不一致）可<strong>看出</strong>守恒律有几条。</p>
<p>思路就是这样，有点意思，意思在于<strong>干扰影响了守恒律</strong>，且合适水平的干扰可以最大影响守恒律；另外就是其实专门去寻找可能存在的守恒律，这一点与流形上有些概念似乎可以对应。私以为本文<strong>只有这两点可以借鉴</strong>。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>做了5种有守恒律的哈密顿方程的实验。但是私以为讲得不清楚，因为用ERD看出的守恒律数目和文章描述的似乎不一样😶。也可能是我菜看不出来🤡。。。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Ziming Liu and Max Tegmark. AI Poincaré: Machine Learning Conservation Laws from Trajectories. arXiv e-prints, page arXiv:2011.04698, November 2020.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Manifold</tag>
        <tag>PDE</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文略读29】深度生成模型的黎曼几何理论</title>
    <url>/2020/12/19/%E3%80%90%E8%AE%BA%E6%96%87%E7%95%A5%E8%AF%BB29%E3%80%91%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%BB%8E%E6%9B%BC%E5%87%A0%E4%BD%95%E7%90%86%E8%AE%BA/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>阅读文章<em>The Riemannian Geometry of Deep Generative Models</em></p>
<p>名字挺吓人</p>
</blockquote>
<a id="more"></a>
<h2 id="文章信息"><a href="#文章信息" class="headerlink" title="文章信息"></a>文章信息</h2><ul>
<li><p>18年CVPR workshop，据说评价不高</p>
</li>
<li><p>作者来自盐湖城犹他大学和IBM</p>
</li>
</ul>
<h2 id="简单笔记"><a href="#简单笔记" class="headerlink" title="简单笔记"></a>简单笔记</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Background</th>
<th>Motivation</th>
<th>Targets/Contributions</th>
<th>实验结论：</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 假设实际复杂数据存在流形表示，高维数据在低维流形上有结构</td>
<td></td>
<td>1. 研究生成模型中，低维隐空间中流形可能的黎曼几何性质。真研究黎曼几何中的流形，一共三个算法，见表格下方所述</td>
<td>实际数据的流形非线性，接近０曲率</td>
</tr>
<tr>
<td>2. 假设生成模型学习低维隐空间到高维数据空间的映射，即模型能利用低维隐空间参数化，映射到高维空间中的数据流形（生成器）</td>
<td></td>
<td>2. 计算测地线，主要是点到流形距离</td>
<td>隐空间中的（线性路径）直线相当于流形上的测地线</td>
</tr>
<tr>
<td></td>
<td></td>
<td>3. 沿流形path上切向量的平移（对应数据点的语义平移，generate analogies），path是啥我还不知道</td>
</tr>
</tbody>
</table>
</div>
<p>表中提到的文章提出三个算法：</p>
<ul>
<li>geodesic interpolation between two points on the manifold</li>
<li>parallel translation of a tangent vector along a path on the manifold</li>
<li>geodesic shooting from an initial point and velocity on the manifold</li>
</ul>
<p>问题：</p>
<ul>
<li>Chapter 2中有不少维度让我困惑，最多是D维像集微分同胚于d维隐空间？</li>
<li>（4）式左边是E？</li>
<li>为什么（4）的近似可以直接在M上用范数？需要度量来导？有点像欧氏空间了，不知道能不能这样写</li>
</ul>
<p>细节：</p>
<ul>
<li>雅可比阵把隐空间中切向量映为像集中切向量，通过自动求导计算</li>
<li>数据空间诱导黎曼度量，提供内积</li>
<li>黎曼流形平坦，曲率为0，但不一定线性！</li>
</ul>
<p>打</p>
<hr>
<p><strong>回头再更新，觉得这个文章写得很垃圾，前前后后花了一个星期推导</strong>。。。</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>GAN</tag>
        <tag>Manifold</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文略读28】继PDE-Net 2.0后引用它的一系列文章</title>
    <url>/2020/12/12/%E3%80%90%E8%AE%BA%E6%96%87%E7%95%A5%E8%AF%BB28%E3%80%91%E7%BB%A7PDE-Net-2-0%E5%90%8E%E5%BC%95%E7%94%A8%E5%AE%83%E7%9A%84%E4%B8%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>许久未记录正式的博客笔记了，在此补上一系列文章</p>
<p>它们是我读完PDE-Net 2.0后顺着这条线索收集的</p>
</blockquote>
<a id="more"></a>
<h2 id="搜集文献（不分顺序）"><a href="#搜集文献（不分顺序）" class="headerlink" title="搜集文献（不分顺序）"></a>搜集文献（不分顺序）</h2><ol>
<li>Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning<ul>
<li>董彬老师组</li>
<li><a href="https://arxiv.org/abs/1905.11079?context=math" target="_blank" rel="noopener">https://arxiv.org/abs/1905.11079?context=math</a></li>
</ul>
</li>
<li>Data-driven recovery of hidden physics in reduced order modeling of fluid flows<ul>
<li><a href="https://aip.scitation.org/doi/abs/10.1063/5.0002051" target="_blank" rel="noopener">https://aip.scitation.org/doi/abs/10.1063/5.0002051</a></li>
<li>期刊名Physics of Fluids，20年初</li>
</ul>
</li>
<li>DeepMoD: Deep learning for model discovery in noisy data<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0021999120307592" target="_blank" rel="noopener">https://www.sciencedirect.com/science/article/pii/S0021999120307592</a></li>
<li>期刊名Journal of Computational Physics，20年11月</li>
</ul>
</li>
<li>Stability selection enables robust learning of partial differential equations from limited noisy data<ul>
<li><a href="https://arxiv.org/abs/1907.07810" target="_blank" rel="noopener">https://arxiv.org/abs/1907.07810</a></li>
<li>ArXiv上的分类是Mathematics—&gt;Numerical Analysis，19年7月</li>
</ul>
</li>
<li>Derivatives Pricing via Machine Learning<ul>
<li><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3352688" target="_blank" rel="noopener">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3352688</a></li>
<li><a href="https://www.scirp.org/journal/paperinformation.aspx?paperid=94637" target="_blank" rel="noopener">https://www.scirp.org/journal/paperinformation.aspx?paperid=94637</a></li>
<li>期刊名Business &amp; Economics，19年</li>
</ul>
</li>
<li>Extracting Interpretable Physical Parameters from Spatiotemporal Systems Using Unsupervised Learning<ul>
<li><a href="https://journals.aps.org/prx/abstract/10.1103/PhysRevX.10.031056" target="_blank" rel="noopener">https://journals.aps.org/prx/abstract/10.1103/PhysRevX.10.031056</a></li>
<li>期刊名PHYSICAL REVIEW X，20年9月</li>
</ul>
</li>
<li>DLGA-PDE: Discovery of PDEs with incomplete candidate library via combination of deep learning and genetic algorithm<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0021999120303582" target="_blank" rel="noopener">https://www.sciencedirect.com/science/article/pii/S0021999120303582</a></li>
<li>期刊名Journal of Computational Physics，20年10月</li>
</ul>
</li>
<li>Feature engineering and symbolic regression methods for detecting hidden physics from sparse sensor observation data<ul>
<li><a href="https://aip.scitation.org/doi/abs/10.1063/1.5136351" target="_blank" rel="noopener">https://aip.scitation.org/doi/abs/10.1063/1.5136351</a></li>
<li>期刊名Physics of Fluids，20年1月</li>
</ul>
</li>
<li>Data-driven Discovery of Partial Differential Equations for Multiple-Physics Electromagnetic Problem<ul>
<li><a href="https://arxiv.org/abs/1910.13531" target="_blank" rel="noopener">https://arxiv.org/abs/1910.13531</a></li>
<li>ArXiv上分类Physics—&gt;Computational Physics，19年10月</li>
</ul>
</li>
<li>TIME: A Transparent, Interpretable, Model-Adaptive and Explainable Neural Network for Dynamic Physical Processes<ul>
<li><a href="https://arxiv.org/abs/2003.02426" target="_blank" rel="noopener">https://arxiv.org/abs/2003.02426</a></li>
<li>20年5月</li>
</ul>
</li>
<li>Sparse Symplectically Integrated Neural Networks<ul>
<li><a href="https://proceedings.neurips.cc/paper/2020/hash/439fca360bc99c315c5882c4432ae7a4-Abstract.html" target="_blank" rel="noopener">https://proceedings.neurips.cc/paper/2020/hash/439fca360bc99c315c5882c4432ae7a4-Abstract.html</a></li>
<li>NIPS’20</li>
</ul>
</li>
<li>DeepM&amp;Mnet: Inferring the electroconvection multiphysics fields based on operator approximation by neural networks<ul>
<li><a href="https://arxiv.org/abs/2009.12935" target="_blank" rel="noopener">https://arxiv.org/abs/2009.12935</a></li>
<li>ArXiv上分类Physics—&gt;Computational Physics，20年9月</li>
</ul>
</li>
<li>Integration of Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery<ul>
<li><a href="https://ieeexplore.ieee.org/abstract/document/9180100" target="_blank" rel="noopener">https://ieeexplore.ieee.org/abstract/document/9180100</a></li>
<li>IEEE Transactions on Neural Networks and Learning Systems，20年8月</li>
</ul>
</li>
</ol>
<h2 id="1-Learning-to-Discretize-Solving-1D-Scalar-Conservation-Laws-via-Deep-Reinforcement-Learning"><a href="#1-Learning-to-Discretize-Solving-1D-Scalar-Conservation-Laws-via-Deep-Reinforcement-Learning" class="headerlink" title="1. Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning"></a>1. Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning</h2><h3 id="文献资料"><a href="#文献资料" class="headerlink" title="文献资料"></a>文献资料</h3><ol>
<li><p>董彬老师组的文章，2020/10挂出来</p>
</li>
<li><p><a href="https://arxiv.org/abs/1905.11079?context=math" target="_blank" rel="noopener">https://arxiv.org/abs/1905.11079?context=math</a></p>
</li>
</ol>
<h3 id="表格总结"><a href="#表格总结" class="headerlink" title="表格总结"></a>表格总结</h3><p>总结完了觉得文章的<strong>思路正常</strong>，文章的一个<strong>亮点</strong>是抽象数值方法为RL问题时<strong>引入了meta-learner</strong>的概念？</p>
<table id="tfhover" class="tftable" border="1" style="table-layout:fixed;">
    <col style="width: 15%">
    <col style="width: 85%">
<tr><th>文献条目</th><th>具体内容</th></tr>
<tr>
<td>Target</td>
<td>
<body>
  <ul type="disc">
    <li>先声明大领域，依然是拟合PDE数据，包括数值方法、DL方法</li>
    <li>本文针对求解特定的PDE，守恒律方程（associated with conservation laws）</li>
  </ul>
</body>
</td></tr>
<tr><td>Motivation/Idea</td>
<td>
<body>
  <ul type="disc">
    <li>守恒律很重要，应用多。且Burgers方程就是其一个特例</li>
    <li>从数值方法出发，把PDE-solver看成MDP（Markov Decision Process），进而抽象为强化学习基本问题</li>
    <li>上述数值方法是WENO（Weighted Essentially Non-Oscillatory Schemes），稍后在细节介绍，基于它有两个ideas：<ul><li>自动化得到方法中的weights</li><li>自动（原WENO需要进行数值判断）判断方法中的upwind direction，此概念在细节中介绍</li></ul></li>
  </ul>
</body>
</td>
</tr>
<tr><td>Method</td>
<td>
<body>
  <ul type="disc">
    整个模型没有命名，其实提供了一种把数值方法转化为网络、DL问题的思路
    <li>基本模型就是求解PDE的WENO数值方法抽象为MDP形式</li>
    <li>MDP问题再引入RL，具体细节将在下面介绍</li>
  </ul>
</body>
</td>
</tr>
<tr><td>Pros and Cons</td>
<td>
<body>
  <ul type="disc">
    Pros:
      <li>把求解1维情况下守恒律方程的数值方法转化为RL问题，其建模过程很标准，可以说是一个framework</li>
      <li>数值方法直接转化为MDP，RL，model-driven么</li>
      <li><b>文章重点提到action的构建是个meta-learner,一个说法是，这样的模型泛化不错，一个功劳就归meta-learner</b></li>
    <br>Cons:
      <li>别问我为什么meta-learner，怎么就meta-learner了，原因我放在后面单独提一下</li>
      <li>只考虑了1维情形下守恒律，不能直接推广的话。。。</li>
      <li>流（flux）的建模依托于WENO，插值太多，虽然插值很精细但总觉得不优美，说不清楚</li>
  </ul>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</td>
</tr>
</table>


<h3 id="部分细节"><a href="#部分细节" class="headerlink" title="部分细节"></a>部分细节</h3><h4 id="模型基本设置"><a href="#模型基本设置" class="headerlink" title="模型基本设置"></a>模型基本设置</h4><p>先讲讲所谓有守恒律的方程指什么吧，其实就是指这个方程有守恒律，形式如下：</p>
<script type="math/tex; mode=display">\displaystyle u_x(x,t)+(f(u(x,t)))_x = 0,\ where\ a\leq x\leq b,\ t\in [0,T],\ u(x,0)=u_0(x) \tag{1}</script><p>这个形式就叫守恒律。由参数 $\{x,t\}$ 取值在区间上可知，ob数据的形式是把时空间分别分割，得到网格式数据，分割一般均匀，设分割长度和总数分别为 $\Delta x, \Delta t$ 和 $J, N$，详见原文公式 $(2.2)$。</p>
<p>然后有几个概念要说一下，真实解 $u(x,t)$ 在网格上的取值为 $u(x_j,t_n)$，它的近似记为 $\mathcal{U}_j^n$；另外，$(1)$ 式中间的 $f$ 称为flux，可以理解为守恒律中的流，真实的flux记为 $f_j^n=f(u(x_j,t_n))$。</p>
<p>最后有个空间中的插值，记号为 $\displaystyle x_{j\pm\frac{1}{2}}=x_j\pm\frac{\Delta x}{2}$，感觉就是更细一点的插值。</p>
<h4 id="WENO数值方法"><a href="#WENO数值方法" class="headerlink" title="WENO数值方法"></a>WENO数值方法</h4><p>从名字上看这个方法，Weighted Essentially Non-Oscillatory Schemes，推测就是<strong>插值更细</strong>因此可能拟合结果波动更小，插值的时候还有重要性（插值准确性）加权。</p>
<p>下面给个WENO方法的计算过程：</p>


	<div class="row">
    <embed src="20201212-1D-1.pdf" width="100%" height="550" type="application/pdf">
	</div>



<p>如上最后两行，WENO就像一个<strong>多重平均插值</strong>近似，<strong>主要问题</strong>在于不同插值权重的计算、和最后一个upwind direction计算。后者应该是希望数值解不要震荡的方法，参考<a href="https://baike.baidu.com/item/%E4%BA%8C%E9%98%B6%E8%BF%8E%E9%A3%8E%E6%A0%BC%E5%BC%8F/3533165" target="_blank" rel="noopener">二阶迎风格式</a>。</p>
<h4 id="WENO对应到MDP"><a href="#WENO对应到MDP" class="headerlink" title="WENO对应到MDP"></a>WENO对应到MDP</h4><p>没啥好说的，根据WENO的计算方法写成算法，然后对应到MDP中的state $S$，action $A$，transition dynamics $P$，reward $r$。</p>
<img src="/2020/12/12/【论文略读28】继PDE-Net-2-0后引用它的一系列文章/1D-1.png" title="WENO算法">
<img src="/2020/12/12/【论文略读28】继PDE-Net-2-0后引用它的一系列文章/1D-2.png" title="RL对应">
<p>这样就完事了，再简要介绍一下里面的东西是什么：</p>
<ul>
<li>$s$ 是state，有个state function，把指标集对应的$\mathcal{U}_j^\lambda,\ \lambda\in \Lambda$ 映射到 $\hat{f}_j^n$。一个例子是对pdf的三次插值，那 $s$ 就映到3个向量，每个向量是每次插值的所有点（$f_j^\lambda$）和该次插值对应的权重，具体实现是方式是用6层，每层64神经元的MLP，并use the Twin Delayed Deep Deterministic (TD3) policy gradient algorithm to train the RL policy。。。</li>
<li>$A$ 是action，就是pdf最后一行提到的选择哪些插值，由 $s$ 函数生成</li>
<li>$P$ 相当于迭代机制，比如前向欧拉对应的迭代形式。。。</li>
<li>$r$ 用的插值时的无穷范数的相反数</li>
</ul>
<h4 id="哪来的meta-learner"><a href="#哪来的meta-learner" class="headerlink" title="哪来的meta-learner"></a>哪来的meta-learner</h4><p>思想不错，$A$ 成为一个meta-learner，原因只有这个靠谱：这个RL里的 $A$ 是通过 $s$ 函数输出得到的，是从当前状态判断的，不是像原来的数值方法那样，在没有其它网络（如上文MLP）的帮助下直接从数值机制推断的。</p>
<p>文章提的其它原因不太靠谱：</p>
<ul>
<li>Learning the policy $P$ within the RL framework makes the algorithm meta-learning like [1, 5, 10, 20, 29].</li>
<li>The learned policy network is carefully designed to determine a good local discrete approximation based on the current state of the solution, which essentially makes the proposed method a meta-learning approach.</li>
<li>We attribute the good generalization ability of RL-WENO to our careful action design, which essentially makes RL-WENO a meta-learner under the WENO framework and thus have  strong out-of-distribution generalization.</li>
</ul>
<h2 id="2-DeepMoD-Deep-learning-for-model-discovery-in-noisy-data"><a href="#2-DeepMoD-Deep-learning-for-model-discovery-in-noisy-data" class="headerlink" title="2. DeepMoD: Deep learning for model discovery in noisy data"></a>2. DeepMoD: Deep learning for model discovery in noisy data</h2><p><strong><font color="#FF0000">突然想到一个问题，目前没看到几篇文章很关注PDE的边界条件！</font></strong>本文也没有考虑。</p>
<h3 id="文献资料-1"><a href="#文献资料-1" class="headerlink" title="文献资料"></a>文献资料</h3><ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0021999120307592" target="_blank" rel="noopener">https://www.sciencedirect.com/science/article/pii/S0021999120307592</a></li>
<li>好像是巴黎大学的研究者写的</li>
<li>期刊名Journal of Computational Physics，20年11月</li>
</ul>
<h3 id="小结：我觉得不彳亍"><a href="#小结：我觉得不彳亍" class="headerlink" title="小结：我觉得不彳亍"></a>小结：我觉得不彳亍</h3><h4 id="模型：DeepMoD及具体内容"><a href="#模型：DeepMoD及具体内容" class="headerlink" title="模型：DeepMoD及具体内容"></a>模型：DeepMoD及具体内容</h4><p>本文提出模型，DeepMoD，指的是deep learning based model discovery algorithm，目标是从数据中学习背后的PDE。该PDE模型的形式其实比较局限，固定为：</p>
<script type="math/tex; mode=display">\displaystyle \partial_t u = \partial_t u(x,t)=\mathcal{F}(u,u_x,uu_x,u_xx\cdots)\approx \Theta\xi \tag{1}</script><p>两个大困惑，读了好几遍搞不明白，文章为什么要刻意避开这个问题❔我觉得只考虑了这样的形式是因为把神经网络 $f_i$ 作为函数字典，输入是 $(\mathbf{x, t})$，那么输出对输出自动求导看成偏导数。但是<strong>为什么原文公式 $(1)$ 没有 $u$ 对 $t$ 求导呢，是不是在刻意混淆</strong>？而且文章的实验表示不是整个grid上都有数据，可以随机取，那么偏导数也是不能全的啊，怎么保证神经网络就能对输入求导得到字典中的基函数，见下面图中的Library？</p>
<img src="/2020/12/12/【论文略读28】继PDE-Net-2-0后引用它的一系列文章/DeepMoD-1.png" title="DeepMoD">
<p>具体方法采用了函数字典，使用稀疏回归，回归时加正则。使用densely-connected feed-forward neural network作为函数的估计，来构建函数字典。考虑了三种loss，MSE损失针对 $u(x,t)$，只考虑每条轨线末端值的监督；回归损失针对 $\Theta\xi$ 的拟合，但是原文的式子下标没写清楚，弄不明白哪里做了监督；最后是字典系数 $\xi$ 的 $L_1$ 正则</p>
<p>网络训练有2个骚操作：</p>
<ul>
<li>数据有一些处理，当神经网络训练完之后，得到的函数字典的稀疏系数其实会不那么稀疏（$L_1$ 不能保证完全稀疏），进一步进行无量纲化，方式是所有变量标准化，包括原文 $(2)$ 式中的 $\partial_t u,\Theta,\xi$，具体意义见原文 $(3,4)$ 式。</li>
<li>网络训练完之后，再练一次，不加 $L_1$ 正则了，回归项只用之前筛选出来的，原文只说这样得系数的无偏估计❔</li>
</ul>
<p>这个方法起效果需要函数字典充足，不过实验结果似乎表示很充足也不至于过拟合，有对函数系数的正则，这个正则和系数某个阈值的设置有关。这个设置似乎不是general的（见Discussion部分）。字典中函数的系数代表了某种模型选择。</p>
<h4 id="Pros-and-Cons"><a href="#Pros-and-Cons" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h4><p>前两个文章自己说的优点很奇怪：</p>
<ul>
<li><p>一个是<strong>对噪声非常稳健</strong>，是实验结论</p>
</li>
<li><p>第二个是<strong>不需要训练集</strong>。这应该指的是<strong>不需要太多训练数据，小样本</strong>也可以，并不是完全不需要，原文进行了5种方程的人工实验，一个结论是几种方程的模拟只需要 $\mathcal{O}(10^2)$ 的数据量。</p>
<blockquote>
<p>优点原文：This construction makes it <strong>extremely robust to noise</strong>, <strong>applicable to small data sets</strong>, and, contrary to other deep learning methods, <strong>does not require a training set</strong>.</p>
<p>实验结果之一：</p>
<p>find that it requires as few as $\mathcal{O}(10^2)$ samples and works at noise levels up to 75%</p>
</blockquote>
</li>
<li><p>第三个是DeepMoD对数据的维度没有要求，之前有些模型是针对1维数据的</p>
</li>
<li>第四个，函数字典有模型选择的功能，只是少了点味</li>
</ul>
<p>文章表示DeepMoD很稳健，需要数据量少是因为利用regression-based approach完成model discovery任务，用神经网络infer system parameters。。。说🔨呢，真的是原文。。。我很不喜欢这样的说法</p>
<p>缺点来了，读不明白就甩锅：</p>
<ul>
<li>PDE模型的形式到底局限么？原文是不是在故意回避这个问题</li>
<li>怎么保证神经网络就能对输入求导得到字典中的基函数？偏导数不一定能全？</li>
<li>两次训练为什么是无偏估计，没说</li>
<li>阈值的设置好像是special的</li>
<li>只靠loss得到所谓的稳健、小样本。难道又是实验验证？我不信，而且全是模拟数据</li>
<li>本文提了一下边界条件，但是仍然没考虑</li>
</ul>
<h2 id="10-TIME-A-Transparent-Interpretable-Model-Adaptive-and-Explainable-Neural-Network-for-Dynamic-Physical-Processes"><a href="#10-TIME-A-Transparent-Interpretable-Model-Adaptive-and-Explainable-Neural-Network-for-Dynamic-Physical-Processes" class="headerlink" title="10. TIME: A Transparent, Interpretable, Model-Adaptive and Explainable Neural Network for Dynamic Physical Processes"></a>10. TIME: A Transparent, Interpretable, Model-Adaptive and Explainable Neural Network for Dynamic Physical Processes</h2><ul>
<li><a href="https://arxiv.org/abs/2003.02426" target="_blank" rel="noopener">https://arxiv.org/abs/2003.02426</a></li>
<li>20年5月</li>
</ul>


	<div class="row">
    <embed src="TIME.pdf" width="100%" height="550" type="application/pdf">
	</div>



<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>APA 7th格式</p>
<p>[1] Wang, Y., Shen, Z., Long, Z., &amp; Dong, B. (2019). Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning. arXiv e-prints, arXiv:1905.11079. <a href="https://ui.adsabs.harvard.edu/abs/2019arXiv190511079W" target="_blank" rel="noopener">https://ui.adsabs.harvard.edu/abs/2019arXiv190511079W</a> </p>
<p>[2] Pawar, S., Ahmed, S. E., San, O., &amp; Rasheed, A. (2020). Data-driven recovery of hidden physics in reduced order modeling of fluid flows. Physics of Fluids, 32(3), 036602. <a href="https://doi.org/10.1063/5.0002051" target="_blank" rel="noopener">https://doi.org/10.1063/5.0002051</a> </p>
<p>[3] Both, G.-J., Choudhury, S., Sens, P., &amp; Kusters, R. (2020). DeepMoD: Deep learning for model discovery in noisy data. Journal of Computational Physics, 109985. <a href="https://doi.org/https://doi.org/10.1016/j.jcp.2020.109985" target="_blank" rel="noopener">https://doi.org/https://doi.org/10.1016/j.jcp.2020.109985</a> </p>
<p>[4] Maddu, S., Cheeseman, B. L., Sbalzarini, I. F., &amp; Müller, C. L. (2019). Stability selection enables robust learning of partial differential equations from limited noisy data. arXiv e-prints, arXiv:1907.07810. <a href="https://ui.adsabs.harvard.edu/abs/2019arXiv190707810M" target="_blank" rel="noopener">https://ui.adsabs.harvard.edu/abs/2019arXiv190707810M</a> </p>
<p>[5] Ye, T., &amp; Zhang, L. (2019). Derivatives Pricing via Machine Learning. Journal of Mathematical Finance, 09, 561-589. <a href="https://doi.org/10.4236/jmf.2019.93029" target="_blank" rel="noopener">https://doi.org/10.4236/jmf.2019.93029</a> </p>
<p>[6] Lu, P. Y., Kim, S., &amp; Soljačić, M. (2020). Extracting Interpretable Physical Parameters from Spatiotemporal Systems Using Unsupervised Learning. Physical Review X, 10(3), 031056. <a href="https://doi.org/10.1103/PhysRevX.10.031056" target="_blank" rel="noopener">https://doi.org/10.1103/PhysRevX.10.031056</a> </p>
<p>[7] Xu, H., Chang, H., &amp; Zhang, D. (2020). DLGA-PDE: Discovery of PDEs with incomplete candidate library via combination of deep learning and genetic algorithm. Journal of Computational Physics, 418, 109584. <a href="https://doi.org/https://doi.org/10.1016/j.jcp.2020.109584" target="_blank" rel="noopener">https://doi.org/https://doi.org/10.1016/j.jcp.2020.109584</a> </p>
<p>[8] Vaddireddy, H., Rasheed, A., Staples, A. E., &amp; San, O. (2020). Feature engineering and symbolic regression methods for detecting hidden physics from sparse sensor observation data. Physics of Fluids, 32(1), 015113. <a href="https://doi.org/10.1063/1.5136351" target="_blank" rel="noopener">https://doi.org/10.1063/1.5136351</a> </p>
<p>[9] Xiong, B., Fu, H., Xu, F., &amp; Jin, Y. (2019). Data-driven Discovery of Partial Differential Equations for Multiple-Physics Electromagnetic Problem. arXiv e-prints, arXiv:1910.13531. <a href="https://ui.adsabs.harvard.edu/abs/2019arXiv191013531X" target="_blank" rel="noopener">https://ui.adsabs.harvard.edu/abs/2019arXiv191013531X</a> </p>
<p>[10] Singh, G., Gupta, S., Lease, M., &amp; Dawson, C. N. (2020). TIME: A Transparent, Interpretable, Model-Adaptive and Explainable Neural Network for Dynamic Physical Processes. arXiv e-prints, arXiv:2003.02426. <a href="https://ui.adsabs.harvard.edu/abs/2020arXiv200302426S" target="_blank" rel="noopener">https://ui.adsabs.harvard.edu/abs/2020arXiv200302426S</a> </p>
<p>[11] DiPietro, D. M., Xiong, S., &amp; Zhu, B. (2020). Sparse Symplectically Integrated Neural Networks. arXiv e-prints, arXiv:2006.12972. <a href="https://ui.adsabs.harvard.edu/abs/2020arXiv200612972D" target="_blank" rel="noopener">https://ui.adsabs.harvard.edu/abs/2020arXiv200612972D</a> </p>
<p>[12] Cai, S., Wang, Z., Lu, L., Zaki, T. A., &amp; Karniadakis, G. E. (2020). DeepM&amp;Mnet: Inferring the electroconvection multiphysics fields based on operator approximation by neural networks. arXiv e-prints, arXiv:2009.12935. <a href="https://ui.adsabs.harvard.edu/abs/2020arXiv200912935C" target="_blank" rel="noopener">https://ui.adsabs.harvard.edu/abs/2020arXiv200912935C</a> </p>
<p>[13] Kim, S., Lu, P. Y., Mukherjee, S., Gilbert, M., Jing, L., Čeperić, V., &amp; Soljačić, M. (2020). Integration of Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery. IEEE Transactions on Neural Networks and Learning Systems, 1-12. <a href="https://doi.org/10.1109/TNNLS.2020.3017010" target="_blank" rel="noopener">https://doi.org/10.1109/TNNLS.2020.3017010</a> </p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Machine Learning</tag>
        <tag>ODE</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读27】RODE-Net——不确定性引入与GAN的精妙引入</title>
    <url>/2020/11/21/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB27%E3%80%91RODE-Net%E2%80%94%E2%80%94%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%BC%95%E5%85%A5%E4%B8%8EGAN%E7%9A%84%E7%B2%BE%E5%A6%99%E5%BC%95%E5%85%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>许久的颓废之后重新开始看文章，这次给大家带来一首隐式meta和GAN结合的思想之歌！</p>
<p><code>RODE-Net: Learning Ordinary Differential Equations with Randomness from Data</code></p>
</blockquote>
<a id="more"></a>
<h1 id="Overview-amp-Target"><a href="#Overview-amp-Target" class="headerlink" title="Overview &amp; Target"></a>Overview &amp; Target</h1><img src="/2020/11/21/【论文阅读27】RODE-Net——不确定性引入与GAN的精妙引入/RODE-Net-1.png" title="文献概览">
<p>这是董彬老师组于2020/06挂在Arxiv上的文章，文章链接：<a href="https://arxiv.org/abs/2006.02377" target="_blank" rel="noopener">https://arxiv.org/abs/2006.02377</a></p>
<p>文献主要<strong>研究对象</strong>叫RODE，全称<code>Random ordinary differential equations</code>，指的是ODE系统中的参数看成随机变量。研究的基本问题还是从ODE的轨线（轨迹）数据学习背后的方程究竟是什么</p>
<p>不过读完了文章不是很推荐，因为走的还是拟合（如果ODE-Net基本模型有解释性那么不错）的路子。只是觉得有些思路可以借鉴</p>
<h1 id="Background-amp-Motivation"><a href="#Background-amp-Motivation" class="headerlink" title="Background &amp; Motivation"></a>Background &amp; Motivation</h1><p>文献<strong>背景</strong></p>
<ul>
<li>RODE这类方程引入了一定的随机性</li>
<li>实际数据可能有不同量纲的特征，这对拟合影响很大</li>
</ul>
<p>过去方法缺点：</p>
<ol>
<li>如果采用暴力拟合，总归是不好的</li>
<li>从实际数据获取背后的ODE系统往往需要强先验（ODE形式的归纳偏置）</li>
<li>即使加了强先验，大家过去也假设参数固定，因此只是平均意义下近似真实ODE系统；真实情况下还有随机性</li>
</ol>
<p>进一步地，文章的<strong>动机</strong>就出来了，其实就是一般的从轨线找背后ODE规律的问题，只是相当于直接把原来的ODE中参数添加先验，期以引入不确定性更符合实际，然后为了拟合加了GAN的对抗机制，请见下文</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Data-generation"><a href="#Data-generation" class="headerlink" title="Data generation"></a>Data generation</h2><p>刚开始看不懂数据是怎么采样出来的，后来明白了，数据生成的过程以文中 $(4.1)$ 节为例。先有模型参数的归纳偏置即先验假设，然后生成 $M$ 组参数，这样就有 $M$ 个ODE，选择初始点 $N_i$ 个，分别用4阶龙格-库塔法生成轨线，这样每个确定的ODE采样了 $N_i$ 条轨线，共 $M$ 个确定的ODE。所以最后得到的数据就是</p>
<script type="math/tex; mode=display">X=\{x_{\eta_i}^j(t_k)|1\leq i\leq M,\ 1\leq j\leq N_i,\ 0\leq k\leq S\} \tag{1}</script><p>其中 $(1)$ 中 $x$ 是其实是轨线向量，下标 $\eta_i$ 就是每次采样的参数，上标 $j$ 表示初值的编号，后面的 $t_k$ 就是龙格-库塔得到的不同采样时刻</p>
<blockquote>
<p>原文表述：</p>
<p>we generate data $x_{\eta_i}^j(t_k)$ by solving $ODE-\eta_i$ using the fourth order Runge-Kutta method with $N_i$ different initial values$x_{\eta_i}^j(t_0),j=1,\cdots,k$</p>
</blockquote>
<h2 id="RODE-Net"><a href="#RODE-Net" class="headerlink" title="RODE-Net"></a>RODE-Net</h2><p>文章提出的引入不确定性的模型，以及对应的网络模型称为RODE-Net。前者即：</p>
<script type="math/tex; mode=display">\displaystyle \dfrac{dx}{dt} = F_{\eta}(x), x=(x_1,\cdots,x_d)\in \mathbb{R}^d,t\geq 0\tag{2}</script><p>其中 $\eta$ 就是假定有先验分布的参数。后者RODE-Net的结构图如下：</p>
<img src="/2020/11/21/【论文阅读27】RODE-Net——不确定性引入与GAN的精妙引入/RODE-Net-2.png" title="RODE-Net">
<blockquote>
<p>ps：上面的先验假设应该是可以嵌套的</p>
</blockquote>
<p>这个结构里有如下亮点：</p>
<ul>
<li>中间的ODE-Net其实就可以是以往的暴力拟合框架，这里文章用了所谓的SymNet中包含了符号运算，内蕴了dynamic操作，因此某种程度上有可解释性。这个SymNet我不知道具体是怎么进行符号运算的，但是我推测是他们组PDE-Net2.0中卷积相关的操作，这个可以；但是后文提到引入GAN的目的之一是避免符号预算的求导，我就<strong>不懂</strong>了，既然符号运算不便求导，那主网络是怎么BP的？</li>
<li>基础模型ODE-Net之外引入GAN，目的之一是避免SymNet中符号运算求导，目的之二是提供原网络参数更好的监督，进而形成目的三是作为了原网络的正则。联想到之前的ODE2VAE应该是有类似的考虑！</li>
<li><strong>引入GAN的同时有meta-learning的味道，体现在GAN的double使用，生成之后反过来指导主网络的训练</strong>，类比一下是不是所有模型与GAN嵌套都有这个作用，生成的数据质量越来越高，乃至接近于meta数据，大概也是为什么GAN效果好的解释之一？</li>
</ul>
<p>RODE-Net中具体用的ODE-Net主模型就是PDE-Net中变过来的带有符号运算性质的网络；GAN用了WGAN（Wasserstein GAN），细节应该问题不大</p>
<p>最后讲一下整个RODE-Net是怎么训练的以及其目标函数，从这里感觉，其训练方式是有meta-learning的味道的，这个GAN的double使用，但是<strong>loss上看还是所谓的外练筋骨皮</strong></p>
<p>训练的过程很像meta方法。先是主网络正常训练，有数据点拟合的监督和对应于异常点不容忍的Huber正则；第二步是第一步得到的模型参数生成，扔到WGAN里训练一下，期中也加了一个梯度惩罚正则，即WGAN-GP；第三步是WGAN返回来作为主模型的正则，希望对抗前后主模型的参数不要相差过大，这样提供了一种整体上的控制</p>
<blockquote>
<p>Huber损失介绍，介于 $L_1$ 和 $L_2$ 之间，思想是调整对异常的容忍程度，因此有额外的一个超参数，很棒，可以日后借鉴：</p>
<p>知乎用户<a href="https://www.zhihu.com/org/jing-lue-ji-zhi" target="_blank" rel="noopener">景略集智</a>的专栏文章<a href="https://zhuanlan.zhihu.com/p/39239829" target="_blank" rel="noopener">机器学习从业者必知的5种回归损失函数</a>，细节满满</p>
<p>博客园用户<a href="https://www.cnblogs.com/nowgood/" target="_blank" rel="noopener">nowgood</a>的博客<a href="https://www.cnblogs.com/nowgood/p/Huber-Loss.html" target="_blank" rel="noopener">Huber Loss</a>，主要是函数的介绍</p>
</blockquote>
<h1 id="Pros-amp-Cons"><a href="#Pros-amp-Cons" class="headerlink" title="Pros &amp; Cons"></a>Pros &amp; Cons</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Pros</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>RODE建模的不确定性是指参数本身的不确定性，实际意义挺好</td>
<td>文章有一处不懂的：既然符号运算不便求导而引入GAN，那主网络是怎么BP的</td>
</tr>
<tr>
<td>GAN交替使用，相当于用两个GAN，有meta的味道</td>
<td>这里的多次正则也只是相对于ob数据的，没有真正意义上的泛化</td>
</tr>
<tr>
<td>GAN作为数据驱动正则的思想（目前比 $L_1$ 效果上好）</td>
<td>文章最后提了一句先验引入可以使高维参数降维，我寻思着不是一般升维了…</td>
</tr>
<tr>
<td>GAN和RODE的”R”天然契合，因为GAN本身就是参数分布的一种估计</td>
<td>文章最后说non-transparency，确实某种程度上没有建模深层次的dynamic性质（符号运算？）</td>
</tr>
</tbody>
</table>
</div>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>这次会用EndNote了，试试APA 7th格式，其它参考链接见文中注释部分</p>
<p>[1] Liu, J., Long, Z., Wang, R., Sun, J., &amp; Dong, B. (2020). RODE-Net: Learning Ordinary Differential Equations with Randomness from Data. arXiv:2006.02377. Retrieved June 01, 2020, from <a href="https://ui.adsabs.harvard.edu/abs/2020arXiv200602377L" target="_blank" rel="noopener">https://ui.adsabs.harvard.edu/abs/2020arXiv200602377L</a></p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>ODE</tag>
        <tag>Regularization</tag>
      </tags>
  </entry>
  <entry>
    <title>MLA‘20见闻</title>
    <url>/2020/11/10/MLA%E2%80%9820%E8%A7%81%E9%97%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>MLA‘20之行让我学到了很多，包括领域知识，对学习生活的前瞻，以及更多对当下学习困境的思考</p>
<p>故创文记之，以思故我。就当是小小随笔</p>
<p>不太好写啊！干脆随便总结下吧，细节留在网盘里就好</p>
</blockquote>
<a id="more"></a>
<img src="/2020/11/10/MLA‘20见闻/MLA-1.JPG" title="MLA" alt="20开冲！">
<p>会议日程 <a href="http://www.lamda.nju.edu.cn/conf/mla20/program.html" target="_blank" rel="noopener">http://www.lamda.nju.edu.cn/conf/mla20/program.html</a></p>
<h1 id="流水账"><a href="#流水账" class="headerlink" title="流水账"></a>流水账</h1><h2 id="第一天上午"><a href="#第一天上午" class="headerlink" title="第一天上午"></a>第一天上午</h2><p>第一场，北大王立威教授介绍深度学习，主要提到了泛化理论的新解释。</p>
<p>先介绍了下深度学习的方向：</p>
<pre class="mermaid">graph LR
A(深度学习) -->B(模型/结构)
A(深度学习) -->C(训练/优化)
A(深度学习) -->D(泛化/测试优化)
B -->E[CNN]
B -->F[RNN]
B -->G[LPWHW, 2017]
B -->H[Cybenko, 1998]</pre>

<blockquote>
<p>ps：画图的时候要选择mermaid而不是flow，否则报错</p>
<p><code>ERROR: [Flowchart] Cannot set property &#39;next&#39; of undefined</code></p>
<p>ps2：mermaid在hexo中需要安装包才能使用，且里面所有的操作都要加上 <a href="https://www.dazhuanlan.com/2019/11/20/5dd4ec5f4f552/" target="_blank" rel="noopener">https://www.dazhuanlan.com/2019/11/20/5dd4ec5f4f552/</a> <a href="https://tyloafer.github.io/posts/7790/" target="_blank" rel="noopener">https://tyloafer.github.io/posts/7790/</a></p>
</blockquote>
<p>重点提到了学习理论的泛化解释可能是有问题的。</p>
<img src="/2020/11/10/MLA‘20见闻/IMG_1996.JPG" title="过参数化而非过拟合">
<p>即过拟合并不是过拟合，而是过参数化，过参数化不会导致泛化性能的大幅下降，还能保持一定的泛化能力。</p>
<p>然后讲了优化，一个小背景是即便是4阶多项式局部极小的优化也是NP难的问题。然后有个结论是一般的SGD方法在绝大多数方法上都work well，为什么？2019年ICML的DLLWZ证明了梯度下降GD可以以一个线性收敛速度（in a linear convergence rate）找到全局极小，但是有两个条件（没条件显然不太对头），一是网络要足够宽（即足够过参数化），然后参数的初始化选择Gauss，选择其中的协方差参数，最后选合适的step size。ResNet的确更好。</p>
<p>当网络无限宽时，网络会退化成kernel machine。</p>
<img src="/2020/11/10/MLA‘20见闻/MLA-3.JPG" title="神经网络的拟合能力">
<p>方法的一个insight是，在这样的初始点附近，其邻域的性质好，存在一个所谓内在的全局极优？</p>
<img src="/2020/11/10/MLA‘20见闻/IMG_1998.JPG" title="DLLWZ, 2019">
<p>最后有个文章列表</p>
<img src="/2020/11/10/MLA‘20见闻/IMG_2011.JPG" title="老师组内文章列表（仅参考）">
<hr>
<p>第二位老师是UCLA的印卧涛，主题是Faster Learning over Networks and Opensource Framework BlueFog，介绍大规模模型。</p>
<p>大规模数据或者模型的处理方式是</p>
<pre class="mermaid">graph LR
A(大规模) -->B(数据并行)
A(深度学习) -->C(模型并行)
A(深度学习) -->D(模型分割后并行?)</pre>

<p>其中一种方案是parameter server approach，但不适合大规模。方式是一张卡和多张卡之间传输数据，但是n21似乎不太好整合。</p>
<pre class="mermaid">graph LR
A(大卡) -->B(卡1)
A(大卡) -.1对多.->C(...)
A(大卡) -->D(卡n)
E(左到右:1对多<br>右到左:多对1)</pre>

<p>报告中指出一个方案是环上的整合，叫ring allreduce，缺点是同一时刻edge只有一个</p>
<p>然后推荐了他们自己的framework：bluefog，好像几行代码能完成相关工作。后面没听了</p>
<hr>
<p>第三场是微软亚洲研究院的秦涛讲神经语言合成。</p>
<pre class="mermaid">graph LR
A(神经语言合成) -->B(拼接方法<br>比如concatenative TTS<br>缺点是自由度低)
A -->C(参数化方法<br>parametric TTS<br>优点是灵活,缺点是可能不自然)
A -->D(神经网络方法<br>更前沿就对了)
%%E(左到右:1对多<br>右到左:多对1)</pre>

<p>然后倒是讲了不少neural方法的例子：</p>
<ul>
<li>Deep Voice 3(ICLR 2018)</li>
<li>Tacotron 2(LSTM)</li>
<li>Transformer TTS(AAAI 2019)</li>
<li>Fast Speech(NIPS 2019)</li>
</ul>
<hr>
<p>第四场是是顶会交流介绍。列表参考 <a href="http://www.lamda.nju.edu.cn/conf/mla20/poster.html#spotlight%2011_07" target="_blank" rel="noopener">http://www.lamda.nju.edu.cn/conf/mla20/poster.html#spotlight%2011_07</a> ，有需求的时候自取。</p>
<p>其中我自己想看的列在这里：</p>
<ul>
<li>王红师姐的模型驱动+数据驱动</li>
<li>清华的Understanding and Exploring the Network with Stochastic Architectures</li>
</ul>
<h2 id="第一天下午"><a href="#第一天下午" class="headerlink" title="第一天下午"></a>第一天下午</h2><p>第五场是孙剑介绍CV前沿，自己的笔记记得挺多的。太多了所以不想整理了喵。不过注意他这里面讲得非常全的，有几个我感兴趣的，比如过参数化的解释、shuffleNet特征图间操作、目标检测的一些东西，etc。</p>
<hr>
<p>第六场是南大吴建鑫介绍CNN压缩。一个背景是小设备上需要轻量网络。一般的方法是剪枝，总结如下：</p>
<pre class="mermaid">graph LR
A(网络剪枝) -->B(剪枝基本方法<br>网络权重低于某阈值时就压缩<br>缺点是使模型非结构化,难以加速)
B --> E(一篇文章作为例子 Han Song NIPS 2015)
A -->C(结构化剪枝<br>即保持网络形状不变)
C --> F(剪哪些<br>idea是剪一部分,后面对应的net也会受影响)
C --> G(怎么剪<br>具体没听清楚,大概是数学优化的角度做reconstruction,最小化下一层的啥目标)
%%E(左到右:1对多<br>右到左:多对1)</pre>

<p>上面是以往设计的压缩剪枝方式，总结的话就是人工剪枝三段式：确定权重/重要性，剪掉，再微调网络参数。现在呢，有人在做自动压缩的方法，咋做没记笔记，大概是auto-pruner，另外还有人做ResNet剪枝、小数据任务上的剪枝，等等。</p>
<hr>
<p>第七场是复旦黄萱菁讲中文命名实体识别，感觉是NLP方法的一些变种，具体没有咋听。</p>
<hr>
<p>第八场是孟老师操作一手，讲模型驱动与数据驱动之间的差异、联系、结合。</p>
<p>以图像去噪为例，模型驱动主要是生成方式的理解， $\displaystyle \arg\min_{z} L(Y-z)+R(z)$ ，其中 $Y$ 是带噪图， $z$ 是潜在的干净图，所以 $Y-z$ 其实就相当于噪声；数据驱动则直接网络拟合干净图，</p>
<script type="math/tex; mode=display">\displaystyle \arg\min_{w} ||z-net_w (Y)||_2</script><p>其中 $z$ 可以是提供的真实干净图，但也可以去拟合噪声啊，这样就是 </p>
<script type="math/tex; mode=display">\displaystyle \arg\min_{w} ||\varepsilon-ResNet_w (Y)||_2</script><p>但是数据驱动的确是黑箱操作，缺点是解释性相对较差，无生成功能，且依赖于监督标记。</p>
<p>孟老师操作了一手模型、数据驱动结合的理解，二者能否互补？由此孟老师来了一手外练筋骨皮、内练一口气、万法归宗的比喻，具体放在图里看一下：</p>
<pre class="mermaid">graph LR
A(模型驱动&数据驱动) -->B(外练筋骨皮<br>需要知道/建模数据是怎么走的)
A -->C(内练一口气<br>网络结构需要有可解释性)
A -->D(万法归宗<br>数据驱动学习生成规律?)

B -.paper1.-> E(MAP的, TPAMI 18, 讲CT的,链接放在文末)
B -.paper2.-> F(王红大佬的去雨模型<br>ps:我怎么觉得这个是内练?可能笔记记错了)

C -.idea.-> G(idea<br>网络由黑箱隐式函数到显示函数<br>优化算法解释为网络结构的迭加形式)
C -.e.g..-> H(deep unfolding就是其中一类方法)
%%E(左到右:1对多<br>右到左:多对1)</pre>

<hr>
<p>第九场也非常精彩！是朱军老师介绍贝叶斯深度学习，但是我的笔记没有记全😭。只记了以下几点：</p>
<ol>
<li><p>Bayes和NN的结合要回到Hopfield当年一个学生的博士论文</p>
</li>
<li><p>BNN可以与高斯过程联系起来！</p>
</li>
<li><p>Drop out也可以看/理解成是一种贝叶斯</p>
</li>
<li><p>NICE中的flow-based-model</p>
</li>
<li><p>VFlow没有拍到，但是好像非常有意思，回头搜一下</p>
<blockquote>
<p>搜到了：<em>VFlow: More Expressive Generative Flows with Variational Data Augmentation</em></p>
<p><a href="https://www.aminer.cn/pub/5e54f1813a55acae32a25d67/vflow-more-expressive-generative-flows-with-variational-data-augmentation" target="_blank" rel="noopener">https://www.aminer.cn/pub/5e54f1813a55acae32a25d67/vflow-more-expressive-generative-flows-with-variational-data-augmentation</a></p>
<p><a href="https://arxiv.org/abs/2002.09741" target="_blank" rel="noopener">https://arxiv.org/abs/2002.09741</a></p>
<p><a href="https://github.com/thu-ml/vflow" target="_blank" rel="noopener">https://github.com/thu-ml/vflow</a></p>
</blockquote>
</li>
</ol>
<hr>
<p>第十场是京东的易津峰老师介绍对抗攻击。基本的idea就是准确率不是唯一的度量模型好坏的标准，在某种程度上准确率上升，稳健性会下降。以图像任务为例就是抗像素扰动能力下降了。笔记没咋记，又一篇京东在ECV18上面的文章，有空的话浏览下吧。</p>
<blockquote>
<p>搜到了：<em>Is Robustness the Cost of Accuracy? — A Comprehensive Study on the Robustness of 18 Deep Image Classification Models</em></p>
<p><a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Dong_Su_Is_Robustness_the_ECCV_2018_paper.html" target="_blank" rel="noopener">https://openaccess.thecvf.com/content_ECCV_2018/html/Dong_Su_Is_Robustness_the_ECCV_2018_paper.html</a></p>
</blockquote>
<h2 id="第一天晚上"><a href="#第一天晚上" class="headerlink" title="第一天晚上"></a>第一天晚上</h2><p>第一天晚上结束，和同学讨论了dual-SVM中QP问题转化为minmax问题的细节哈哈哈，都忘了，好在最后推出来了</p>
<h2 id="第二天上午"><a href="#第二天上午" class="headerlink" title="第二天上午"></a>第二天上午</h2><p>第二天，第十一场，南方科技大学的姚新讲博弈、共生演化问题。总的来说这是老问题了，但是和机器学习的联系不是非常紧密，现在做这个的人很少了。其实我也不太想弄，在这里打个广告科普一下吧qwq。</p>
<p>姚老师举了很多例子：囚徒困境、协同演化的有效性验证实验、学生AA（忘了具体是什么了）问题、晚自习关灯（电量浪费）问题、农场主共用草场问题，etc。老师表示这背后其实都有数学理论，只是没时间分享了，手动狗头[doge]。理论的概览提了一下，放在下图中：</p>
<pre class="mermaid">graph LR
A(共生演化问题例子<br>老师以NIPD为例给出了许多实验结果) -->B(历史记录<br>一般问题中玩家/用户只能记住前几步的经验,记得越多结果的总体效益越好)
A -->C(有个group size<br>我忘了是啥了,反正就是越小越好)
A -->D(迭代次数影响最终结果)

B --> E(一般共生演化问题)
C --> E
D --> E

E --> F(无训练集,数据都是即时模拟生成的)
E --> G(初始化一般是随机的)
E --> H(需要环境中用户的交互)</pre>

<hr>
<p>第十二场是阿里的华先胜讲城市大规模视觉数据、交通的预测和调度。老师先广告一波，表示Ali旗下做ML的越来越多了，包括machine vision、speech、NLP、DL及其优化问题，etc。然后华老师介绍了城市大脑和交通管理安全的概念，其中需要大规模的感知认知和优化技术。一般来说城市中交通数据中的异常数据都是imbalanced data，且都有突变性，可以看作是异常检测问题，可以直接分类，方法众多，如KNN、SVR、VAR，不过这些方法有利有弊。另外，城市中红绿灯感应方面的协同调度比较复杂，也是一个重要应用。</p>
<p>最后有个东西我可以参考，应该算是重要的，我截了一张图好像，讲的是记忆卷积，是否在ODE中可以考虑前几步的数据+重加权成为meta方法？</p>
<img src="/2020/11/10/MLA‘20见闻/MLA-2.PNG" title="记忆卷积截图">
<hr>
<p>第十三场是山世光老师讲自监督学习，主要是视觉感知计算中的标记增强。自监督学习就是一种无监督，没有标记的数据，但是还是能提取出一些标记，可以说是弱标签。</p>
<blockquote>
<p>ps：师兄表示山老师是人脸识别领域中国带🐂，绕不过去的那种</p>
</blockquote>
<p>无标记的data本身不一定就是完全没有标记的，一般会有一些弱标签；预训练的迁移也一定程度上缓解了标签较弱的问题，但是缺点是过早进行分类会丢失信息（是指预训练提取的特征不够么🤔）；而任务的迁移一般是多任务，需要上下游任务之间联系；另外，对于图像任务，自然语言中的一些常识也有机会迁移过来，比如考虑图像的词法、句法，成为视觉的常识表示。标记如何增强？增广、数据再生（包括打乱、图像擦补等手段）。</p>
<h2 id="第二天下午"><a href="#第二天下午" class="headerlink" title="第二天下午"></a>第二天下午</h2><p>最后一天下午的听得都不细致了，主要是企业打广告，讲的内容比较少。主要是有一些有趣的应用，比如：</p>
<ul>
<li>英特尔的陈玉荣博士讲以人为中心的视觉分析，有个视线预测的例子很有意思</li>
<li>南大毕业的倍漾科技冯霁博士讲了金融投资的一些idea，网上发表的相关文章几乎没有核心技术，而且往往出错众多😂。提到了金融投资分几类，瞬发的高频决策，中高频的几时内的决策（割散户韭菜❓），以及低频（数月或者季度）的决策（如公司价值的跃迁）。另外，一个idea是量化交易是团队合作！自己一个人搞模型瞎投似乎不切实际</li>
<li>涂威威介绍第四范式，做各个方面应用产品的公司</li>
</ul>
<hr>
<p>最后的最后是顶会介绍，基本上全听了，不过不方便写在这里。就只说个结论，强化学习意外地成为近期最火热的方向，其次是自监督学习、对抗、表示学习等等。</p>
<h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>这次南京之行，对领域的各个方向有了进一步了解，以及了解了近期火热的方向（可惜暂时我不是很感兴趣），以及各个方向基础的一些方法，有意思的一些应用。目前磕盐吃还是磕磕自己原来感兴趣的吧。</p>
<p>其实帮助最大的还是师兄的几番谈话，对未来的打算又有了一些思考，先走一步看一步吧，11月底正式想一想接下来该怎么走。</p>
<p>另外，读文章还是慢了些，最近我要试图再加快读文章的速度、把握文章的重点并重点记录，流水账式的描述就先放放吧，先达到师兄描述的境界~</p>
<hr>
<p>回来想看的文章：</p>
<ul>
<li>前面提到的大概2-3篇</li>
<li>TPAMI文章，好像是有关模型驱动与数据驱动的结合 Kronecker-basis-representation based tensor sparsity and its applications to tensor recovery <a href="https://ieeexplore.ieee.org/document/8000407" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/8000407</a></li>
<li>去之前留的一些没看完的</li>
<li>回来之后老师推荐的一篇NIP20文章Ode to an ODE <a href="https://proceedings.neurips.cc/paper/2020/file/228669109aa3ab1b4ec06b7722efb105-Paper.pdf" target="_blank" rel="noopener">https://proceedings.neurips.cc/paper/2020/file/228669109aa3ab1b4ec06b7722efb105-Paper.pdf</a></li>
</ul>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>【文献阅读26】YoloV4</title>
    <url>/2020/11/03/%E3%80%90%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB26%E3%80%91YoloV4/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>头一次正式看目标检测方面的文章，有些功利，直接从YoloV4开始！</p>
<p><code>YOLOv4: Optimal Speed and Accuracy of Object Detection</code></p>
</blockquote>
<a id="more"></a>
<img src="/2020/11/03/【文献阅读26】YoloV4/YoloV4-1.png" title="YoloV4文献简介">
<h1 id="文献简介"><a href="#文献简介" class="headerlink" title="文献简介"></a>文献简介</h1><p>先放文献地址： <a href="https://arxiv.org/abs/2004.10934" target="_blank" rel="noopener">https://arxiv.org/abs/2004.10934</a></p>
<p>YoloV4于2020/04挂在Arxiv上，作者之一是之前Yolo代码的维护者之一，<a href="https://github.com/AlexeyAB" target="_blank" rel="noopener">Alexey Bochkovskiy</a>，暂时没有找到他的更多资料，另外两个作者是台湾中央研究院的，不认识。</p>
<h1 id="先修知识"><a href="#先修知识" class="headerlink" title="先修知识"></a>先修知识</h1><p>直接上手这篇目标检测领域的文章稍有困难，因为以前没咋接触过目标检测，现在直接来一手业界极强的文章，很多名词概念都不清楚。</p>
<p>本来打算补一补，但是发现YoloV4用的模型方法实在是有（多）些（炸）多（了），所以其实只补了文章刚开始的一些概念，码在这里：</p>
<ul>
<li><p>Pareto optimality curve，中文名帕累托最优曲线，以本文图 $(1,8)$ 为例，都是2维曲线，这是因为本文选取的评价指标（目标）有两个。我联想到了宏观经济学中的供求曲线、价格需求曲线，感觉道理有一点点像（瞎想的）。</p>
<blockquote>
<p>具体可参考知乎用户<a href="https://www.zhihu.com/people/li-lin-song-85" target="_blank" rel="noopener">木木松</a>的专栏文章<a href="https://zhuanlan.zhihu.com/p/54691447" target="_blank" rel="noopener">多目标优化之帕累托最优</a></p>
</blockquote>
</li>
<li><p>Anchor-based和Anchor-free方法，都是目标检测中的方法。前者参考下面的链接，讲得很清楚了，依次设计锚点的位置、锚框的个数大小，我猜最后是根据ground truth去判断标记框的优劣；后者似乎是没有具体设置所谓的锚点，只是先识别目标的中心，然后根据中心进行回归得到标记框（未求证）</p>
<blockquote>
<p>具体可参考知乎用户<a href="https://www.zhihu.com/people/emiya-98" target="_blank" rel="noopener">emiya</a>的专栏文章<a href="https://zhuanlan.zhihu.com/p/86741707" target="_blank" rel="noopener">Anchor-Based-01 目标检测算法设计思想一：anchor是什么</a></p>
</blockquote>
</li>
</ul>
<h1 id="YoloV4"><a href="#YoloV4" class="headerlink" title="YoloV4"></a>YoloV4</h1><h2 id="文献总结"><a href="#文献总结" class="headerlink" title="文献总结"></a>文献总结</h2><p>其实刚开始看觉得很舒服，讲的东西都非常合理有道理（后面也是），但是后面讲的名词知识太多了，让我觉得这模型不会就是<strong>各种已有的优秀方法组合拼接</strong>出来的吧！不会吧不会吧，它还<strong>真的就是这样搞的</strong>！</p>
<p>经过多篇博客总结查证，我基本上确定这文章就是这样做的，不过人家做的确实猛，优点总结如下：</p>
<ul>
<li>文章提出的YoloV4达到了文章目标（预期）：保证准确率大前提下，设计出一个<strong>快速实时轻量准确</strong>的目标识别模型。确实，本文表示<strong>只需要1张</strong>优秀的显卡就可以完成快速训练（又向应用前进一步！），并且的确是轻量实时准确的</li>
<li>在contributions部分提出可以用YoloV4的想法修饰SOTA模型使之有效轻量，应该是这个意思</li>
<li>集成了<strong>大量已有tricks</strong>，据参考文献表示至少有20多种；这篇文章的工作量十分之大，值得肯定</li>
<li>文章开头提出了一个说法：CNN任务的一个完整框架需要理论保证+实验验证。本文的tricks可以说是验证了的东西，然后在多个数据集上也显示出帕累托最优的效果</li>
</ul>
<h2 id="主要tricks"><a href="#主要tricks" class="headerlink" title="主要tricks"></a>主要tricks</h2><p>说tricks之前忘了提一嘴YoloV4的整体网络框架了，和经典的目标识别检测框架一致，有一个</p>
<script type="math/tex; mode=display">input \rightarrow backbone \rightarrow neck \rightarrow head(dense\ prediction \rightarrow sparse\ prediction)</script><p>的整体框架，其中某些部分不知道有没有跨连接的操作。下面放一张原文的图 $(2)$ ：</p>
<img src="/2020/11/03/【文献阅读26】YoloV4/YoloV4-2.png" title="目标检测框架">
<p>那么YoloV4其实就说各种tricks，或者说是<strong>各种模型插件</strong>套进这个框架。</p>
<p>这个我其实没有细看文章了，毕竟陌生名词太多，搞不清哪些是重点，这里根据他人博客自己总结其中有用的tricks：</p>
<ul>
<li><p>首先有个idea起源于Yolo系列的任务是目标识别检测。目标检测方法往往是任务special的，特定任务要特定设计，在通用模型没有发展起来并大规模应用之前我是站这个东西的。本文希望能做到通用的general目标检测模型，那么一个idea就是special特征和general特征，YoloV4在网络模块的使用上<strong>主动选择了很多general特征</strong>（我认为是特征图）</p>
</li>
<li><p>为了轻量实时，<strong>选择Yolo系列</strong>的模型是当前合适的做法，本文在head区域使用了YoloV3</p>
</li>
<li><p><strong>各种数据增强技术</strong>的使用</p>
</li>
<li><p><strong>自对抗方法增强图像风格</strong>，我觉得可以算数据增强了</p>
</li>
<li><p>大佬们还总结了“跨最小批的归一化（Cross mini-batch Normal）、修改的SAM（逐点attention）、修改的PAN（通道合并方式改变）”，不过我现在不太懂，先码在这里。这里最后三条参考：</p>
<blockquote>
<p>知乎用户<a href="https://www.zhihu.com/people/william.hyin" target="_blank" rel="noopener">william</a>的专栏文章<a href="https://zhuanlan.zhihu.com/p/161083602" target="_blank" rel="noopener">一文读懂YOLO V5 与 YOLO V4</a></p>
<p>知乎用户<a href="https://www.zhihu.com/people/jianghongliang" target="_blank" rel="noopener">元峰</a>的回答<a href="https://www.zhihu.com/question/390191723/answer/1177584901" target="_blank" rel="noopener">如何评价新出的YOLO v4 ？</a>不错，从创新性的角度分析了YoloV4使用的主要tricks</p>
</blockquote>
</li>
</ul>
<h2 id="个人感想（bb）"><a href="#个人感想（bb）" class="headerlink" title="个人感想（bb）"></a>个人感想（bb）</h2><p>本来头一遍翻完这篇文章觉得还挺ysk（😁）的，但是抱着这个怀疑我去读了他人的总结，包括：</p>
<blockquote>
<p>知乎用户<a href="https://www.zhihu.com/people/amusi1994" target="_blank" rel="noopener">Amusi</a>的回答<a href="https://www.zhihu.com/question/390191723/answer/1176986665" target="_blank" rel="noopener">如何评价新出的YOLO v4 ？</a>很好，我很赞同。人家工作量在这里，确实很良心</p>
<p>知乎用户<a href="https://www.zhihu.com/people/david-12-84-45" target="_blank" rel="noopener">David</a>的专栏文章<a href="https://zhuanlan.zhihu.com/p/136172670" target="_blank" rel="noopener">YOLO-V4解读：速度与精度的完美结合[已开源]</a>是一篇简洁的翻译，可以速读参考</p>
<p>补充资料：知乎用户<a href="https://www.zhihu.com/people/magic-frog-sjtu" target="_blank" rel="noopener">996黄金一代</a>的回答<a href="https://www.zhihu.com/question/399884529/answer/1374024055" target="_blank" rel="noopener">如何评价YOLOv5？</a></p>
</blockquote>
<p>我这才有些缓过来，人家毕竟是总结了大量方法，有充足的工作量，而且效果也上去了，那的确是不错的。大家反驳之的意见是似乎大家应用还是不用它，用YoloV3，大概是觉得它这样相当于调参？不过人家做了很多实验，在不少数据集上也足够优秀了，那我是没话说了。</p>
<p>这种套方法的方式，可取，但是得有充足的调研，充足的实验经验。加油吧骚年！</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection, 2020.</p>
<p>[2] emiya. Anchor-Based-01 目标检测算法设计思想一：anchor是什么[EB/OL]. <a href="https://zhuanlan.zhihu.com/p/86741707" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/86741707</a>, 2019-10-26.</p>
<p>[3] 木木松. 多目标优化之帕累托最优[EB/OL]. <a href="https://zhuanlan.zhihu.com/p/54691447" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/54691447</a>, 2020-10-07.</p>
<p>[4] william. 一文读懂YOLO V5 与 YOLO V4[EB/OL]. <a href="https://zhuanlan.zhihu.com/p/161083602" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/161083602</a>, 2020-07-28.<br>[5] 元峰. 如何评价新出的YOLO v4 ？[EB/OL]. <a href="https://www.zhihu.com/question/390191723/answer/1177584901" target="_blank" rel="noopener">https://www.zhihu.com/question/390191723/answer/1177584901</a>, 2020-04-25.<br>[6] Amusi. 如何评价新出的YOLO v4 ？[EB/OL]. <a href="https://www.zhihu.com/question/390191723/answer/1176986665" target="_blank" rel="noopener">https://www.zhihu.com/question/390191723/answer/1176986665</a>, 2020-04-24.<br>[7] David. YOLO-V4解读：速度与精度的完美结合[已开源][EB/OL]. <a href="https://zhuanlan.zhihu.com/p/136172670" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/136172670</a>, 2020-05-20.<br>[8] 996黄金一代. 如何评价YOLOv5？[EB/OL]. <a href="https://www.zhihu.com/question/399884529/answer/1374024055" target="_blank" rel="noopener">https://www.zhihu.com/question/399884529/answer/1374024055</a>, 2020-07-31.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>CNN</tag>
        <tag>Computer Vision</tag>
        <tag>Yolo</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读25】HollowNet——NN对微分算子的模拟</title>
    <url>/2020/10/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB25%E3%80%91HollowNet%E2%80%94%E2%80%94NN%E5%AF%B9%E5%BE%AE%E5%88%86%E7%AE%97%E5%AD%90%E7%9A%84%E6%A8%A1%E6%8B%9F/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>这篇文章标题并没有提到HollowNet，标题为<em>Neural Networks with Cheap Differential Operators</em></p>
<p>是去年（19）NIPS，由著名的<a href="http://www.cs.toronto.edu/~rtqichen/" target="_blank" rel="noopener">陈天琦</a>写的文章</p>
</blockquote>
<a id="more"></a>
<img src="/2020/10/24/【论文阅读25】HollowNet——NN对微分算子的模拟/HollowNet-1.png" title="文献预览">
<p>文献链接： <a href="https://papers.nips.cc/paper/9187-neural-networks-with-cheap-differential-operators.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/9187-neural-networks-with-cheap-differential-operators.pdf</a> ，更详细的地址在 <a href="http://papers.nips.cc/paper/9187-neural-networks-with-cheap-differential-operators" target="_blank" rel="noopener">http://papers.nips.cc/paper/9187-neural-networks-with-cheap-differential-operators</a></p>
<p>这篇文章我其实没有读透，文中的 $(6)$ 式我还没懂，而且我喜欢挑刺，我总觉得文章里很多记号写得不好不统一…不过问题也不大，记个笔记就好啦~</p>
<h1 id="Why-HollowNet？——背景"><a href="#Why-HollowNet？——背景" class="headerlink" title="Why HollowNet？——背景"></a>Why HollowNet？——背景</h1><p>本文的标题没有提及HollowNet，但是其实说明了文章做了什么，即用NN来模拟微分算子，只不过模拟的微分算子可能比较简单，所以说是cheap的。</p>
<p>那么文章是怎么考虑到要用NN模拟微分算子的呢？（我塔喵其实就是在想这个问题！）</p>
<img src="/2020/10/24/【论文阅读25】HollowNet——NN对微分算子的模拟/TAMIAO.jpg" title="我的塔喵是巴萨卡🐷~">
<p>其实没什么可考虑的，就是想模拟微分算子嘛，但是微分算子比较复杂，本身就是抽象的数学记号，而且还有各种微妙的复杂性，如对时间、或者是对空间等变量的<strong>变化</strong>的描述。</p>
<p>这篇文章就是基于此想法，提出了HollowNet，它完成的任务是对输入为向量 $\mathbf{x}$ 的向量函数 $\mathbf{f}(\mathbf{x})$ 求对 $\mathbf{x}$ 的单变量 $x_j$ （以第 $j$ 元为例）的高阶偏导数，即 $\displaystyle \dfrac{\partial^k \mathbf{f}(\mathbf{x})}{\partial x_j^k}$ 。文中称之为<font color="#0000FF">dimension-wise k-th order derivatives</font>。</p>
<h1 id="How-HollowNet？——挖空NN"><a href="#How-HollowNet？——挖空NN" class="headerlink" title="How HollowNet？——挖空NN"></a>How HollowNet？——挖空NN</h1><p>这里要先声明文章完成的<strong>任务</strong>是模拟了<font color="#0000FF">dimension-wise k-th order derivatives</font> $\displaystyle \dfrac{\partial^k \mathbf{f}(\mathbf{x})}{\partial \mathbf{x}_j^k}$ ，其中 $\mathbf{f}:\mathbb{R}^d\rightarrow \mathbb{R}^d$ ，<font color="#0000FF">dimension-wise k-th order derivatives</font>为：</p>
<script type="math/tex; mode=display">\displaystyle \mathcal{D}_{dim}^k(\mathbb{f}) = \dfrac{\partial^k \mathbf{f}(\mathbf{x})}{\partial \mathbf{x}_j^k} = \left[\dfrac{\partial^k f_1(\mathbf{x})}{\partial \mathbf{x}_1^k}\ \dfrac{\partial^k f_2(\mathbf{x})}{\partial \mathbf{x}_2^k}\ \cdots,\ \dfrac{\partial^k f_d(\mathbf{x})}{\partial \mathbf{x}_d^k} \right]^T\in \mathbb{R}^d</script><p>文章这里就有一些小问题，这些真的是dimension-wise的，如果需要混合偏导，比如 $\dfrac{\partial^k f_i(\mathbf{x})}{\partial x_j^k},\ where\ i\neq j$ ，那这个式子就无能为力了，这大概也是为什么文章标题说是“cheap”的微分算子</p>
<hr>
<p>至于具体怎么用网络实现，话不多说，日常上图：</p>
<img src="/2020/10/24/【论文阅读25】HollowNet——NN对微分算子的模拟/HollowNet-2.png" title="HollowNet结构">
<p><strong>左图是完整的前向计算过程</strong>，这里为了求 $f_k$ 对 $x_k$ 的偏导数，先由除了 $x_k$ 之外的所有分量 $\mathbf{x}_{-k} \triangleq \{x_1, x_2, \cdots, x_d\}/ \{x_k\}$ ，计算一个 $h_k = c_k(\mathbf{x}_{-k})$ ，其中 $h_k$ 不一定是标量，因为标量的信息一般不够描述高阶导数，一般是向量，这里先假定为 $d_h(\forall k)$ 维，然后这个 $h_k$ 和 $x_k$ 再作为输入得到函数值分量 $f_k=\tau_k(h_k,x_k)$ 。这样设计的目的是为了方便后面挖空网络之后，函数值分量 $f_k$ 只保留对输入分量 $x_k$ 的偏导数。</p>
<p>那么<strong>右图就是挖空计算图的操作</strong>了。由左图的设计，只要在编程中通过<code>detach</code>一类的操作断掉 $f_k$ 和 $h_k$ 之间的联系，那么在计算图中就只保留了 $\dfrac{\partial f_k}{\partial x_k}$ ，对其它分量的偏导数一律为0。</p>
<p>在左图中，从输入变量的分量 $\mathbf{x}_{-k}$ 到中间变量 $h_k$ 的过程叫做<code>conditioner</code>，表示这种特殊设计的条件关系；从中间变量 $h_k$ 到最终函数分量 $f_k$ 的过程则叫做<code>transformer</code>，这个不太好解释，反正是这么个说法。</p>
<p>有了这个挖空计算图的操作还不够，因为显然我们会注意到 $f_k$ 本身还是由所有分量计算得到的，单独切掉 $\mathbf{x}_{-i}$ 对应的计算图，那么如果有对 $f_k$ 的后续计算，比如计算损失函数 $c$ ，那么就有 $\displaystyle \dfrac{\partial c}{x_k} = \sum_j \dfrac{\partial c}{\partial f_j}\dfrac{\partial f_j}{\partial x_k}$ ，而这样的话相当于在计算图中，右边只能保留一项梯度 $\dfrac{\partial c}{\partial f_k}\dfrac{\partial f_k}{\partial x_k}$ ，这和我们的认知不符，我认为文章<strong>基于这个考虑对挖空的计算图做了一个补全</strong>，即文章的 $(6)$ 式，我自己学了点计算图想推一下，但是<strong>应该是推错了</strong>，过程记录如下：</p>
<script type="math/tex; mode=display">\displaystyle \begin{aligned} \dfrac{\partial \mathcal{D}_{dim}(\mathbb{f})}{\partial w} &= \sum_j\dfrac{\partial \mathcal{D}_{dim}^k(\mathbb{f})}{\partial h_j} \dfrac{\partial h_j}{\partial w} = \sum_j\left(\dfrac{\partial \mathcal{D}_{dim}^k(\mathbb{f})}{\partial \mathcal{D}_{dim}^k(\hat{\mathbb{f}})}\dfrac{\partial \mathcal{D}_{dim}^k(\hat{\mathbb{f}})}{\partial h_j}\right) \dfrac{\partial h_j}{\partial w}\\ &= \sum_j\left(\dfrac{\partial \mathcal{D}_{dim}^k(\mathbb{f})}{\partial \mathcal{D}_{dim}^k(\hat{\mathbb{f}})} \left(\dfrac{\partial \mathcal{D}_{dim}^k(\hat{\mathbb{f}}_j)}{\partial h_j} \bigoplus \dfrac{\partial \mathcal{D}_{dim}^k(\hat{\mathbb{f}}_{-j})}{\partial h_j}\right)\right) \dfrac{\partial h_j}{\partial w}\\ &?= \sum_j\left(\dfrac{\partial \mathcal{D}_{dim}^k(\mathbb{f})}{\partial \mathcal{D}_{dim}^k(\hat{\mathbb{f}})} \left(\dfrac{\partial \mathcal{D}_{dim}^k(\hat{\mathbb{f}})}{\partial \hat{h}} \bigoplus \dfrac{\partial \mathcal{D}_{dim}^k(\hat{\mathbb{f}}_{-j})}{\partial h_j}\right)\right) \dfrac{\partial h_j}{\partial w}\\ &?= \dfrac{\partial \mathcal{D}_{dim}^k(\hat{\mathbb{f}})}{\partial \hat{h}}\dfrac{\partial h}{\partial w} + \dfrac{\partial \mathcal{D}_{dim}^k(\hat{\mathbb{f}})}{\partial w}\end{aligned}</script><font color="#FF0000"> 最后两行我是懵了，看了点计算图还是不懂，先放在这里了</font>

<h1 id="Pros-amp-Cons"><a href="#Pros-amp-Cons" class="headerlink" title="Pros &amp; Cons"></a>Pros &amp; Cons</h1><p>讲方法优劣之前，其实看了一点点实验，为文章的 $(5.2)$ 节，大概就是说对于一个随机微分方程，这个HollowNet模拟得不错</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>cheap微分算子的成功模拟❕</td>
<td>只构建了cheap微分算子，维度-wise的，那么混合偏导等算子呢</td>
</tr>
<tr>
<td>HollowNet构建方式和我想的不一样，这也是我可以借鉴的地方之一</td>
<td>原文$(2.1)$ 节提到 $d_h$ 要小一些，这样可以大量减少计算量，但是这样维度较小，信息流足够么❓</td>
</tr>
<tr>
<td>HollowNet没有太多对模型的限制，之前很多流模型，包括NICE等都有很多限制（原文 $(2.1)$ 节）</td>
<td>原文 $(6)$ 式的计算图补偿方式讲得不洗，没看懂哇，是不是和图 $(1)$ 提到的对角分解有关❓</td>
</tr>
<tr>
<td></td>
<td>原文的例子都看起来很复杂，有实例的一个还是SDE，为何不举像NODE中那样一般的简单例子呢❓</td>
</tr>
</tbody>
</table>
</div>
<h1 id="补充知识——学习计算图"><a href="#补充知识——学习计算图" class="headerlink" title="补充知识——学习计算图"></a>补充知识——学习计算图</h1><p>学这个是因为开始看文章的时候 $(6)$ 式不明白，所以先快速补一下</p>
<p>参考的资源是B大李宏毅深度学习(2017)课程第三讲： <a href="https://www.bilibili.com/video/BV1Ux411S7rk?p=3" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Ux411S7rk?p=3</a></p>
<h2 id="一般计算图"><a href="#一般计算图" class="headerlink" title="一般计算图"></a>一般计算图</h2><p>计算图的目的是计算梯度，一般的有效方法是BP，不过程序计算大多都是通过计算图。</p>
<p>计算图可以看成描述函数的语言，包括节点（变量）和边（运算），和PRML第8章讲得差不多。</p>
<p>举了好几个例子，都看懂了，注意一点就是每个中间变量都可以单独作为一个节点；另一点是链式法则可以根据计算图考虑，把箭头连接看成偏微分再写出链式法则的式子。</p>
<p>使用计算图的几个原因：</p>
<ol>
<li>一般我们会得到一个loss，对这个单输出求参数的偏导数，在计算图上从单输出reverse比较有效率</li>
<li>NN中往往存在参数共享的时候，同一参数可能反复使用，这时考虑从该变量到输出的所有路径，偏导加起来就好了，非常直接</li>
</ol>
<h2 id="前馈网络的计算图"><a href="#前馈网络的计算图" class="headerlink" title="前馈网络的计算图"></a>前馈网络的计算图</h2><img src="/2020/10/24/【论文阅读25】HollowNet——NN对微分算子的模拟/LHY1.png" title="前馈网络计算图表示（图源B站，侵删）">
<p>算好每条边上对应的偏导数，然后reverse计算总的偏导数就好了。不过真正的计算过程挺麻烦的，我直接截一张图了：</p>
<img src="/2020/10/24/【论文阅读25】HollowNet——NN对微分算子的模拟/LHY2.png" title="前馈网络梯度计算过程（图源B站，侵删）">
<h2 id="RNN的计算图"><a href="#RNN的计算图" class="headerlink" title="RNN的计算图"></a>RNN的计算图</h2><p>RNN都忘光了，放一张图在这里吧：</p>
<img src="/2020/10/24/【论文阅读25】HollowNet——NN对微分算子的模拟/LHY3.png" title="RNN模型（图源B站，侵删）">
<img src="/2020/10/24/【论文阅读25】HollowNet——NN对微分算子的模拟/LHY4.png" title="RNN计算图（图源B站，侵删）">
<p>行了，大概明白具体是怎么算的了</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>[1] Ricky T. Q. Chen and David K Duvenaud. Neural networks with cheap differential operators. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 9961–9971. Curran Associates, Inc., 2019.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Machine Learning</tag>
        <tag>ODE</tag>
        <tag>Flow Model</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读24】On Second Order Behaviour in ANODE——真二阶NODE</title>
    <url>/2020/10/19/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB24%E3%80%91On-Second-Order-Behaviour-in-ANODE%E2%80%94%E2%80%94%E7%9C%9F%E4%BA%8C%E9%98%B6NODE/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p><code>On Second Order Behaviour in Augmented Neural ODEs</code>，简称<strong>SONODE</strong>，似乎是2020ICML的文章但是我没有搜到…</p>
</blockquote>
<a id="more"></a>
<img src="/2020/10/19/【论文阅读24】On-Second-Order-Behaviour-in-ANODE——真二阶NODE/2nd-NODE-1.png" title="文献预览">
<p>文献链接：<a href="https://arxiv.org/abs/2006.07220" target="_blank" rel="noopener">https://arxiv.org/abs/2006.07220</a></p>
<p><code>SONODE</code>是非常新的一篇文章，发布于2020/06，是剑桥的人写的。该文是一个非常理论的东西，基本上是基于ANODE、NODE，附录有好多证明，现在还是啃不动，只能啃一些浅层的东西，记录于此</p>
<h1 id="Hook——a-simple-comparison"><a href="#Hook——a-simple-comparison" class="headerlink" title="Hook——a simple comparison"></a>Hook——a simple comparison</h1><p>之前写过一篇比较ODE2VAE和ANODE文章的比较，大概意思是说ODE2VAE中对二阶导的建模本质上可以看成ANODE在维度上进行增广的一个特例。而本文<code>SONODE</code>是我读的一篇关于ANODE<strong>对2阶导处理</strong>的文章，可以看成是两个NODE在ANODE的框架下套娃，而且比较理论了，基于原始NODE的理论进行了拓展，不过更多地和ANODE进行比较</p>
<p>笔记写得差不多了，这里再补上一些比较，原文认为SONODE不错的一个原因是它更多地考虑了实际ODE，模型的动力性质，对物理过程的描述更好一些。不过我觉得也就是多建模了一个二阶导啊，只是结合了增广维度的思想，建模的过程也没有那么物理…</p>
<h1 id="Why-SONODE？——背景"><a href="#Why-SONODE？——背景" class="headerlink" title="Why SONODE？——背景"></a>Why SONODE？——背景</h1><p>话不多说，直接上背景</p>
<p>背景之一是基于NODE、ANODE，这类模型非常不错，就是奔着描述连续动力系统去的，未来应用一定大好👍；背景之二是之前这些模型只讨论了一阶的性质，缺乏对ODE二阶导的讨论，这也是我在关注的</p>
<p>所以本文的idea很朴素，就是<font color="#0000FF">顺着ANODE推广到2阶导</font>的情形，就成为SONODE模型。不过idea朴素，方法却比较复杂，本文顺着NODE推广了它的<code>adjoint sensitivity</code>方法，这个尤其复杂，都在附录里我就不看了嘻嘻。另外，本文在得到SONODE之后做了大量实验，<font color="#0000FF">探究了许多对ANODE和SONODE的理解与比较</font>，这些东西内容满满啊！</p>
<h1 id="What’s-SONODE——实现方法"><a href="#What’s-SONODE——实现方法" class="headerlink" title="What’s SONODE——实现方法"></a>What’s SONODE——实现方法</h1><p>先回顾一下在NODE上做了维度增广的ANODE：</p>
<script type="math/tex; mode=display">\displaystyle \dfrac{d}{dt}\begin{bmatrix} \textbf{h}(t) \\ \textbf{a}(t) \end{bmatrix} = f(\begin{bmatrix} \textbf{h}(t) \\ \textbf{a}(t) \end{bmatrix}, t),\ \begin{bmatrix} \textbf{h}(t_0) \\ \textbf{a}(t_0) \end{bmatrix} = \begin{bmatrix} \textbf{h}(0) \\ \textbf{a}(0) \end{bmatrix} \triangleq \begin{bmatrix} \textbf{x} \\ \textbf{0} \end{bmatrix}</script><p>2阶NODE的建模考虑到了要拟合二阶导，SONODE就是这样一个非常暴力直观的想法，直接对二阶导建模（联想之前ODE2VAE利用贝叶斯神经网络来学习之），如下所示：</p>
<script type="math/tex; mode=display">\mathbf{h}^{\prime\prime} = \mathbf{f}^{(a)}(\mathbf{h}, \mathbf{h}^{\prime}, t, \theta_\mathbf{f}),\ where\ \mathbf{h}(t_0) = \mathbf{H}_0,\ \mathbf{h}^{\prime}(t_0) = \mathbf{g}(\mathbf{h}(t_0), \theta_\mathbf{g}) \tag{1}</script><p>其中上标 $(a)$ 表示这个网络函数 $\mathbf{f}$ 相当于是拟合加速度的函数。原始函数初值 $\mathbf{h}(t_0)$ 和一阶导初值 $\mathbf{h}^{\prime}(t_0)$ 都给定了</p>
<p>而这个暴力的建模方式相当于把ANODE的增广维度设置成一阶导 $\mathbf{h}^\prime(t)$ ：</p>
<script type="math/tex; mode=display">\displaystyle \mathbf{z} = \begin{bmatrix} \textbf{h}(t) \\ \textbf{h}^\prime(t) \end{bmatrix},\ \mathbf{z}^\prime = \dfrac{d}{dt}\begin{bmatrix} \textbf{h}(t) \\ \textbf{h}^\prime(t) \end{bmatrix} = \mathbf{f}^{(v)}(\begin{bmatrix} \textbf{h}(t) \\ \textbf{h}^\prime(t) \end{bmatrix}, t, \theta_\mathbf{f}) = \begin{bmatrix} \textbf{h}^\prime(t) \\ \mathbf{f}^{(a)}(\mathbf{h}, \mathbf{h}^{\prime}, t, \theta_\mathbf{f}) \end{bmatrix},\ where\ IV\ is\ \mathbf{z}(t_0) = \begin{bmatrix} \mathbf{H}_0 \\ \mathbf{g}(\mathbf{h}(t_0), \theta_\mathbf{g}) \end{bmatrix}\tag{2}</script><p>如上所示， $(1)$ 式和 $(2)$ 式形式上其实是一样的，即<font color="#0000FF">对二阶导的暴力建模 $\Leftrightarrow$ 两个NODE以ANODE的方式嵌套</font>！</p>
<h1 id="Theory-of-SONODE——NODE理论延拓"><a href="#Theory-of-SONODE——NODE理论延拓" class="headerlink" title="Theory of SONODE——NODE理论延拓"></a>Theory of SONODE——NODE理论延拓</h1><p>这个主要是NODE中<code>adjoint method</code>的延拓了。由于SONODE可以看作两个NODE在ANODE的框架下套娃，那么作者就考虑了一阶<code>adjoint method</code>延拓成二阶，进而辅助网络的训练</p>
<p>式子为原文命题 $(3.1)$ 但是我真心不想看证明估计也看不太明白…</p>
<p>进一步，原文给出了这个<code>adjoint method</code>的计算性质比较，即原文命题 $(3.2)$ ，说的是计算复杂度的问题，只用两组一阶<code>adjoint method</code>计算要优于用二阶的<code>adjoint method</code>，后者应该是需要更多的矩阵乘法运算。因此，文中的实验都是准备用两组一阶<code>adjoint method</code>来计算的</p>
<h1 id="How-to-Use-SONODE——Experiments"><a href="#How-to-Use-SONODE——Experiments" class="headerlink" title="How to Use SONODE——Experiments"></a>How to Use SONODE——Experiments</h1><p>这篇文章做了很多实验，我稍微整理了一下，前三个主要是为了说明SONODE的基本性质；第4个是为了说明SONODE和ANODE二阶性质的比较；后面的是为了进一步以困难数据说明二者二阶性质的比较：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验名称</th>
<th>实验简介</th>
<th>实验结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>Generalised Parity Problems</td>
<td>是原来NODE提到的相图不交问题的高维推广——高维初值问题</td>
<td>ANODE没能学到最一般的轨迹；SONODE学到了（结果如此但我不太明白）</td>
</tr>
<tr>
<td>Nested N-Spheres</td>
<td>似乎是两个N维球面嵌套的分离问题，并在流（轨）形（迹）意义下讨论</td>
<td>NODE不能在<strong>原来</strong>的实空间中分离轨迹；ANODE在增广的维度上分离了轨迹；SONODE在实空间中就做到了轨迹分（相）离（交）（不太明白原理），并提出了命题 $(3.3)$ ，SONODE不被限制在实空间上的同胚变换，毕竟实空间上就成功做到相图相交了</td>
</tr>
<tr>
<td>2 Damped Harmonic Oscillators</td>
<td>即2个衰减简谐振动轨迹的学习</td>
<td>只是说明了，ANODE扩充维度不用超过原来实空间维度，也能学到一定的二阶性质</td>
</tr>
<tr>
<td>Interpretability of ANODES</td>
<td>对一些较不规则螺旋线进行学习</td>
<td>SONODE和ANODE在该问题上的表现为：ANODE在不同初始化条件下结果不同，其实空间上轨迹一致，但增广维度上轨迹不同（有点迷）；SONODE轨迹始终一致。这说明ANODE的二阶导学习得还是可能有问题，<strong>可能</strong>不适合实际问题的应用。更进一步的结果是，原文提出命题 $(5.2, 5.3)$ ，表示ANODE可学习的非平凡二阶导形式是无限的，SONODE的则是唯一的（这么强的么？👍）</td>
</tr>
<tr>
<td>Noise Robustness</td>
<td>对 $\sin$ 函数加不同水平高斯噪声</td>
<td>图7的结果表示SONODE更稳健</td>
</tr>
<tr>
<td>Real-world Dynamic Systems</td>
<td>实际数据</td>
<td>不太明白，反正看图猛就完了</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Pros-amp-Cons"><a href="#Pros-amp-Cons" class="headerlink" title="Pros &amp; Cons"></a>Pros &amp; Cons</h1><p>写到这里总觉得文章哪里不太对劲，先这样吧，说不定用到的时候就明白了。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>本文的idea很正统，基于ANODE研究2阶性质，不仅给出了adjoint方法的延拓，还与双ANODE联系在一起</td>
<td>此类方法由于应用非常直接，确实很容易用于不好的用途。。这个问题挺人文的，我还是希望人类能和平发展技术。。</td>
</tr>
<tr>
<td>理论的保证还是比较足了，主要是adjoint方法、以及确实是在对<strong>真正</strong>的二阶导建模</td>
<td>文中提到了一手ANODE增广的维度可能发生混乱，是指学到的东西没有可解释性么，这个只提了几句没讲清楚</td>
</tr>
<tr>
<td>对比ANODE的实验做得很多，确实让我们看到了建模二阶导带来的优势</td>
<td>二阶导adjoint方法推导似乎就比较复杂，那高阶推导难道要一直嵌套么？这不是我们想要的</td>
</tr>
<tr>
<td>此一类方法的潜力都很好，应用场景必然很多，且已经出现！本文二阶方法则尤其适合存在加速度的场景</td>
<td>TBD</td>
</tr>
</tbody>
</table>
</div>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>[1] Alexander Norcliffe, Cristian Bodnar, Ben Day, Nikola Simidjievski, and Pietro Liò. On second order behaviour in augmented neural odes. arXiv preprint arXiv:2006.07220, 2020.</p>
<p>[2] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3140–3150. Curran Associates, Inc., 2019.</p>
<p>[3] Cagatay Yildiz, Markus Heinonen, and Harri Lahdesmaki. Ode2vae: Deep generative second order odes with bayesian neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 13412–13421. Curran Associates, Inc., 2019.</p>
<p>[4] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt and David Duvenaud. Neural Ordinary Differential Equations[EB/OL]. <a href="https://arxiv.org/abs/1806.07366" target="_blank" rel="noopener">https://arxiv.org/abs/1806.07366</a>, 2018.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Machine Learning</tag>
        <tag>ODE</tag>
        <tag>Flow Model</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读23】CLNN——用于图像分类的跨层神经元+CNN</title>
    <url>/2020/10/19/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB23%E3%80%91CLNN%E2%80%94%E2%80%94%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E8%B7%A8%E5%B1%82%E7%A5%9E%E7%BB%8F%E5%85%83-CNN/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>本文是我对18年Information Sciences上一篇讲网络跨层文章CLNN的理解：</p>
<p><code>Convolutional networks with cross-layer neurons for image recognition</code></p>
</blockquote>
<a id="more"></a>
<img src="/2020/10/19/【论文阅读23】CLNN——用于图像分类的跨层神经元-CNN/CLNN-1.png" title="文献预览">
<p>文献链接：<a href="https://www.sciencedirect.com/science/article/pii/S0020025517311659" target="_blank" rel="noopener">https://www.sciencedirect.com/science/article/pii/S0020025517311659</a></p>
<p>这篇文章是18年的sci一区文章，引用量似乎惨淡…这篇文章在DenseNet，等文章之后。文章的一个操作是认为自己首先提出了cross-layer的思想，而之前的只能叫做cross-line，这个概念先码在这里。</p>
<h1 id="Why-跨层？——CLNN背景"><a href="#Why-跨层？——CLNN背景" class="headerlink" title="Why 跨层？——CLNN背景"></a>Why 跨层？——CLNN背景</h1><p>本文的标题很明确，是针对图像分类识别问题去的，文章的思想来源也和该问题有关，且idea很简单，只是后面操作联想到了不同的知识。</p>
<p>背景之一是深层CNN确实表现可以，在18年那个时候能良好完成各种图像任务，包括ImageNet等；背景之二是深层CNN的训练比较难，确实，堆叠卷积层太多了会出点问题也是正常的吧（我现在不知道除了参数多，训练可能有梯度上的问题之外还有啥显著缺点）。</p>
<p>针对图像任务的话，本文的idea顺着深层CNN出来了，以文中图2为例非常好理解，CNN的底层提取的多是图像上的直观纹理特征，比较基础，高层提取的是抽象的高级特征，但是很抽象。这些与人的视觉感官类似，但是人一眼看到一张图片，需要放大图像中的细节，还是依据图像中的纹理等特征来判断图像中到底是什么。因此<font color="#0000FF">深层CNN应当要好好利用这些底层的基础特征</font>。但是显然不行，CNN过深之后，前面浅层的特征（信息）很多其实被<strong>浪费</strong>或者说逐渐消散了。</p>
<p>这就是文章要<font color="#0000FF">考虑跨层的一个原因</font>。</p>
<h1 id="How-跨层？——CLNN实现方法"><a href="#How-跨层？——CLNN实现方法" class="headerlink" title="How 跨层？——CLNN实现方法"></a>How 跨层？——CLNN实现方法</h1><p>那么它提出的是什么样的网络结构呢？这个感觉真的很难写出来，所以下面还是要放文章的图。</p>
<p>首先，文章中提到了4个重要的概念：cross-line，cross-layer neuron（=cross-line neuron），cross-layer block和connection layer。由于我不知道怎么在Hexo平台上并排插入图片，所以只好分开介绍了。</p>
<h2 id="cross-line"><a href="#cross-line" class="headerlink" title="cross-line"></a>cross-line</h2><p>据本文阐述，之前的跨层文章都是cross-line的思想，即网络层层之间连接一下，只是指这么个连接的操作来传递信息，一般通过一个卷积就行。</p>
<p>注意不是相邻层的连接（本来就有的），而是至少中间隔两层的跨连接。具体的定义在文章的定义1，不过文章的定义稍有些晦涩，其实没那么复杂，只是定义起来要晦涩一点。</p>
<h2 id="cross-layer-neuron-amp-cross-layer-block"><a href="#cross-layer-neuron-amp-cross-layer-block" class="headerlink" title="cross-layer neuron &amp; cross-layer block"></a>cross-layer neuron &amp; cross-layer block</h2><img src="/2020/10/19/【论文阅读23】CLNN——用于图像分类的跨层神经元-CNN/CLNN-2.png" title="cross-layer neuron">
<p>上图其实不是是cross-layer neuron的结构图，而是cross-layer block的结构图啊，后者作为一个模块，<font color="#0000FF">是这个跨连接一堆的整体</font>，目的是把底层的信息传到高层去，实现的手段是在普通的卷积层之间添加了cross-line，并把输入给merge一下。前者在图中只表现为一个小小的neuron，其实没啥，是指这个cross-layer的操作存在就可以了。具体的定义在文章的定义2。</p>
<p>这个merge的对象是cross-layer neuron的主（原）输入和各层跨连接得到的需要传递给顶（高）层的输入。方式有三种，保持维度后直接相加、保持维度后加权相加、跨连接的所有输出相加后再保持维度并与cross-layer neuron的主（原）输入直接合起来做维度扩充。文中为方便采用了第一种。</p>
<blockquote>
<p>我的一个疑惑，直接相加有用么，能保持底层信息么？举个例子，一堆底层的纹理加在一块是个啥？</p>
</blockquote>
<h2 id="connection-layer"><a href="#connection-layer" class="headerlink" title="connection-layer"></a>connection-layer</h2><img src="/2020/10/19/【论文阅读23】CLNN——用于图像分类的跨层神经元-CNN/CLNN-3.png" title="connection-layer">
<p>上图是connection-layer，它的作用是连接两个cross-layer block，中间有两条向高层传递信息的通道，表示两条flow，这也是为什么我在tags中加了流模型的原因，文章还有有这么个流模型的考虑的。</p>
<blockquote>
<p>ps：我觉得这个connection只是表现在两条flow的连接，分开的话可以看成是两个block</p>
</blockquote>
<p>其中每一条flow都有两层卷积，包括一次stride操作来下采样。这样就导致了上图的三种结构。</p>
<p>而这个connection-layer其实你看啊，边上有个恒等映射作为cross-line，中间为两层，所以也是个cross-layer neuron，最后有个merge的操作，采用的是第一种。</p>
<h2 id="cross-layer-block和connection-layer的连接"><a href="#cross-layer-block和connection-layer的连接" class="headerlink" title="cross-layer block和connection-layer的连接"></a>cross-layer block和connection-layer的连接</h2><img src="/2020/10/19/【论文阅读23】CLNN——用于图像分类的跨层神经元-CNN/CLNN-4.png" title="cross-layer">
<p>这个就是整体的网络模型了，把多个block和多个connection-layer组合起来，形成文章的最终模型<strong>CLNN</strong>！</p>
<p>连接的方式有两种，左边是直接顺序连接，先一堆（图中只有一个我觉得一堆也行）block再最后来个连接层；右边是调整了连接顺序，并不是直接一条路走到头了，把block中的neurons分别都连到最后的连接层上。</p>
<p>说实话我不明白为什么就可以这样做，道理只是这个信息merge传递的idea和实验验证么？</p>
<h1 id="Pros-amp-Cons"><a href="#Pros-amp-Cons" class="headerlink" title="Pros &amp; Cons"></a>Pros &amp; Cons</h1><p>写到这里总觉得文章哪里不太对劲，先这样吧，说不定用到的时候就明白了。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>idea还是不错的，跨层的花样+++（模型名字CLNN也不错）</td>
<td>我个人觉得除了实验验证少了什么</td>
</tr>
<tr>
<td>把底层信息多次传到高层，缓解了底层特征的浪费（消散）问题✔</td>
<td>跨层操作还是不少的，这样的多次利用可不可以精简？</td>
</tr>
<tr>
<td>分类任务的收敛速度加快（如何通过梯度加速还没明白），通过实验验证</td>
<td>merge特征（信息）的方式有用么？见上文我的疑惑</td>
</tr>
<tr>
<td>某种程度上CLNN也是训练深度CNN的方法</td>
<td>TBD</td>
</tr>
<tr>
<td>首次提出cross-layer？不清楚，不过确实连接方式丰富了</td>
<td></td>
</tr>
<tr>
<td>文章说梯度消失也缓和了，暂时我没啥头绪…</td>
</tr>
</tbody>
</table>
</div>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>[1] Zeng Yu, Tianrui Li, Guangchun Luo, Hamido Fujita, Ning Yu, and Yi Pan. Convolutional networks with cross-layer neurons for image recognition. Information Sciences, 433-434:241 – 254, 2018.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Machine Learning</tag>
        <tag>CNN</tag>
        <tag>Flow Model</tag>
      </tags>
  </entry>
  <entry>
    <title>【NBA2K13】掘金重新制作</title>
    <url>/2020/10/01/%E3%80%90NBA2K13%E3%80%91%E6%8E%98%E9%87%91%E9%87%8D%E6%96%B0%E5%88%B6%E4%BD%9C/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>真正的大佬一看就知道我弄的东西很菜…</p>
<p>但是小白也不会看得懂我在干啥…</p>
<p>😂</p>
</blockquote>
<a id="more"></a>

<p>前几周其实学到了新的修改技术，目前已经能够在不破坏原来球员名字database的基础上添加新的名字了，也就是说，<strong>可以添加任意长度的球员名字啦</strong>！</p>
<p>另外，对个人来说，添加面补现在也能做到不替换原始球员的面补啦！</p>
<p>基于此，决定重新制作2020版的数据包~从掘金开始</p>
<h1 id="基本数据"><a href="#基本数据" class="headerlink" title="基本数据"></a>基本数据</h1><h2 id="年龄身高设置"><a href="#年龄身高设置" class="headerlink" title="年龄身高设置"></a>年龄身高设置</h2><div class="table-container">
<table>
<thead>
<tr>
<th>年龄</th>
<th>20</th>
<th>21</th>
<th>22</th>
<th>23</th>
<th>24</th>
<th>25</th>
<th>26</th>
<th>27</th>
<th>28</th>
<th>29</th>
<th>30</th>
<th>31</th>
<th>32</th>
<th>33</th>
<th>34</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>8b 7c</td>
<td>76 7c</td>
<td>62 7c</td>
<td>5c 7c</td>
<td>4b 7c</td>
<td>35 7c</td>
<td>28 7c</td>
<td>1d 7c</td>
<td></td>
<td>f6 7b</td>
<td>e2 7b</td>
<td></td>
<td>c9 7b</td>
<td>b9 7b</td>
<td>a3 7b</td>
</tr>
<tr>
<td>身高</td>
<td>6‘1’</td>
<td>6’2’</td>
<td>6’3’</td>
<td>6’4’</td>
<td>6’5’</td>
<td>6‘6’</td>
<td>6‘7’</td>
<td>6’8’</td>
<td>6’9’</td>
<td>6’10’</td>
<td>6’11’</td>
<td>7’0’</td>
<td>7‘1’</td>
<td>7’2’</td>
<td>7‘3’</td>
</tr>
<tr>
<td></td>
<td>187.96</td>
<td>188？</td>
<td>190.5</td>
<td>193.5</td>
<td>195.58</td>
<td>198.12</td>
<td>200.66</td>
<td>205.8</td>
<td>206</td>
<td>208.28</td>
<td>？</td>
<td>215.9</td>
<td>216?</td>
<td>218.44</td>
<td>221</td>
</tr>
</tbody>
</table>
</div>
<h2 id="上场时间设置"><a href="#上场时间设置" class="headerlink" title="上场时间设置"></a>上场时间设置</h2><p>优先按stat-nba网站提供的2019-2020赛季常规赛时间，未声明即是。</p>
<p>会适当根据季后赛上场时间数据调整。</p>
<h2 id="球员名单设置"><a href="#球员名单设置" class="headerlink" title="球员名单设置"></a>球员名单设置</h2><div class="table-container">
<table>
<thead>
<tr>
<th>PG</th>
<th>SG</th>
<th>SF</th>
<th>PF</th>
<th>C</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.2kratings.com/jamal-murray" target="_blank" rel="noopener">Jamal Murray</a>/Ty Lawson（你没了）/SG</td>
<td><a href="https://www.2kratings.com/gary-harris" target="_blank" rel="noopener">Gary Harris</a></td>
<td><a href="http://www.espn.com/nba/player/_/id/4278104/michael-porter-jr" target="_blank" rel="noopener">Michael Porter Jr.</a>/Jordan Hamilton</td>
<td><a href="https://www.2kratings.com/paul-millsap" target="_blank" rel="noopener">Paul Millsap</a></td>
<td><a href="https://www.2kratings.com/nikola-jokic" target="_blank" rel="noopener">Nikola Jokic</a>/Timofey Mozgov</td>
</tr>
<tr>
<td><strong><a href="https://www.2kratings.com/monte-morris" target="_blank" rel="noopener">Monte Morris</a></strong>/Andre Miller</td>
<td><a href="https://www.2kratings.com/torrey-craig" target="_blank" rel="noopener">Torrey Craig</a></td>
<td><a href="https://www.2kratings.com/will-barton" target="_blank" rel="noopener">Will Barton</a></td>
<td><a href="https://www.2kratings.com/jerami-grant" target="_blank" rel="noopener">Jerami Grant</a>/Anthony Randolph</td>
<td><a href="https://www.2kratings.com/mason-plumlee" target="_blank" rel="noopener">Mason Plumlee</a>/Kosta Koufos</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td><a href="https://www.2kratings.com/bol-bol" target="_blank" rel="noopener">Bol Bol</a></td>
</tr>
</tbody>
</table>
</div>
<h2 id="名字代码添加"><a href="#名字代码添加" class="headerlink" title="名字代码添加"></a>名字代码添加</h2><div class="table-container">
<table>
<thead>
<tr>
<th>name</th>
<th>hex</th>
<th>address</th>
<th>dec</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nikola</td>
<td>4e696b6f6c61</td>
<td>17a82e</td>
<td>1550382</td>
</tr>
<tr>
<td>Jokic</td>
<td>4a6f6b6963</td>
<td>1795f8</td>
<td>1545720</td>
</tr>
<tr>
<td>Jamal</td>
<td>4a616d616c</td>
<td>179e94</td>
<td>1547924</td>
</tr>
<tr>
<td>Murray</td>
<td>4d7572726179</td>
<td>182710</td>
<td>1582864</td>
</tr>
<tr>
<td>Michael</td>
<td>4d69636861656c</td>
<td>1797a6</td>
<td>1546150</td>
</tr>
<tr>
<td>Porter Jr.</td>
<td>506f72746572204a722e</td>
<td>1795e2</td>
<td>1545698</td>
</tr>
<tr>
<td>Monte</td>
<td>4d6f6e7465</td>
<td>18a314</td>
<td>1614612</td>
</tr>
<tr>
<td>Morris</td>
<td>4d6f72726973</td>
<td>17a750</td>
<td>1550160</td>
</tr>
<tr>
<td>Mason</td>
<td>4d61736f6e</td>
<td>17a354</td>
<td>1549140</td>
</tr>
<tr>
<td>Plumlee</td>
<td>506c756d6c6565</td>
<td>17ac5e</td>
<td>1551454</td>
</tr>
<tr>
<td>Gary</td>
<td>47617279</td>
<td>17af3e</td>
<td>1552190</td>
</tr>
<tr>
<td>Harris</td>
<td>486172726973</td>
<td>179a26</td>
<td>1546790</td>
</tr>
<tr>
<td>Bol</td>
<td>426f6c</td>
<td>1795da</td>
<td>1545690</td>
</tr>
<tr>
<td>Will</td>
<td>57696c6c</td>
<td>17ad2c</td>
<td>1551660</td>
</tr>
<tr>
<td>Barton</td>
<td>426172746f6e</td>
<td>1795cc</td>
<td>1545676</td>
</tr>
<tr>
<td>Jerami</td>
<td>4a6572616d69</td>
<td>1795be</td>
<td>1545662</td>
</tr>
<tr>
<td>Grant</td>
<td>4772616e74</td>
<td>179eaa</td>
<td>1547946</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Nikola-Jokic"><a href="#Nikola-Jokic" class="headerlink" title="Nikola Jokic"></a>Nikola Jokic</h1><p>队内Timofey Mozgov</p>
<p>Nikola 1550382</p>
<p>Jokic 1545720</p>
<p>Timofey 17ab1c 1551132 和目标差-750</p>
<p>Mozgov 17ab0e 1551118 和目标差-5398</p>
<p>203e4位置有个2b a7 15 00 —双字15a72b—-1419051——-&gt;-5398——&gt;1413653—&gt;159215</p>
<p>203e8位置有个35 a7 15 00 —双字15a735—-1419061——-&gt;-750——&gt;1418311—&gt;15a447</p>
<p>原面补49 07=1865</p>
<p>新面补2901=55 0b</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>7‘0’=215.8999</td>
<td>253</td>
<td>55 0b</td>
<td>C</td>
<td>05</td>
<td>2020季后赛36.5=146=92</td>
<td></td>
<td>15-&gt;30-&gt;1e</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM405</th>
<th>91—&gt;88</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>20580</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  62</td>
<td>2 med  54</td>
<td>3 3pt  4f</td>
<td>4 ft 52</td>
<td>5 layup  46</td>
<td>6 dunk  55</td>
<td>28 stdnk  5b</td>
<td>23  sit 56</td>
<td>22 sod 56</td>
<td>25 hus  47</td>
</tr>
<tr>
<td>7 hndl  41</td>
<td>8  pass 50</td>
<td>32</td>
<td>40</td>
<td>9 opost  5f</td>
<td>10 dpost  58</td>
<td>11 block   3a</td>
<td>26 hnd  57</td>
<td>12 steal  36</td>
<td>15 speed  41</td>
<td>16 stam  61</td>
<td>4b</td>
<td>21 vert 60</td>
<td>13 oreb  4f</td>
<td>14 dreb  59</td>
<td>17 dur  50</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  3c</td>
<td><strong>19  oawr</strong>  62</td>
<td>19</td>
<td>50</td>
<td>27 def 50</td>
<td>24 qui 5c</td>
<td>潜力 5c</td>
<td>20 str 40</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h1 id="Jamal-Murray"><a href="#Jamal-Murray" class="headerlink" title="Jamal Murray"></a>Jamal Murray</h1><p>Ty Lawson</p>
<p>Jamal 1547924</p>
<p>Murray 182710</p>
<p>Ty 17aaa0 1551008 和目标差-3084</p>
<p>Lawson 17aa92 1550994 和目标差+31870</p>
<p>1fc64位置有个2f ae 15 00 —双字15ae2f—-1420847——-&gt;+31870——&gt;1452717—&gt;162aad</p>
<p>1fc68位置有个39 ae 15 00 —双字15ae39—-1420857——-&gt;-3084——&gt;1417773—&gt;15a22d</p>
<p>原面补4f 06=1615，用2902=56 0b</p>
<p>原来405改出来是6‘3’和82，再想办法加强一点。三分改成84</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘4’=用193—&gt;193.5吧</td>
<td>207</td>
<td>56 0b</td>
<td>PG/SG</td>
<td>04</td>
<td>2020季后赛39.6=158.4=9e</td>
<td></td>
<td>27—&gt;54—&gt;36</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM405</th>
<th>84—&gt;82</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>1fe00</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  5b</td>
<td>2 med  50—&gt;57</td>
<td>3 3pt  50—&gt;54</td>
<td>4 ft 5c</td>
<td>5 layup  52</td>
<td>6 dunk  3c</td>
<td>28 stdnk  3f</td>
<td>23  sit 3e</td>
<td>22 sod 3b</td>
<td>25 hus  42</td>
</tr>
<tr>
<td>7 hndl  58</td>
<td>8  pass 53</td>
<td>3c</td>
<td>32</td>
<td>9 opost  4b</td>
<td>10 dpost  46</td>
<td>11 block   3f</td>
<td>26 hnd  3c</td>
<td>12 steal  3b</td>
<td>15 speed  4f</td>
<td>16 stam  5a</td>
<td>3c</td>
<td>21 vert 3c</td>
<td>13 oreb  3d</td>
<td>14 dreb  3f</td>
<td>17 dur  3c</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  4d</td>
<td><strong>19  oawr</strong>  53</td>
<td>46</td>
<td>5b</td>
<td>27 def 32</td>
<td>24 qui 4f</td>
<td>潜力 56</td>
<td>20 str 42</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps:26岁=28 7c，6‘1’=187.96</p>
<h1 id="Michael-Porter-Jr"><a href="#Michael-Porter-Jr" class="headerlink" title="Michael Porter Jr."></a>Michael Porter Jr.</h1><p>用本队Jordan Hamilton，两个名字都是继承的</p>
<p>Michael 1546150</p>
<p>Porter Jr. 1545698</p>
<p>Jordan 179e4a 1547850 实际上是179e4e 1547854 和目标差-1704 4a6f7264616e</p>
<p>Hamilton 179a5c 1546844 和目标差-1146 48616d696c746f6e</p>
<p>212e4位置有个79 87 15 00 —双字158779—-1410937——-&gt;-1146——&gt;1409791—&gt;1582ff</p>
<p>212e8位置有个67 8b 15 00 —双字158b67—-1411943——-&gt;-1704——&gt;1410239—&gt;1584bf</p>
<p>原面补6f 08=2159，用2903=57 0b</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘10’用208.2799吧</td>
<td>209</td>
<td>2903=57 0b</td>
<td>SF/SG=02/01</td>
<td>04</td>
<td>2020季后赛23.7=94.8=5f</td>
<td></td>
<td>1—&gt;02</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM405</th>
<th>81</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>21480</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  47</td>
<td>2 med  4f</td>
<td>3 3pt  57</td>
<td>4 ft 53</td>
<td>5 layup  54</td>
<td>6 dunk  4b</td>
<td>28 stdnk  3c</td>
<td>23  sit 48</td>
<td>22 sod 4c</td>
<td>25 hus  3e</td>
</tr>
<tr>
<td>7 hndl  4e</td>
<td>8  pass 41</td>
<td>32</td>
<td>32</td>
<td>9 opost  4b</td>
<td>10 dpost  4e</td>
<td>11 block   44</td>
<td>26 hnd  4f</td>
<td>12 steal  32</td>
<td>15 speed  52</td>
<td>16 stam  50</td>
<td>23</td>
<td>21 vert 58</td>
<td>13 oreb  42</td>
<td>14 dreb  54</td>
<td>17 dur  55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  42</td>
<td><strong>19  oawr</strong>  4d</td>
<td>28</td>
<td>50</td>
<td>27 def 47</td>
<td>24 qui 54</td>
<td>潜力 49</td>
<td>20 str 4f</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>26岁=22 7c，6‘7’=200.66</p>
<h1 id="米尔萨普"><a href="#米尔萨普" class="headerlink" title="米尔萨普"></a>米尔萨普</h1><p>63 05=1379</p>
<p>上场时间用stat的24.5=98=62，号码4-8</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Jazz</th>
<th>KM405</th>
<th>82—&gt;</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>15ae0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  42</td>
<td>2 med  52</td>
<td>3 3pt  55</td>
<td>4 ft 59</td>
<td>5 layup  4f</td>
<td>6 dunk  49</td>
<td>28 stdnk  57</td>
<td>23  sit 51</td>
<td>22 sod 36</td>
<td>25 hus  52</td>
</tr>
<tr>
<td>7 hndl  3b</td>
<td>8  pass 3d</td>
<td>32</td>
<td>32</td>
<td>9 opost  55</td>
<td>10 dpost  52</td>
<td>11 block   40</td>
<td>26 hnd  48</td>
<td>12 steal  32</td>
<td>15 speed  3d</td>
<td>16 stam  59</td>
<td>19</td>
<td>21 vert 42</td>
<td>13 oreb  42</td>
<td>14 dreb  44</td>
<td>17 dur  5f</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  50</td>
<td><strong>19  oawr</strong>  4e</td>
<td>50</td>
<td>37</td>
<td>27 def 38</td>
<td>24 qui 36</td>
<td>潜力 50</td>
<td>20 str 49</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h1 id="Monte-Morris"><a href="#Monte-Morris" class="headerlink" title="Monte Morris"></a>Monte Morris</h1><p>Andre Miller，又他喵是继承的名字。</p>
<p>Monte 1614612</p>
<p>Morris 1550160</p>
<p>Andre 17aab8 1551032 差+63580</p>
<p>Miller 17a266 1548902 差+1258</p>
<p>20984位置有个e3 98 15 00 —双字1598e3—-1415395——-&gt;+1258——&gt;1416653—&gt;159dcd</p>
<p>20988位置有个31 a1 15 00 —双字15a131—-1417521——-&gt;+63580——&gt;1481101—&gt;16998d</p>
<p>原面补55 02=，用2906=5a 0b</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘4’=用193—&gt;193.5吧</td>
<td>207</td>
<td>2906=5a 0b</td>
<td>PG/05</td>
<td>03</td>
<td>stat 21.6=86.6=57</td>
<td></td>
<td>27—&gt;54—&gt;36</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM405</th>
<th>78</th>
<th></th>
<th>20b20</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  44</td>
<td>2 med  59</td>
<td>3 3pt  54</td>
<td>4 ft 4e</td>
<td>5 layup  4f</td>
<td>6 dunk  26</td>
<td>28 stdnk  1e</td>
<td>23  sit 43</td>
<td>22 sod 4c</td>
<td>25 hus  46</td>
</tr>
<tr>
<td>7 hndl  56</td>
<td>8  pass 5a</td>
<td>32</td>
<td>32</td>
<td>9 opost  36</td>
<td>10 dpost  40</td>
<td>11 block   1e</td>
<td>26 hnd  52</td>
<td>12 steal  32</td>
<td>15 speed  55</td>
<td>16 stam  5e</td>
<td>19</td>
<td>21 vert 58</td>
<td>13 oreb  31</td>
<td>14 dreb  28</td>
<td>17 dur  48</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  46</td>
<td><strong>19  oawr</strong>  4b</td>
<td>4b</td>
<td>54</td>
<td>27 def 49</td>
<td>24 qui 61</td>
<td>潜力 55</td>
<td>20 str 2a</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h1 id="Torrey-Craig"><a href="#Torrey-Craig" class="headerlink" title="Torrey Craig"></a>Torrey Craig</h1><p>KM405没这人…</p>
<h1 id="Jerami-Grant"><a href="#Jerami-Grant" class="headerlink" title="Jerami Grant"></a>Jerami Grant</h1><p>Anthony Randolph，全他喵是继承来的名字</p>
<p>Jerami 1545662</p>
<p>Grant 1547946</p>
<p>Anthony 416e74686f6e79 17a0a6 1548454 和目标差-2792（用别的名字找不到，咋弄。硬干，两名字之间地址差了1414163-1414463=300，第一次就找到了哈哈哈）</p>
<p>Randolph 179f76 1548150 和目标只差-204</p>
<p>20b64位置有个13 94 15 00 —双字159413—-1414163——-&gt;-204——&gt;1413959—&gt;159347</p>
<p>20b68位置有个3f 95 15 00 —双字15953f—-1414463——-&gt;-2792——&gt;1411671—&gt;158a57</p>
<p>原面补f0 05=1520，用2904=58 0b</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘8’=205.74</td>
<td>210</td>
<td>2904=58 0b</td>
<td>PF=03/05</td>
<td>02</td>
<td>2020季后赛34.4=137.6=8a</td>
<td></td>
<td>9—&gt;18—&gt;12</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>77</th>
<th></th>
<th>20d00</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  48</td>
<td>2 med  46</td>
<td>3 3pt  53</td>
<td>4 ft 46</td>
<td>5 layup  55</td>
<td>6 dunk  50</td>
<td>28 stdnk  5a</td>
<td>23  sit 4d</td>
<td>22 sod 4e</td>
<td>25 hus  57</td>
</tr>
<tr>
<td>7 hndl  3c</td>
<td>8  pass 3c</td>
<td>32</td>
<td>32</td>
<td>9 opost  46</td>
<td>10 dpost  48</td>
<td>11 block   47</td>
<td>26 hnd  3d</td>
<td>12 steal  32</td>
<td>15 speed  49</td>
<td>16 stam  63</td>
<td>19</td>
<td>21 vert 50</td>
<td>13 oreb  4b</td>
<td>14 dreb  4c</td>
<td>17 dur  3e</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  46</td>
<td><strong>19  oawr</strong>  45</td>
<td>2d</td>
<td>58</td>
<td>27 def 4a</td>
<td>24 qui 4c</td>
<td>潜力 55</td>
<td>20 str 38</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps:23岁=5b 7c，6‘7’=200.66</p>
<h1 id="Bol-Bol"><a href="#Bol-Bol" class="headerlink" title="Bol Bol"></a>Bol Bol</h1><p>Free Hamady N’diaye</p>
<p>Bol 1545690</p>
<p>Bol 1545690</p>
<p>Hamady 181f66 1580902 差-35212</p>
<p>N’diaye 181f56 1580886 差-35196</p>
<p>7b104位置有个53 6e 10 00 —双字106e53—-1076819——-&gt;-35196——&gt;1041623—&gt;0fe4d7</p>
<p>7b108位置有个5f 6e 10 00 —双字106e5f—-1076831——&gt;-35212——&gt;1041619—&gt;0fe4d3</p>
<p>原面补40 07=，用2909=5d 0b</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>7‘2’=218.44</td>
<td>207</td>
<td>2909=5d 0b</td>
<td>C</td>
<td>00</td>
<td>2020季后赛5.3=21.2=15</td>
<td></td>
<td>10—&gt;20—&gt;14</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>73</th>
<th></th>
<th>7b2a0</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  4e</td>
<td>2 med  4f</td>
<td>3 3pt  4d</td>
<td>4 ft 4b</td>
<td>5 layup  40</td>
<td>6 dunk  3c</td>
<td>28 stdnk  32</td>
<td>23  sit 44</td>
<td>22 sod 4a</td>
<td>25 hus  46</td>
</tr>
<tr>
<td>7 hndl  43</td>
<td>8  pass 1b</td>
<td>32</td>
<td>32</td>
<td>9 opost  3c</td>
<td>10 dpost  3d</td>
<td>11 block   56</td>
<td>26 hnd  54</td>
<td>12 steal  21</td>
<td>15 speed  46</td>
<td>16 stam  5f</td>
<td>19</td>
<td>21 vert 46</td>
<td>13 oreb  43</td>
<td>14 dreb  54</td>
<td>17 dur  55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  40</td>
<td><strong>19  oawr</strong>  46</td>
<td>19</td>
<td>50</td>
<td>27 def 4d</td>
<td>24 qui 57</td>
<td>潜力 3c</td>
<td>20 str 37</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h1 id="Will-Barton"><a href="#Will-Barton" class="headerlink" title="Will Barton"></a>Will Barton</h1><p>Free球员 Mickael Pietrus</p>
<p>Will 1551660</p>
<p>Barton 1545676</p>
<p>Mickael 182006 1581062 差-29402</p>
<p>Pietrus 181ff6 1581046 差-35370</p>
<p>7bc44位置有个b3 63 10 00 —双字1063b3—-1074099——-&gt;-35370——&gt;1038729—&gt;0fd989</p>
<p>7bc48位置有个bf 63 10 00 —双字1063bf—-1074111——-&gt;-29402——&gt;1044709—&gt;0ff0e5</p>
<p>原面补ff 03=，用2908=5c 0b</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘5’用196吧</td>
<td>175</td>
<td>2908=5c 0b</td>
<td>SF=02</td>
<td>02</td>
<td>stat 33.1=132.4=84</td>
<td></td>
<td>5—&gt;10—&gt;0a</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM405</th>
<th>80</th>
<th></th>
<th>7bde0</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  46</td>
<td>2 med  5c</td>
<td>3 3pt  52</td>
<td>4 ft 4b</td>
<td>5 layup  4b</td>
<td>6 dunk  3f</td>
<td>28 stdnk  3e</td>
<td>23  sit 39</td>
<td>22 sod 47</td>
<td>25 hus  46</td>
</tr>
<tr>
<td>7 hndl  4f</td>
<td>8  pass 4b</td>
<td>32</td>
<td>32</td>
<td>9 opost  3c</td>
<td>10 dpost  3c</td>
<td>11 block   35</td>
<td>26 hnd  50</td>
<td>12 steal  30</td>
<td>15 speed  4b</td>
<td>16 stam  58</td>
<td>63</td>
<td>21 vert 36</td>
<td>13 oreb  3c</td>
<td>14 dreb  3f</td>
<td>17 dur  49</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  4b</td>
<td><strong>19  oawr</strong>  55</td>
<td>3c</td>
<td>4c</td>
<td>27 def 44</td>
<td>24 qui 52</td>
<td>潜力 4f</td>
<td>20 str 32</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>24岁=4b 7c，6‘8’=205.74</p>
<h1 id="Gary-Harris"><a href="#Gary-Harris" class="headerlink" title="Gary Harris"></a>Gary Harris</h1><p>Free球员 Manny Harris</p>
<p>Gary 1552190</p>
<p>Harris</p>
<p>Manny 4d616e6e79 181d3e=1580350，差了-28160</p>
<p>Harris 不动</p>
<p>77c84位置有个a3 1d 10 00 —双字101da3—-1056163——-&gt;0——&gt;0—&gt;0</p>
<p>77c88位置有个b7 a0 10 00 —双字10a0b7—-1089719——-&gt;-28160——&gt;1061559—&gt;1032b7</p>
<p>原面补70 07=，用2905=59 0b</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘4’=用193—&gt;193.5吧</td>
<td>210</td>
<td>2905=59 0b</td>
<td>SG/05</td>
<td>04</td>
<td>stat 31.8=127.2=7f</td>
<td></td>
<td>14—&gt;28—&gt;1c</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>Free</th>
<th>KM405</th>
<th>76</th>
<th></th>
<th>77e20</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  35</td>
<td>2 med  56</td>
<td>3 3pt  4a</td>
<td>4 ft 53</td>
<td>5 layup  52</td>
<td>6 dunk  3e</td>
<td>28 stdnk  28</td>
<td>23  sit 3d</td>
<td>22 sod 3c</td>
<td>25 hus  53</td>
</tr>
<tr>
<td>7 hndl  4c</td>
<td>8  pass 4b</td>
<td>32</td>
<td>32</td>
<td>9 opost  49</td>
<td>10 dpost  48</td>
<td>11 block   22</td>
<td>26 hnd  49</td>
<td>12 steal  3f</td>
<td>15 speed  48</td>
<td>16 stam  55</td>
<td>19</td>
<td>21 vert 48</td>
<td>13 oreb  4f</td>
<td>14 dreb  46</td>
<td>17 dur  46</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  50</td>
<td><strong>19  oawr</strong>  3c</td>
<td>19</td>
<td>50</td>
<td>27 def 32</td>
<td>24 qui 49</td>
<td>潜力 45</td>
<td>20 str 3a</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h1 id="Mason-Plumlee"><a href="#Mason-Plumlee" class="headerlink" title="Mason Plumlee"></a>Mason Plumlee</h1><p>Kosta Koufos</p>
<p>Mason 1549140</p>
<p>Plumlee 1551454</p>
<p>Kosta 17ab80 1551232 差-2092</p>
<p>Koufos 17ab72 1551218 差+236</p>
<p>21104位置有个6f 9a 15 00 —双字159a6f—-1415791——-&gt;+236——&gt;1416027—&gt;159b5b</p>
<p>21108位置有个79 9a 15 00 —双字159a79—-1415801——&gt;-2092——&gt;1413709—&gt;15924d</p>
<p>原面补f9 05=，用2907=5b 0b</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘11’=210试试</td>
<td>235</td>
<td>2907=5b 0b</td>
<td>C</td>
<td>05</td>
<td>stat 17.0=68=44</td>
<td></td>
<td>7—&gt;14—&gt;e</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>79</th>
<th></th>
<th>212a0</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  3c</td>
<td>2 med  4b</td>
<td>3 3pt  3a</td>
<td>4 ft 38</td>
<td>5 layup  4f</td>
<td>6 dunk  4c</td>
<td>28 stdnk  60</td>
<td>23  sit 4a</td>
<td>22 sod 37</td>
<td>25 hus  3a</td>
</tr>
<tr>
<td>7 hndl  33</td>
<td>8  pass 3b</td>
<td>32</td>
<td>32</td>
<td>9 opost  4b</td>
<td>10 dpost  4b</td>
<td>11 block   48</td>
<td>26 hnd  57</td>
<td>12 steal  38</td>
<td>15 speed  3b</td>
<td>16 stam  5b</td>
<td>19</td>
<td>21 vert 19</td>
<td>13 oreb  4d</td>
<td>14 dreb  54</td>
<td>17 dur  46</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  4b</td>
<td><strong>19  oawr</strong>  50</td>
<td>19</td>
<td>48</td>
<td>27 def 26</td>
<td>24 qui 1c</td>
<td>潜力 3e</td>
<td>20 str 43</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <tags>
        <tag>NBA2K13</tag>
        <tag>DIY</tag>
      </tags>
  </entry>
  <entry>
    <title>【Hexo3】博客置顶与搜索功能</title>
    <url>/2020/09/30/%E3%80%90Hexo3%E3%80%91%E5%8D%9A%E5%AE%A2%E7%BD%AE%E9%A1%B6%E4%B8%8E%E6%90%9C%E7%B4%A2%E5%8A%9F%E8%83%BD/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>记录一下添加功能的代码</p>
</blockquote>
<a id="more"></a>
<p>其实这些没啥干货，参考前端大佬们的分享即可，列出参考链接供以后不时之需</p>
<p>这次给自己的博客加了两个功能，置顶和站内搜索，均试验成功~</p>
<p>置顶代码包括包的安装和<code>post.swig</code>的代码添加（不过我的似乎自己加上了qwq），其中另外把置顶文字的颜色从紫色改成了深天蓝色🤭（颜色代码<code>00BFFF</code>）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-index-pin-top --save</span><br></pre></td></tr></table></figure>
<p>搜索的功能一样，注意要去<code>./themes/next/_config.yml</code>文件下开启搜索功能：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>
<p>参考链接：</p>
<ul>
<li><p><a href="http://wangwlj.com/about/" target="_blank" rel="noopener">王立杰</a>的博客<a href="http://wangwlj.com/2018/01/09/blog_pin_post/" target="_blank" rel="noopener">Hexo博客彻底解决置顶问题</a></p>
</li>
<li><p><a href="https://www.zhihu.com/people/huang-piao-72" target="_blank" rel="noopener">黄飘</a>的知乎文章<a href="https://zhuanlan.zhihu.com/p/55796365" target="_blank" rel="noopener">Hexo博客优化之Next主题功能强化</a></p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文略读1】</title>
    <url>/2020/09/30/%E3%80%90%E8%AE%BA%E6%96%87%E7%95%A5%E8%AF%BB1%E3%80%91/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>此博客略读文献列表：</p>
<ol>
<li><p>Densely Connected Convolutional Networks（写完笔记之后觉得略读个🔨…）</p>
</li>
<li><p>Augmented Neural ODEs</p>
</li>
<li><p>Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs</p>
<blockquote>
<p>updated on 12th, Oct</p>
</blockquote>
</li>
<li><p>DelugeNets: Deep Networks with Efficient and Flexible Cross-layer Information Inflows（读得不太懂…）</p>
<blockquote>
<p>updated on 12th, Oct</p>
</blockquote>
</li>
<li><p>IDENT: Identifying Differential Equations with Numerical Time Evolution</p>
<blockquote>
<p>updated on 20th, Oct</p>
</blockquote>
</li>
</ol>
</blockquote>
<a id="more"></a>
<p>最近读文章有两个比较大的困惑，一个是速度很慢，因为读得相对还是有点细的，很多细节没怎么放过；另一个是效率的问题，即使做了一堆堆的笔记，也未必能把重点都表现出来，甚至还会混淆文章重点。</p>
<p>因此考虑锻炼自己的略读能力，不过略读论文毕竟不是读网络小说，看过不能随随便便丢掉。而觉得单开博客记录略读笔记有些浪费空间，所以决定开一些略读博客，把略读的笔记集中起来，方便日后查阅。</p>
<h1 id="Densely-Connected-Convolutional-Networks"><a href="#Densely-Connected-Convolutional-Networks" class="headerlink" title="Densely Connected Convolutional Networks"></a>Densely Connected Convolutional Networks</h1><p>读这篇文章是因为我希望了解一些网络跨层连接的知识。感觉这个文章关系很大也比较基础，就拿来了</p>
<img src="/2020/09/30/【论文略读1】/DenseNet-1.png" title="DenseNet">
<h2 id="Why-DenseNet"><a href="#Why-DenseNet" class="headerlink" title="Why DenseNet?"></a>Why DenseNet?</h2><p>该网络跨层模型的背景是<code>CNN</code>若在输入输出两端之间有更短的连接时效果一般更好（没去找这个结论的出处），基于此，本文的初始目的应当是减小<code>CNN</code>网络中层与层之间的连接长度。</p>
<h2 id="What’s-and-How-DenseNet"><a href="#What’s-and-How-DenseNet" class="headerlink" title="What’s and How DenseNet?"></a>What’s and How DenseNet?</h2><p>这个<code>DenseNet</code>名称的来源是因为给<code>CNN</code>网络加了跨层连接，使得网络更加“<strong>密集连结</strong>”，因此就称为<code>DenseNet</code>了。注意这里<code>DenseNet</code>还是一个<code>CNN</code>的结构</p>
<p>而它的跨层也是<strong>真的</strong>进行了完全跨层，假设网络一共 $L$ 层，比如原文的图 $1$ 就是5层，输入和输出也各算一层。那么所有层与层之间的连接数（正向）有 $\displaystyle 1+2+\cdots+L = \dfrac{L(L+1)}{2}$，画个图自己连一下就推出来了</p>
<p>由于跨层连接是forward的，它并没有考虑反传，<code>DenseNet</code>考虑每个layer的输入都是前面所有layer输出的特征图的<strong>concatenating</strong>（这隐含了靠后layer输入可能维度大的问题），也就是说把前面所有的特征图都通通用上了😄</p>
<h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><p>这个是文章直接列的，我觉得都差不多，比较有道理，就也通通列出来：</p>
<ul>
<li><p>保持了CNN的结构，可以堆砌到一般网络中，好像有个Dense层的命令就是干这个的</p>
</li>
<li><p>真的是跨层连接啊，都是forward方向的，有加强特征前向传播的意思</p>
</li>
<li><p>跨层的方式是特征图的充（全）分（部）重复使用，这进一步有一些优点：</p>
<ul>
<li><p>特征图的充分利用，其实是保持了数据信息流的完整性</p>
</li>
<li><p>不用像其它模型每次训练都重新学习大量特征图导致网络的参数量减小，计算消耗减小</p>
<blockquote>
<p>Why？特征图该有还是有啊，网络还是那么多层，哪里参数不用学习了？</p>
</blockquote>
</li>
<li><p>特征图充分利用，可以看成是良好的特征提取器，方式包括特征图的reuse，恒等映射的使用，深度监督（最后的loss在每一层都对之前的特征图有梯度？我猜是这个意思）的使用，多样化的深度（我觉得是指跨层变相加深网络深度？）</p>
</li>
</ul>
</li>
<li><p>实验效果SOTA啊，且实验发现<code>DenseNet</code>参数量增加时，模型性能保持增加，没有出现明显的过拟合</p>
</li>
<li><p>缓解了梯度消失问题</p>
<blockquote>
<p>暂时没看出来在哪里…</p>
</blockquote>
</li>
</ul>
<p>最后ps一下，之前提到“靠后layer输入可能维度大”的问题，原文对此似乎有个操作，精心设计了一些结构，应该是通过每次下采样（原图图2）减小了特征图的深度，以及有一些 $1\times 1$ 卷积和pooling的操作，等等</p>
<h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><ul>
<li>堆叠特征图是不是有些冗余的感觉呢❓</li>
<li>有没有反向的跨层？不知道会不会有什么意义🙄</li>
<li>“保持了数据信息流的完整性”这个说法与流模型的建模有关联么🙊</li>
<li>直接通通跨层有些暴力，有没有后续工作做部分跨层的选择呢🤔</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">DenseNet</a>，<a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">Identity mappings</a></p>
<p>[1] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261–2269, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society.</p>
<p>[2] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. ArXiv, abs/1603.05027, 2016.</p>
<h1 id="Augmented-Neural-ODEs"><a href="#Augmented-Neural-ODEs" class="headerlink" title="Augmented Neural ODEs"></a>Augmented Neural ODEs</h1><p><strong>这篇文章本来打算列为精读</strong>，但是一方面读了很久了，另一方面我能写的其实不多，所（偷）以（懒）就列为略读了。</p>
<p>文献链接：<a href="https://arxiv.org/abs/1904.01681v2" target="_blank" rel="noopener">https://arxiv.org/abs/1904.01681v2</a> 或 <a href="http://papers.nips.cc/paper/8577-augmented-neural-odes" target="_blank" rel="noopener">http://papers.nips.cc/paper/8577-augmented-neural-odes</a></p>
<img src="/2020/09/30/【论文略读1】/ANODE.png" title="ANODE">
<h2 id="My-Marvelous-Confusion-solved"><a href="#My-Marvelous-Confusion-solved" class="headerlink" title="My Marvelous Confusion (solved)"></a>My Marvelous Confusion (solved)</h2><p>已经读了很久了，是因为有一个问题一直想不通，直到今天上午开始写总结才反应过来是怎么一回事😭。就是文章的<code>proposition 1</code>提出ODE的流，即原来的NODE函数不能描述这样的一个函数：</p>
<script type="math/tex; mode=display">\displaystyle \begin{equation} g(x)=\left\{ \begin{aligned} 1,\ x=-1\\ -1,\ x=1. \end{aligned} \right. \end{equation},\ where\ g(x):\mathbb{R}\rightarrow \mathbb{R}.</script><blockquote>
<p>ps: 这样的一个流函数希望把ODE $\displaystyle \dfrac{h(t)}{dt} = f(h(t), t),\ h(t_0) =h(0) \triangleq x_0$ 中的初始状态 $x_0$ 映射到某一固定时刻 $t_1$ 的状态 $h(t_1)$，即 $g(x):h(t_0)\mapsto h(t_1)$。但是我想了很长时间，暂时都没有举出一个这样的例子😭…后来才反应过来，这他🐱是举不出例子的啊，这样的轨线不存在啊啊啊啊啊啊啊啊啊</p>
</blockquote>
<p>但是我之前想，为什么要去要求NODE描述这样的一个表示轨线相交的函数呢？不是已经证明了这样的函数不能成为一个ODE的流函数么？这样的 $g(x)$ 作为所谓的“流函数”应该是不存在的，那你去学习个🔨❓</p>
<p>后来又想了想，如果能采用<strong>增广维度</strong>的方式，确实，在子空间中其实就不存在轨线相交的情形了，所以ANODE其实是<span style="border-bottom:1px solid black;">通过维度的增广扩宽了NODE能学到的函数空间</span>吧，这样就解决了我的困惑了…</p>
<h2 id="Why-ANODE"><a href="#Why-ANODE" class="headerlink" title="Why ANODE"></a>Why ANODE</h2><p>为什么读这个文章呢？</p>
<ul>
<li>人家是NIPS2019</li>
<li>最近顺着ODE看这个，挺有意思</li>
</ul>
<p>这个文章是核心思想是在NODE的基础上进行数据（状态）的维度增广，通过增广的那部分维度拓宽了可以学习到的函数空间，i.e. 学到了一些原来NODE在原空间中学习不到的函数</p>
<h2 id="How-ANODE"><a href="#How-ANODE" class="headerlink" title="How ANODE"></a>How ANODE</h2><p>没啥可说的了，原来NODE是考虑ODE方程（不要求自治）</p>
<script type="math/tex; mode=display">\displaystyle \dfrac{h(t)}{dt} = f(h(t), t),\ h(t_0) =h(0) \triangleq x_0</script><p>但是现在增广维度，可以克服原空间中轨线相交型流函数的学习：</p>
<script type="math/tex; mode=display">\displaystyle \dfrac{d}{dt}\begin{bmatrix} \textbf{h}(t) \\ \textbf{a}(t) \end{bmatrix} = f(\begin{bmatrix} \textbf{h}(t) \\ \textbf{a}(t) \end{bmatrix}, t),\ \begin{bmatrix} \textbf{h}(t_0) \\ \textbf{a}(t_0) \end{bmatrix} = \begin{bmatrix} \textbf{h}(0) \\ \textbf{a}(0) \end{bmatrix} \triangleq \begin{bmatrix} \textbf{x} \\ \textbf{0} \end{bmatrix}</script><p>我看到这个式子就全明白轨线相交、NODE学习不了的函数这些东西都是在干啥了~</p>
<h2 id="Advantages-1"><a href="#Advantages-1" class="headerlink" title="Advantages"></a>Advantages</h2><p>直接列举：</p>
<ul>
<li>从思想出发，基础的NODE学习不了有的流函数，增广维度可以克服这一点</li>
<li>所提ANODE表达能力🚀，稳定性🚀，泛化🚀，计算量➖🚀（均未确认）</li>
<li>保留了NODE的一个优点，保持了原始输入空间的拓扑（阿巴阿巴…）</li>
</ul>
<h2 id="Thinking-1"><a href="#Thinking-1" class="headerlink" title="Thinking"></a>Thinking</h2><p>有个固有的想法是：</p>
<ul>
<li>维度的增广<strong>本来就是</strong>在拓宽模型的函数空间的，从这角度提出ANODE的话，就是inductive的</li>
<li>而从这个角度——NODE不能学习的函数，来引入维度增广的思想，这样就是deductive的</li>
</ul>
<p>这样一想，这篇ANODE真的就是个idea啊，顶会密码🔞get嗷嗷嗷嗷嗷嗷嗷🤣</p>
<h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><p>[1] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3140–3150. Curran Associates, Inc., 2019.</p>
<h1 id="Model-based-Reinforcement-Learning-for-Semi-Markov-Decision-Processes-with-Neural-ODEs"><a href="#Model-based-Reinforcement-Learning-for-Semi-Markov-Decision-Processes-with-Neural-ODEs" class="headerlink" title="Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs"></a>Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs</h1><p>这篇文章是NODE在RL上进行应用的例子，2020/06/29挂在了ArXiv上，文献地址<a href="https://arxiv.org/abs/2006.16210" target="_blank" rel="noopener">链接</a></p>
<img src="/2020/09/30/【论文略读1】/RL-NODE.png" title="RL with NODE">
<h2 id="RL-Concepts"><a href="#RL-Concepts" class="headerlink" title="RL Concepts"></a>RL Concepts</h2><p>这篇文章涉及到了很多RL，但是我没怎么细看过，所以需要先补一些文中出现的概念。</p>
<ul>
<li><p>RL中的model-free方法，指的是啥？懒得总结，引用<a href="https://www.zhihu.com/people/Mr.Zh" target="_blank" rel="noopener">奶油花诶</a>在知乎问题<a href="https://www.zhihu.com/question/64369408" target="_blank" rel="noopener">model-based和model-free，on-policy和off-policy区别？</a>中的解释：</p>
<blockquote>
<p>“<strong>model-free</strong>, 是agent和environment进行实时的交互；而<strong>model-based</strong>，从名字上我们就能感受到的到，是先根据真实的情况先学得一个model，即比model-free多了一个对真实世界建模的过程罢了”</p>
</blockquote>
<p>由于直接由agent和环境交互学习，这里应当在训练中用到大量训练数据</p>
</li>
</ul>
<h2 id="Why-RL-Framework-for-SMDPs-with-NODE？"><a href="#Why-RL-Framework-for-SMDPs-with-NODE？" class="headerlink" title="Why RL Framework for SMDPs with NODE？"></a>Why RL Framework for SMDPs with NODE？</h2><p>本文针对的问题还是对连续的ODE动力系统进行建模。一个背景需求就是现实中很多数据都是连续化的。</p>
<p>过去是怎么做的呢？文中Intro部分第二段开头2-4段给出了过去的方法列表，缺点一是统一的，使用了简单的线性函数估计器；缺点二是这些方法基本上是model-free的，似乎需要在训练中用到大量训练数据；缺点三是它们似乎都是启发式的方法，把连续的系统离散化成离散动力系统，再用标准RL方法，自然，离散话若是粗糙了会丢失信息，离散太细了又太耗时。</p>
<blockquote>
<p>ps：对缺点1，那为啥不用复杂点的呢❓</p>
</blockquote>
<h2 id="What’s-RL-Framework-for-SMDPs-with-NODE"><a href="#What’s-RL-Framework-for-SMDPs-with-NODE" class="headerlink" title="What’s RL Framework for SMDPs with NODE"></a>What’s RL Framework for SMDPs with NODE</h2><p>该文所提方法是<span style="border-bottom:1px solid black;">利用NODE建立对SMDPs的基于模型的RL框架</span>，其中<code>SMDPs</code>是本文针对的基本模型，全名<code>semi-Markov decision processes</code>。</p>
<blockquote>
<p>说实话，RL我真的一下子看不下去…</p>
</blockquote>
<p>这个<code>SMDPs</code>的数学表达大概是：</p>
<script type="math/tex; mode=display">\displaystyle \max_{tuple\ (\mathcal{S}, \mathcal{P}, \mathcal{A}, \mathcal{R}, \mathcal{T}, \gamma)} \mathbb{E}\left[\sum_{i=1}^L \gamma^{t_i}r_i\right]</script><p>其中元组 $(\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{T}, \gamma)$ 中，$\mathcal{S}$ 是状态空间，$\mathcal{A}$ 是行动空间，$\mathcal{R}$ 代表行动的奖励函数，即 $r = R(s, a, s^\prime)$ ，其中奖励函数也可以有不（改）同（进）的表达形式，$\mathcal{T}$ 是采取每次行动的时间间隔设置，$\gamma \in (0,1]$ 是每一步奖励的衰减因子（RL中的一般要求）。最后说一下状态转移规则 $\mathcal{P}$，具体的意义就是给定当前状态和行动给出下一次状态，也可以给出一些附加的辅助变量，一般的形式是 $P(s^\prime|s,a)$。</p>
<p>以上是一般强化学习对<code>SMDPs</code>过程的设置，现在考虑了NODE的应用，具体的方式是对状态变量 $s$ 进行处理。我们把 $s$ 看作像图像一类的原始特征，对它进行编码得到隐空间中的隐变量 $z$，形成编码的过程。按照原文的说法，这就是压缩了原始的状态 $s$。</p>
<p>对于隐空间变量 $z$，我们把它看作NODE模拟的状态，这样的状态维度是可控的，而且暗含了模拟状态变量 $s$ 在低维空间中显示出的流模型，流的变化规律，这一点我很喜欢！</p>
<h2 id="Omitted-Details"><a href="#Omitted-Details" class="headerlink" title="Omitted Details"></a>Omitted Details</h2><p>略读，很多细节没有再去深入了。部分为：</p>
<ul>
<li>原文其实还有一个辅助模型，应该是针对RL学习的过程的，用来优化训练过程</li>
<li>作者表示这是<strong>首篇</strong>结合RL和NODE的文章，思想可借鉴</li>
</ul>
<h2 id="Pros-and-Cons"><a href="#Pros-and-Cons" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>首次结合RL和NODE</td>
<td>broader impact中提到应用的时候要小心（必须啊）</td>
</tr>
<tr>
<td>在隐空间中完成模拟连续动力系统背后状态的流模型</td>
<td>TBD</td>
</tr>
</tbody>
</table>
</div>
<h2 id="References-2"><a href="#References-2" class="headerlink" title="References"></a>References</h2><p>[1] Jianzhun Du, Joseph Futoma, and Finale Doshi-Velez. Model-based reinforcement learning for semi-markov decision processes with neural odes, 2020.</p>
<h1 id="DelugeNets-Deep-Networks-with-Efficient-and-Flexible-Cross-layer-Information-Inflows"><a href="#DelugeNets-Deep-Networks-with-Efficient-and-Flexible-Cross-layer-Information-Inflows" class="headerlink" title="DelugeNets: Deep Networks with Efficient and Flexible Cross-layer Information Inflows"></a>DelugeNets: Deep Networks with Efficient and Flexible Cross-layer Information Inflows</h1><p>这篇文章是2017CVPR的文章，作者来自新加坡南洋理工，和阿里巴巴，2020/10/12在百度学术上查到的引用量为10。文献地址可以有<a href="https://openaccess.thecvf.com/content_ICCV_2017_workshops/w18/html/Kuen_DelugeNets_Deep_Networks_ICCV_2017_paper.html" target="_blank" rel="noopener">ICCV 2017 Open Access Repository</a>和<a href="https://arxiv.org/abs/1611.05552?context=cs.CV" target="_blank" rel="noopener">ArXiv: 1611.05552</a>。</p>
<p>说实话这篇文章没有太看明白，既然是略读，就把已经读得的思考写下来吧。</p>
<img src="/2020/09/30/【论文略读1】/DelugeNets-1.png" title="DelugeNets">
<h2 id="卷积概念回顾"><a href="#卷积概念回顾" class="headerlink" title="卷积概念回顾"></a>卷积概念回顾</h2><p>这篇文章中有个关键的卷积操作，叫<span style="border-bottom:1px solid black;">cross-layer depthwise convolution</span>。</p>
<blockquote>
<p>这个卷积操作其实还是一种<span style="border-bottom:1px solid black;">separable convolution</span>，本文在Intro部分提到这是本文的<strong>思想来源</strong>。后者操作则相当于把一般的卷积进行分块，分解为<span style="border-bottom:1px solid black;">depthwise convolution</span>与<span style="border-bottom:1px solid black;">pointwise convolution</span>两部分 $^{[3]}$ </p>
</blockquote>
<h2 id="Why-and-What’s-DelugeNets"><a href="#Why-and-What’s-DelugeNets" class="headerlink" title="Why and What’s DelugeNets?"></a>Why and What’s DelugeNets?</h2><p>这个概念看起来很有意思，但是这个所谓的洪（什）水（么）网络是啥呢，我也不知道。所以按顺序慢慢来，先讲怎么来的，再概括这是啥。</p>
<p>首先声明DelugeNets基于DenseNet $^{[5]}$ ，并由<span style="border-bottom:1px solid black;">separable convolution</span>启发得到，但我们先从ResNet讲起。</p>
<p>ResNet的一大特点是信息流毫无障碍，通过残差的形式一层接一层，梯度传播的时候用链式法则非常轻松。但是这样一溜烟传播过去有个缺点，就是后面隐层可能难以突出前面特定隐层的信息，进而阻碍ResNet学习网络跨层连接的信息。</p>
<p>而DenseNet作者在Intro部分介绍了一大堆DenseNet的缺点，然后，提出了DelugeNets。</p>
<p>DenseNet具体的缺点有：</p>
<p><a href="https://blog.csdn.net/qq_19329785/article/details/84677841" target="_blank" rel="noopener">https://blog.csdn.net/qq_19329785/article/details/84677841</a></p>
<p><a href="https://www.cnblogs.com/Alliswell-WP/p/Deeplearning_CNN_Review001.html" target="_blank" rel="noopener">https://www.cnblogs.com/Alliswell-WP/p/Deeplearning_CNN_Review001.html</a></p>
<h2 id="References-3"><a href="#References-3" class="headerlink" title="References"></a>References</h2><p>[1] Jason Kuen, Xiangfei Kong, Gang Wang, and Yap-Peng Tan. Delugenets: Deep networks with efficient and flexible cross-layer information inflows. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops, Oct 2017.</p>
<p>[2] Wei Ji. 使用Relu的原因及好处[EB/OL]. <a href="https://blog.csdn.net/qq_19329785/article/details/84677841" target="_blank" rel="noopener">https://blog.csdn.net/qq_19329785/article/details/84677841</a>, 2018-12-01.</p>
<p>[3] YIN GUOBING. 卷积神经网络中的Separable Convolution[EB/OL]. <a href="https://yinguobing.com/separable-convolution/#fnref2" target="_blank" rel="noopener">https://yinguobing.com/separable-convolution/#fnref2</a>, 2018-02-27.</p>
<p>[4] 干巴他爹. Depthwise卷积与Pointwise卷积[EB/OL]. <a href="https://blog.csdn.net/tintinetmilou/article/details/81607721" target="_blank" rel="noopener">https://blog.csdn.net/tintinetmilou/article/details/81607721</a>, 2018-08-12.</p>
<p>[5] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261–2269, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society.</p>
<h1 id="IDENT-Identifying-Differential-Equations-with-Numerical-Time-Evolution"><a href="#IDENT-Identifying-Differential-Equations-with-Numerical-Time-Evolution" class="headerlink" title="IDENT: Identifying Differential Equations with Numerical Time Evolution"></a>IDENT: Identifying Differential Equations with Numerical Time Evolution</h1><img src="/2020/09/30/【论文略读1】/IDENT.png" title="IDENT">
<p>文献链接： <a href="http://people.math.gatech.edu/~wliao60/Research/papers/IDENT.pdf" target="_blank" rel="noopener">http://people.math.gatech.edu/~wliao60/Research/papers/IDENT.pdf</a> 或 <a href="https://arxiv.org/abs/1904.03538" target="_blank" rel="noopener">https://arxiv.org/abs/1904.03538</a></p>
<p>这个文章很长，所以<strong>不总结</strong>了，走翻译的老路子。而且这里<strong>只把算法的思路</strong>搞明白，看看大家是怎么做的，具体证明的细节都略掉。</p>
<h2 id="摘要——姑且算是文献简介"><a href="#摘要——姑且算是文献简介" class="headerlink" title="摘要——姑且算是文献简介"></a>摘要——姑且算是文献简介</h2><p>背景问题是如何由离散的时间序列数据来<strong>识别DE的模式</strong>，其难处是noise即便小也会炸，且非线性性非常复杂，容易<strong>存在多成分</strong>。这个问题的一些偏执归纳是<strong>背后的流模型</strong>——PDE是一系列成分的线性组合，因此主要的问题抽象为<strong>找DE多成分的系数</strong>。</p>
<p>对此问题，提出<strong>IDENT</strong>（Identifying Differential Equations with Numerical Time Evolution）方法，其训练的约束用到了是Time Evolution Error (TEE)，顺便提出噪声等级的度量Noise-to-Signal ratio。后面一堆方法就不管了。</p>
<h2 id="文献思路翻译"><a href="#文献思路翻译" class="headerlink" title="文献思路翻译"></a>文献思路翻译</h2><p>$2.1$ 节全是记号，没问题。 $2.2$ 定义问题：</p>
<p>假设数据背后的PDE模型为分量的线性组合：</p>
<script type="math/tex; mode=display">\mathcal{F}(x,u,u_x,u_{xx}).</script><p>一共 $10=4+3+2+1$ 个分量： $1, u, u^2, u_x, u_x^2, uu_x, u_{xx}, u_{xx}^2, uu_{xx}, u_xu_{xx}$。其中<strong>记</strong></p>
<script type="math/tex; mode=display">u_t = \mathcal{F}(x,u,u_x,u_{xx}) = Linear(10\ items)</script><p>其中每个单项式是一个特征，特征数 $N_3 = 10$ ，RHS的线性组合可以看成 $\mathcal{F}$ 的泰勒展开。</p>
<p>已有的时间数据（注意还有个已知条件是非周期的边界条件）是函数值 $u_i^n = u(x_i, t_n)$ ，以及时空间的分割方式， $(x_i, t_n), i = 1, \cdots, N_1, n = 1, \cdots, N_2$ ，网格式的数据。离散数据可以写成矩阵的形式：</p>
<script type="math/tex; mode=display">\begin{aligned} \mathbf{b} &\triangleq \begin{pmatrix} u_t(x_1, t_1), \cdots, u_t(x_i, t_n), \cdots, u_t(x_{N_1}, t_{N_2})\end{pmatrix}_{\in \mathbb{R}^{N_1N_2\times 1}}\\ &= \begin{pmatrix} 1& u(x_1, t_1)& u^2(x_1, t_1)& u_x(x_1, t_1)& u_x^2(x_1, t_1)& uu_x(x_1, t_1)& u_{xx}(x_1, t_1)& u_{xx}^2(x_1, t_1)& uu_{xx}(x_1, t_1)& u_xu_{xx}(x_1, t_1)\\\vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots \\ 1& u(x_i, t_n)& u^2(x_i, t_n)& u_x(x_i, t_n)& u_x^2(x_i, t_n)& uu_x(x_i, t_n)& u_{xx}(x_i, t_n)& u_{xx}^2(x_i, t_n)& uu_{xx}(x_i, t_n)& u_xu_{xx}(x_i, t_n)\\ \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots \\ 1& u(x_{N_1}, t_{N_2})& u^2(x_{N_1}, t_{N_2})& u_x(x_{N_1}, t_{N_2})& u_x^2(x_{N_1}, t_{N_2})& uu_x(x_{N_1}, t_{N_2})& u_{xx}(x_{N_1}, t_{N_2})& u_{xx}^2(x_{N_1}, t_{N_2})& uu_{xx}(x_{N_1}, t_{N_2})& u_xu_{xx}(x_{N_1}, t_{N_2})\end{pmatrix}_{\in \mathbb{R}^{N_1N_2\times N_3}} \begin{pmatrix} a_1\\ a_2\\ a_3\\ \vdots\\ a_{N_3-2}\\ a_{N_3-1}\\ a_{N_3}\\ \end{pmatrix}_{\in \mathbb{R}^{N_3\times 1}} \\ &\triangleq \begin{pmatrix} F[1], \cdots, F[j], \cdots, F[N_3]\end{pmatrix}\mathbf{a}\\ &\triangleq \mathbf{Fa}\end{aligned}</script><p>啊这，这排版功力绝了~🍗你阿巴阿巴了么💯</p>
<p>目标问题就是求解系数 $\mathbf{a}$ ，而普通的实际问题中，RHS的形式不一定有这么多项，所以系数 $\mathbf{a}$ 的形式很可能是稀疏的。进一步，系数 $\mathbf{a}$ 可能是和 $x$ 相关的，所有的分量都可以写成 $x$ 的函数的形式；再进一步，系数 $\mathbf{a}(x)$ 可以用基函数的形式表出，如分段线性连续（设分 $L$ 段）基函数。那么目标函数可以考虑为泛函 $L^p$ 范数（应该是这样）：</p>
<script type="math/tex; mode=display">\displaystyle ||a_j - \sum_{l=1}^L a_{j,l}\phi_l||_{L^p}\leqslant O(\frac{1}{L}), p\in (0, \infty)</script><p>在基函数的分解形式下， $\mathbf{F}$ 可以把每一列（特征）的基都写开，那么维度变成 $\mathbb{R}^{N_1N_2\times N_3L}$ ，同理， $\mathbf{a}$ 和 $\mathbf{b}$ 的维度也相应改变。</p>
<p>此时由于系数 $\mathbf{a}(x)$ 成为真实  $\mathbf{a}$ 的近似表出，那么离散数据对应的矩阵方程可以分离出残差项，即</p>
<script type="math/tex; mode=display">\mathbf{Fa} = \mathbf{b} + \mathbf{\eta}</script><p>上式需要注意维度已经被基函数扩大了，那么目标函数也变成</p>
<script type="math/tex; mode=display">\displaystyle ||\eta||_{L^p}\leqslant O(\frac{1}{L}), p\in (0, \infty)</script><p>$2.3$ 节正式提出IDENT算法：</p>
<p>首先要由函数值导出对时间 $t$ 的导数，即方程中的 $\mathbf{b}$ ，直接用差分近似（文章这个地方好像打错了个下标）：</p>
<script type="math/tex; mode=display">\hat{u_t}(x_i, t_n) = u_t(x_i, t_n) + O(\Delta t) \triangleq \dfrac{u(x_i, t_n) - u(x_i, t_{n-1})}{\Delta t}</script><p>这就作为 $\mathbf{b}$ 的近似 $\mathbf{\hat{b}}$ 的分量。</p>
<p>其它的特征，包括 $u_x, u_{xx}$ 等不直接用一阶差分近似，引用别人的方法，叫做<code>5点ENO</code>，哎，这样一顿操作，大矩阵 $\mathbf{F}$ 的近似出来了，记作 $\hat{\mathbf{F}}$ 。这样近似基本上都弄好了。原先的矩阵方程成为：</p>
<script type="math/tex; mode=display">\begin{aligned} \hat{\mathbf{F}}\mathbf{a} &= (\hat{\mathbf{F}} + \mathbf{F} - \mathbf{F})\mathbf{a} = \mathbf{b} + \mathbf{\eta} + (\hat{\mathbf{F}} - \mathbf{F})\mathbf{a}\\ &= (\mathbf{b} + \hat{\mathbf{b}} - \hat{\mathbf{b}}) + \mathbf{\eta} + (\hat{\mathbf{F}} - \mathbf{F})\mathbf{a}\\ &= \hat{\mathbf{b}} + (\mathbf{b} - \hat{\mathbf{b}}) + \mathbf{\eta} + (\hat{\mathbf{F}} - \mathbf{F})\mathbf{a}\\ &\triangleq \hat{\mathbf{b}} + \mathbf{e} \end{aligned}</script><p>总的近似误差 $\mathbf{e}$ 满足（应该是后面证明的）</p>
<script type="math/tex; mode=display">\displaystyle ||\mathbf{e}||_{L^2}\leqslant \varepsilon\ s.t.\ \varepsilon = O(\Delta t + \Delta x^3 + \frac{1}{L}), p\in (0, \infty)</script><p>第二步似乎就是求解了？不，不是求解！第二步是用LASSO筛选出稀疏的成分，判断哪些特征对应的系数是比较重要的，方法是使用了LASSO并构造了带 $L_1$ 稀疏正则的能量函数。</p>
<p>第三步则是根据已经知道了方程中含有哪些重要的成分（特征），再用TEE方法（Time Evolution Error）最小二乘进行拟合，得到最后的稀疏系数。</p>
<p>这篇文章后面其实还有很多内容，但是感觉不太能看得下去了，先码在这里，把后面的一些重点列在这里：</p>
<ol>
<li>Why LASSO 选取稀疏特征</li>
<li>TEE效果不错</li>
<li>噪声水平指标NSR：Noise-to-Signal Ratio</li>
<li>对噪声稳健性及采样方法</li>
</ol>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Sung Ha Kang, Wenjing Liao, and Yingjie Liu. Ident: Identifying differential equations with numerical time evolution, 2019.</p>
]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>paper reading</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读22】ODE2VAE——2阶ODE模拟+BNN引入不确定性</title>
    <url>/2020/09/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB22%E3%80%91ODE2VAE%E2%80%94%E2%80%942%E9%98%B6ODE%E6%A8%A1%E6%8B%9F-BNN%E5%BC%95%E5%85%A5%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>本文笔记主要是我对19年NIPS的文章ODE2VAE：</p>
<p><code>ODE2VAE: Deep generative second order ODEs with Bayesian neural networks</code></p>
<p>其模型的生成方式和模型结构的理解，具体概率公式的推导和实验的细节都略掉了，如果需要的话以后再看</p>
</blockquote>
<a id="more"></a>
<img src="/2020/09/25/【论文阅读22】ODE2VAE——2阶ODE模拟-BNN引入不确定性/ODE2VAE-1.png" title="文献预览">
<h1 id="开门见山——文献总结"><a href="#开门见山——文献总结" class="headerlink" title="开门见山——文献总结"></a>开门见山——文献总结</h1><p>这篇文章是19年的NIPS文章，与ANODE是同一次会议产出的。而在我看来，这两篇文章很多<strong>内容是有类似的地方</strong>的，但是在<strong>思想上又大不相同</strong>：</p>
<ul>
<li>前者的目标主要是做生成模型，希望设计学习更好的连续流数据（模型）的建模，方式是在NODE基础上考虑了二阶导的模拟和不确定性的引入，这两个是用一个模块同时完成的，即采用BNN学习二阶导，同时就可以引入不确定性。最后考虑到生成的效果需要保障，因此用VAE的形式进行了对抗</li>
<li>后者的目标主要是对NODE直接做改进，从NODE的一个缺点入手，利用手动添加增广维度，在高维空间中克服了对低维空间中不连续情形的模拟，从而完成了NODE的增广，据说提高了表达能力、稳定性、泛化性，降低了计算消耗。这个缺点即有一大类函数是NODE学习不了的，但是目前对这个我真的还不是很理解，因此后续再进行阅读</li>
</ul>
<p>上面的介绍已经表明了目前我对ODE2VAE模型的认识（<strong>当我自己写到这里的时候，我认为这个ODE2VAE中对二阶导的建模本质上可以看成ANODE在维度上进行增广的一个特例~</strong>）。下面我们回顾一下该模型的前因后果吧。</p>
<h1 id="前因后果——ODE2VAE的前生今世"><a href="#前因后果——ODE2VAE的前生今世" class="headerlink" title="前因后果——ODE2VAE的前生今世"></a>前因后果——ODE2VAE的前生今世</h1><p>故事要从流模型开始讲，流模型在干什么？一个简单的解释是流模型本身是ODE领域的延伸概念，把ODE初值问题中不同时刻的状态变化过程称为一个流。这个概念其实很有意思，后面我会<font color="#0000FF">单开一文</font>来介绍它。</p>
<p>流模型由于其本身脱胎于ODE系统的性质，对很多实际情况的模拟应该是比较适合的，但是与神经网络结合的话，此前只有NODE简单地以ResNet中残差的形式对应到了差分，因此完成了一阶导的模拟。举个应用的例子，连续视频流文件的每一帧都可以看成一个中间状态，其背后是很可能有隐含的流模型的，这时如果能学习出其流模型，那么就可以完成视频补帧，甚至短期预测等重要任务。</p>
<h1 id="模型机理——ODE-amp-VAE"><a href="#模型机理——ODE-amp-VAE" class="headerlink" title="模型机理——ODE&amp;VAE"></a>模型机理——ODE&amp;VAE</h1><p>回顾一下，ODE2VAE的一个目标是学习生成模型，突破NODE1阶导的限制，进行更细的二阶导建模。那么它对二阶导的做法是用BNN来学习一个对应的函数，同时BNN可以引入不确定性。另外，既然是生成模型，那么既要编码也要解码，因此其中又使用了VAE的对抗模式保证数据恢复的质量。下面先上图，再康康这个生成模型是怎么假设的：</p>
<img src="/2020/09/25/【论文阅读22】ODE2VAE——2阶ODE模拟-BNN引入不确定性/ODE2VAE-2.png" title="模型结构">
<p>流模型的生成结构为：</p>
<script type="math/tex; mode=display">\displaystyle \begin{align} s_0 & \sim p(s_0) \tag{1}\\  v_0 &\sim p(v_0) \tag{2}\\  s_t &= s_0 + \int_{0}^{t} v_{\tau} d\tau \tag{3}\\  v_t &= v_0 + \int_{0}^{t} f_{true} (s_{\tau}, v_{\tau})d\tau \tag{4} \\  x_i &\sim p(x_i | s_i), i\in [0, N] \tag{5}\end{align}</script><p><strong>注意</strong>，这里的变量 $s$ 是把原始的流数据经过编码器转换到隐空间中的变量！即在隐空间中假设存在了流模型！</p>
<p>这里前两个式子 $(1, 2)$ 表示流的初始状态：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>符号</th>
<th>流上的意义</th>
<th>形象的意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>$s_0$</td>
<td>初始状态（state）的值</td>
<td>初始时刻0时，点的位置</td>
</tr>
<tr>
<td>$v_0$</td>
<td>初始状态（state）的一阶导</td>
<td>初始时刻0时，点的速度</td>
</tr>
</tbody>
</table>
</div>
<p>接下来的 $(3, 4)$ 式则是流的变化过程，一阶导 $v$ 的积分可以描述状态 $s$ 的变化，即 $(3)$ 式；而二阶导 $v^\prime$ 的积分可以描述一阶导 $v$ 的变化，即 $(4)$ 式。其中二阶导就用贝叶斯神经网络BNN来暴力模拟，记这个BNN的整体函数表达式为 $f_{true}$，$(4)$ 式表明它的输入是先前所有状态和一阶导，不过模型结构图表示输入是当前状态的值和一阶导。由于目前我没有研究代码，我<font color="#0000FF">个人认为</font>这两种输入都可以，但我猜在当前的架构下，<span style="border-bottom:1px solid black;">采用前一段时间内的流数据建模</span>比较好，这个想法是比较合理的。前者输入太多，虽然利用的信息多，但是计算的消耗也会很大，而且可能需要用到RNN等处理序列数据的方法作为子模型；后者输入比较少，指利用当前状态的信息，这其实也不太稳，毕竟二阶导自身的变化也应当被正确建模。</p>
<p>最后的 $(5)$ 式就是图中的解码器了，把学习到的流的变化特征重构成流数据。不过这个 $p(x_i | s_i)$ 具体是怎么解码回去的我就不清楚了，我猜是参数共享，直接把编码器给倒过来，从隐空间恢复原始的流数据。</p>
<hr>
<p>以上，本文的出发点，以生成模型建模流数据的过程就弄得差不多了。这里再补充一个小细节，一阶导 $s$ 的生成，是利用原始流数据 $x$ 的前 $m$ 维完成的，举个例子，这个前 $m$ 维就是视频流数据的前 $m$ 帧，这种近似的思想也与前面我所提到的“<span style="border-bottom:1px solid black;">采用前一段时间内的流数据建模</span>”一致~</p>
<h1 id="优缺点——扶摇直上亿万里"><a href="#优缺点——扶摇直上亿万里" class="headerlink" title="优缺点——扶摇直上亿万里"></a>优缺点——扶摇直上亿万里</h1><p>最后回到总结吧~</p>
<p>前面提到ODE2VAE的目标主要是做生成模型，对流数据建模，引入BNN暴力学习二阶导，顺便引入了不确定性。这些都是文章出彩的思想，也是卖点所在。</p>
<p>而这些<strong>应当</strong>是增广NODE模型的一个维度增广特例，只不过是增广了二阶导。那么ANODE又能增广到什么程度呢？我还需要继续阅读，如果能直接暴力增广学习高阶导，那也不错，在此基础上一定有更加有意思的地方。</p>
<p>Can u get me?</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>不太好整，这<code>markdown</code>上不能整<code>bibtex</code>实在是太难受，给个文献链接直接了事~</p>
<p><a href="http://papers.nips.cc/paper/9497-ode2vae-deep-generative-second-order-odes-with-bayesian-neural-networks" target="_blank" rel="noopener">ODE2VAE: Deep generative second order ODEs with Bayesian neural networks</a></p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Machine Learning</tag>
        <tag>ODE</tag>
        <tag>Flow Model</tag>
      </tags>
  </entry>
  <entry>
    <title>【Jupyter Notebook 1】创建不了文件以及保存文件失败的问题</title>
    <url>/2020/09/25/%E3%80%90Jupyter-Notebook-1%E3%80%91%E5%88%9B%E5%BB%BA%E4%B8%8D%E4%BA%86%E6%96%87%E4%BB%B6%E4%BB%A5%E5%8F%8A%E4%BF%9D%E5%AD%98%E6%96%87%E4%BB%B6%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>具体的错误包括：</p>
<ol>
<li><p>在notebook交互界面一点击保存按钮就会报错</p>
<p><code>[Error 2] No such file or directory: C:\\...\\...\\....ipynb</code></p>
</li>
<li><p>在文件目录界面一创建新文件就会报错</p>
<p><code>An error occurred while creating a new notebook. No such directory: ...</code></p>
</li>
</ol>
</blockquote>
<a id="more"></a>
<h1 id="Bug产生背景"><a href="#Bug产生背景" class="headerlink" title="Bug产生背景"></a>Bug产生背景</h1><p>这次bug的出现是莫名其妙的，白天的时候都好好的，晚上一开电脑就遇到了这个简介中提到cai的两个问题。下面是我的<strong>美丽</strong>截图🤭</p>
<img src="/2020/09/25/【Jupyter-Notebook-1】创建不了文件以及保存文件失败的问题/JN-1.png" title="保存文件失败——美丽的静香打码">
<img src="/2020/09/25/【Jupyter-Notebook-1】创建不了文件以及保存文件失败的问题/JN-2.png" title="创建新文件失败——美丽的静香再打码">
<blockquote>
<p>ps: 本人打码实属打🐎👻才，静香图来源于B站撸大师木源，嗯，好像是这个奇妙的名字…</p>
</blockquote>
<h1 id="探索解决方法"><a href="#探索解决方法" class="headerlink" title="探索解决方法"></a>探索解决方法</h1><p>起初我以为是以往遇到的一个问题，就是notebook的容量太大了，超过5M可能读取起来就会出错，所以我先手动清除了其中的大部分输出，但是点击保存仍然是这个情况。</p>
<p>然后在网上搜了一通也没有找到适合我这种情况的解决方法。</p>
<p>最后注意到一个相关性比较高的回答，它表示路径名称中有特殊字符，好像是中文，不过我忘了具体怎么说的了。虽然以前我都用的中文，但是这次确实是出毛病了啊。我看着一大串的报错信息，觉得这个路径实在是太长了，毕竟我嵌套了很多文件夹…</p>
<p>于是！我在靠近根目录的地方复制了原来的文件，🆗</p>
<p><strong>所以，我这里上述两个bug出现是因为当前文件路径过长…</strong></p>
]]></content>
      <tags>
        <tag>Programming</tag>
        <tag>Bug</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读21】DSN——域分离网络</title>
    <url>/2020/09/19/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB21%E3%80%91DSN%E2%80%94%E2%80%94%E5%9F%9F%E5%88%86%E7%A6%BB%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>DSN全称Domain Separation Network，是我看的第二篇域迁移的文章，这篇文章在DANN之后，思想上的一大亮点是对特征进行了分离，这样潜在地扩大了域迁移的范围~</p>
<p>补充一点，具体实现上的一大亮点是loss的选择，实验表明<span style="border-bottom:1px solid black;">scale-invariant的MSE损失</span>的效果确实很棒~</p>
</blockquote>
<a id="more"></a>
<img src="/2020/09/19/【论文阅读21】DSN——域分离网络/DSN-1.png" title="文献预览">
<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>读这篇文章的时候觉得自己的写作水平有了变化，但是说不清楚是好是坏。因为DSN这篇文章相比DANN有了理论上的改进，但是在写作DANN的时候却漏掉了一些关键的总结，比如这样的模型优缺点分析。</p>
<p>虽然DANN的博客似乎有了点味儿，但是还是要把关键的地方点出来啊💪</p>
<h1 id="为什么读"><a href="#为什么读" class="headerlink" title="为什么读"></a>为什么读</h1><ul>
<li>同DANN一样，IID-CSS上听来的，而且时间在DANN之后，似乎（的确）有了更多（一些）改进的地方</li>
<li>人家虽然不是JMLR，但却是2016年NIPS上的一篇poster啊🙈</li>
<li>在阅读的时候觉得其中可能有个地方有些问题，所以把感受记录下来</li>
</ul>
<p>这次的文献及相关参考链接似乎有点多，在这里列出几个主要的，其它放在文末。</p>
<ol>
<li>DSN文献原文：<a href="http://papers.nips.cc/paper/6254-domain-separation-networks.pdf" target="_blank" rel="noopener">DSN</a></li>
<li>DSN种一个重要损失项——<span style="border-bottom:1px solid black;">scale-invariant的MSE损失</span> 的来源：<a href="http://papers.nips.cc/paper/5539-depth-map-prediction-from-a-single-image-using-a-multi-scale-deep-network.pdf" target="_blank" rel="noopener">Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</a></li>
<li>知乎用户<a href="https://www.zhihu.com/people/ben-ben-18-78" target="_blank" rel="noopener">笨笨</a>的文章<a href="https://zhuanlan.zhihu.com/p/49479734" target="_blank" rel="noopener"><eyd与机器学习>迁移学习：Domain Separation Networks</eyd与机器学习></a>，在我看来这个讲得很干净，内容也都讲到了</li>
<li>CSDN用户<a href="https://blog.csdn.net/hjimce" target="_blank" rel="noopener">hjimce</a>的文章<a href="https://blog.csdn.net/hjimce/article/details/50569474" target="_blank" rel="noopener">深度学习（二十八）基于多尺度深度网络的单幅图像深度估计</a>，该文种的代码让我走出迷雾，弄懂了部分loss的具体实现方式</li>
</ol>
<h1 id="对文章的整体理解"><a href="#对文章的整体理解" class="headerlink" title="对文章的整体理解"></a>对文章的整体理解</h1><p>其实上面所述参考链接中的知乎文章已经够用了。现在大致重新总结一下，让我们从DANN的一个缺点过渡到本文的思路。</p>
<p>DANN在做域迁移的时候，<del>直接</del>将源域和目标域的样本转化到某一特征空间中并混淆它们，达到迁移的效果。</p>
<p>但是这样做，其实有一些问题，即域迁移本身是难以做到全部特征迁移的，因为不同的域对应的特征、特征分布的确是不同的。那么<strong>新的思路</strong>就来了，不同的域之间<span style="border-bottom:1px solid black;">有联系性，也有差异性</span>，那么联系性可以简单描述为特征空间中有一些维度是不同域共享的，称为<strong>共有（common）特征</strong>；反之差异性可以简单描述为特征空间中有另一些维度不是共享的，是域本身的特征，所以称为<strong>私有（private）特征</strong>。</p>
<p>因此，基于这个新思路，我们希望基于DANN的结构，把共有的特征和私有的特征分离开来，期望能够更好地描述不同域之间的关系，达到更好的效果。到这里，文章标题<em>Domain Separation Network</em>的思路就完全确定了，下面就是怎么去实现了。</p>
<h1 id="DSN模型结构"><a href="#DSN模型结构" class="headerlink" title="DSN模型结构"></a>DSN模型结构</h1><p>👴话不多说，直接暴力上结构！</p>
<img src="/2020/09/19/【论文阅读21】DSN——域分离网络/DSN-2.png" title="DSN结构">
<p>结构还是比较清晰的，咱们直接点明该结构是在哪一（几）块完成公、私有特征分离的操作的。就是除了右下角分类外的全部结构，但是<strong>别慌</strong>，别看框框多，其实都是高度对称的结构。类比DANN，输入从全部样本 $x$ 分离为有标记的两域样本 $x_s$ 和 $x_t$，下标自然是源域source和目标域target了；同时输出也从全部特征空间的 $f$ 转化为了公有特征 $h_c$ 和私有特征 $h_p$。</p>
<p>成功分离的保证是暹罗网络。啥？暹罗网络？想到了吧哈哈，哥我曾经看过一篇语音生成的文章<a href="https://arxiv.org/abs/1910.03713?context=eess.AS" target="_blank" rel="noopener">MelGAN-VC</a>，现在看看这个特征分离的操作和这篇文章不是有点像，而是完全一致！都是利用Siamese Network，中文译为暹罗网络或者孪生网络，使用的是特征分离对抗➕样本恢复的形式。具体懒得说了，反正我早学过了🤭，这里就列一篇介绍暹罗网络的博客吧<a href="https://blog.csdn.net/weixin_41866216/article/details/90643068" target="_blank" rel="noopener">Siamese Network：孪生网络 简单理解</a>。</p>
<p>好了，上面把文章的核心对抗方法直接跳过了，那么模型的结构其实就讲完了😄</p>
<h1 id="Loss组成"><a href="#Loss组成" class="headerlink" title="Loss组成"></a>Loss组成</h1><p>讲到这里，忘了说，文章的两大亮点，一是利用暹罗网络分离了公私有特征；二是loss的具体形式了，先点明，就是<span style="border-bottom:1px solid black;">scale-invariant的MSE损失</span>的使用。loss的具体组成请参阅参考链接中的知乎文章<a href="https://zhuanlan.zhihu.com/p/49479734" target="_blank" rel="noopener"><eyd与机器学习>迁移学习：Domain Separation Networks</eyd与机器学习></a>，这里不再赘述。但是其中很多细节还需要研究研究。</p>
<p>其实在阅读这部分的时候，我遇到了很多障碍，这里我主要对<span style="border-bottom:1px solid black;">scale-invariant的MSE损失</span>有以下疑问，该loss似乎在全局图像恢复上的效果比一般的MSE好，效果好的理由是文章中的实验验证：</p>
<ol>
<li><p>但为什么是这种形式呢？（下文解答）</p>
</li>
<li><p>对于图像恢复的颜色和色彩强度没有要求，为什么？（<strong>待求证！</strong>）</p>
</li>
<li><p>这个loss的定义很奇怪，原文中的形式是 $(4)$ 式，前一项记为MSE，后一项记为SIMSE。而我在 <a href="https://github.com/fungtion/DSN" target="_blank" rel="noopener">https://github.com/fungtion/DSN</a> 找到的代码中看别人复现的loss用的是此MSE+SIMSE而不是减？（下文解答）</p>
</li>
<li><p>此loss中前者MSE的计算并不是什么矩阵 $A^TA$ 特征值绝对值最大值（参考 <a href="https://www.zhihu.com/question/50557667" target="_blank" rel="noopener">https://www.zhihu.com/question/50557667</a> ）而是普通的矩阵元素差值平方和？那这里矩阵的 $L_2$ 范数究竟是什么？</p>
</li>
</ol>
<p>针对问题 $1、3、4$，我先后查阅了这种<span style="border-bottom:1px solid black;">scale-invariant的MSE损失</span>的原文，2014NIPS的<em>Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</em>，以及CSDN博客<a href="https://blog.csdn.net/hjimce/article/details/50569474" target="_blank" rel="noopener">深度学习（二十八）基于多尺度深度网络的单幅图像深度估计</a>，和G站上他人的代码 <a href="https://github.com/fungtion/DSN" target="_blank" rel="noopener">https://github.com/fungtion/DSN</a> 。从这三处，我将给出我的分析过程。</p>
<p><strong>首先</strong>，G站上的代码是本文的<code>PyTorch</code>实现，loss函数的定义在 <a href="https://github.com/fungtion/DSN/blob/master/functions.py" target="_blank" rel="noopener">https://github.com/fungtion/DSN/blob/master/functions.py</a> ，训练的过程在 <a href="https://github.com/fungtion/DSN/blob/master/train.py" target="_blank" rel="noopener">https://github.com/fungtion/DSN/blob/master/train.py</a> ，由于把源域和目标域损失都分开算了，所以目标域上没标签，就没有分类损失，源域上则有。其中的MSE项和SIMSE项分别是（引用他人代码啦注明出处应该可以…）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSE</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MSE, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, pred, real)</span>:</span></span><br><span class="line">        diffs = torch.add(real, -pred)</span><br><span class="line">        n = torch.numel(diffs.data)</span><br><span class="line">        mse = torch.sum(diffs.pow(<span class="number">2</span>)) / n</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> mse</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SIMSE</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(SIMSE, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, pred, real)</span>:</span></span><br><span class="line">        diffs = torch.add(real, - pred)</span><br><span class="line">        n = torch.numel(diffs.data)</span><br><span class="line">        simse = torch.sum(diffs).pow(<span class="number">2</span>) / (n ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> simse</span><br></pre></td></tr></table></figure>
<p>这些就是代码实现中的具体loss计算方式，而我觉得原文的 $(4)$ 式右边第一项写成矩阵的 $L_2$ 范数是<strong>不合适</strong>的。毕竟 <code>mse = torch.sum(diffs.pow(2)) / n​</code> 说明了这不是正规的矩阵 $L_2$ 范数。这样<strong>问题 $4$ 就解决了</strong>。</p>
<p><strong>其次</strong>，在CSDN博客中，作者有一段该loss的代码，不过是TF，我不太熟悉，但是写得很棒。关键是代码的注释里有一句</p>
<blockquote>
<p>文献中公式４，参数$\lambda$取值为0.5。公式采用的是简化为（ｎ×sum(d^2)-λ*(sum(d))^2）/(n^2)</p>
</blockquote>
<p>这提示，是不是代码实现简化了特征根的计算，用代码中的形式来代替计算呢？确实看了<em>Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</em> $(4)$ 式，形式上是一致的。那么是不是替代品呢？是的，原文是在log空间计算的，所以度量其实是 $\displaystyle \alpha(x, \hat{x}) = \dfrac{1}{n}\sum_i \left(\log x_i - \log \hat{x}_i\right)$ ，所以现在本文用正常的原空间有 $\displaystyle \alpha(x, \hat{x}) = \dfrac{1}{n}\sum_i \left(x_i - \hat{x}_i\right)$，所以带入参考文献<em>Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</em>的 $(1)$ 式有</p>
<script type="math/tex; mode=display">\begin{align} \displaystyle D(x, \hat{x}) &= \dfrac{1}{2n}\sum_{i=1}^n \left(x_i - \hat{x}_i + \dfrac{1}{n}\sum_i \left(x_i - \hat{x}_i\right)\right)^2 \\ &= \dfrac{1}{2n^2}\sum_{i, j} \left(\left(x_i - x_j\right) - \left(\hat{x}_i - \hat{x}_j\right)\right)^2 \\ &\triangleq \dfrac{1}{n}\sum_i d_i^2 - \dfrac{1}{n^2}\sum_{i, j} d_i d_j \\ &= \dfrac{1}{n}\sum_i d_i^2 - \dfrac{1}{n^2}\left(\sum_i d_i\right)^2 \end{align}</script><p>这的确就是代码实现中的具体计算方式，这进一步说明代码种的形式确实是SIMSE的形式，说明原文的 $(4)$ 式右边第一项写成矩阵的 $L_2$ 范数是不合适的。这样<strong>问题 $1$ 就被推导出来了</strong>。</p>
<p><strong>最后</strong>，针对问题 $3$，我觉得这位兄弟的代码不小心写错了，好了<strong>问题 $3$ 也解决了</strong>~</p>
<hr>
<p>综上，这篇文章我基本上都看完了，还剩一些小问题藏在加粗字中，待办了~</p>
<h1 id="其它小细节"><a href="#其它小细节" class="headerlink" title="其它小细节"></a>其它小细节</h1><ul>
<li><p>学到一个新函数：<a href="https://pytorch.org/docs/master/generated/torch.numel.html" target="_blank" rel="noopener"><strong>torch.numel</strong></a>，输入是tensor，输出是全部元素的个数，感觉就是张量的size</p>
</li>
<li><p><strong>MMD损失是什么</strong>我看不懂，需要再去具体学习！</p>
</li>
<li><p>文章的其它部分，尤其是对比实验、其它loss都可以参考<a href="https://zhuanlan.zhihu.com/p/49479734" target="_blank" rel="noopener">迁移学习：Domain Separation Networks</a>，讲得挺好</p>
</li>
<li><p>文章原文中说代码在这<a href="https://github.com/tensorflow/models/domain_adaptation，但👴没找着" target="_blank" rel="noopener">https://github.com/tensorflow/models/domain_adaptation，但👴没找着</a>…</p>
</li>
<li><p>difference loss用矩阵乘积的形式超过了DANN loss的效果！说明这种分离公共、私有特征的度量是有效的！</p>
</li>
<li><p>上面反复说的知乎文章下面有个评论：</p>
<blockquote>
<p>您好，请问在输入公有特征提取器的时候，是把两个域的样本整体用两个矩阵输入进去，还是从每次从目标域和源域各选取一个成为一对输入进去呢</p>
</blockquote>
<p>从文章原文 $(3)$ 式的loss看应该是全部数据喂进去了，从代码看也是这样，不过多了点细节，是分源域样本和目标域样本分别通通喂进去的</p>
</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>文章主要的4篇参考文献在<code>为什么读</code>章节中已经给出。</p>
<p>[5] 黑猫紧张. PN-35: Domain Separation Networks (NIPS 2016)[EB/OL]. <a href="https://zhuanlan.zhihu.com/p/82403668" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/82403668</a>, 2019-09-13.</p>
<p>[6] gdtop818. Domain Separation Networks[EB/OL]. <a href="https://blog.csdn.net/weixin_37993251/article/details/91472097" target="_blank" rel="noopener">https://blog.csdn.net/weixin_37993251/article/details/91472097</a>, 2019-06-11.</p>
<p>[7] PoemK. Domain Separation Networks (NIPS 2016)[EB/OL]. <a href="https://blog.csdn.net/yskyskyer123/article/details/95664755" target="_blank" rel="noopener">https://blog.csdn.net/yskyskyer123/article/details/95664755</a>, 2019-07-12.</p>
<p>[8] 究竟灰. 论文阅读：Depth Map Prediction from a Single Image using a Multi-Scale Deep Network[EB/OL]. <a href="https://zhuanlan.zhihu.com/p/29312227" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29312227</a>, 2019-09-18.</p>
<p>[9] Xuefeng_BUPT. Depth Map Prediction from a Single Image using a Multi-Scale Deep Network（NIPS2014）论文阅读[EB/OL]. <a href="https://blog.csdn.net/chishuideyu/article/details/83573174" target="_blank" rel="noopener">https://blog.csdn.net/chishuideyu/article/details/83573174</a>, 2018-10-31.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Machine Learning</tag>
        <tag>Domain Adaptation</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读20】DANN——利用域对抗进行简单的域迁移</title>
    <url>/2020/09/17/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB20%E3%80%91DANN%E2%80%94%E2%80%94%E5%88%A9%E7%94%A8%E5%9F%9F%E5%AF%B9%E6%8A%97%E8%BF%9B%E8%A1%8C%E7%AE%80%E5%8D%95%E7%9A%84%E5%9F%9F%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>第一次看领域迁移的文章~</p>
</blockquote>
<a id="more"></a>
<img src="/2020/09/17/【论文阅读20】DANN——利用域对抗进行简单的域迁移/DANN-1.png" title="文献预览">
<h1 id="为什么读"><a href="#为什么读" class="headerlink" title="为什么读"></a>为什么读</h1><ul>
<li>IID组会上听来的，好像很炫酷</li>
<li>人家是JMLR啊，而且似乎是相关工作的开山作？好像是，日后再证</li>
<li>下面的参考博客中把该文章的模型结构都说的很清楚，但是感觉对loss的描述不够精确细节，我第一遍看没法get到模型的实现机理，因此单独细看了loss，并记录于此博客</li>
</ul>
<p>文献及相关参考链接：</p>
<ol>
<li><p><a href="https://www.jmlr.org/papers/volume17/15-239/15-239.pdf" target="_blank" rel="noopener">https://www.jmlr.org/papers/volume17/15-239/15-239.pdf</a> 或者 <a href="https://arxiv.org/abs/1505.07818" target="_blank" rel="noopener">https://arxiv.org/abs/1505.07818</a></p>
</li>
<li><p><a href="https://blog.csdn.net/weixin_37993251" target="_blank" rel="noopener">gdtop818</a>的CSDN博客<a href="https://blog.csdn.net/weixin_37993251/article/details/89398433" target="_blank" rel="noopener">Domain-Adversarial Training of Neural Networks</a></p>
</li>
<li><p><a href="https://www.zhihu.com/people/sabrina-65-94" target="_blank" rel="noopener">我的猫爱睡觉</a>的知乎文章<a href="https://zhuanlan.zhihu.com/p/199514982" target="_blank" rel="noopener">Domain-Adversarial Training of NN(DANNs)</a>，包括了几个有用的链接</p>
</li>
</ol>
<h1 id="DANN及其loss构造"><a href="#DANN及其loss构造" class="headerlink" title="DANN及其loss构造"></a>DANN及其loss构造</h1><p>首先还是介绍一下<strong>DANN——Domain-Adversarial Training of Neural Networks</strong>，在我理解是一篇利用了对抗网络来进行简单的领域迁移的文章。这篇文章除了迁移、对抗，还有表示学习的内容，因为具体操作是通过特征的转换完成的。文章思想<strong>核心</strong>是通过让源域和目标域的样本特征不能被很好地辨识，同时源域上的分类效果很好，达到<strong>混淆样本的特征表示</strong>，使分类器在此特征表示上能够完成域迁移的效果。</p>
<p>话不多说，直接开干原文中第4章的模型结构图，如下图所示：</p>
<img src="/2020/09/17/【论文阅读20】DANN——利用域对抗进行简单的域迁移/DANN-2.png" title="DANN">
<p>那么DANN分3个部分：</p>
<ol>
<li><p>左上角是特征提取器 $G_f$，feature的提取器。输入 $x$ 包括源域和目标域的样本，输出转化后的特征。其目的一方面是特征提取，另一方面是在特征空间中混淆源域和目标域的样本，即两个域的样本对齐，以便迁移；</p>
</li>
<li><p>右上角是分类器 $G_y$，对标签 $y$ 分类没得说。输入 $f$ 只有源域的样本特征了，因为目标域没有标签，监督不了的。只要保证源域上的分类效果良好，由于两类特征被特征提取器  $G_f$ 混淆，所以源域上效果好，目标域上也便于迁移（不知道有没有证明，应该是实验验证）</p>
</li>
<li><p>右下角是域分类器 $G_d$，domain的分类器，这个要做对抗的。输入则是源域和目标域的特征，目标是在特征空间上分离两个域的样本，这一点可以和特征提取器 $G_f$ 形成对抗。注意 $G_d$ 和 $G_f$ 中甲有个<strong>梯度反传层（GRL）</strong>，它的结构是</p>
<blockquote>
<p>leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during the backpropagation</p>
</blockquote>
</li>
</ol>
<p>现在对抗就在这里，因为 $G_f$ 要求两类样本在特征空间中分不开，而 $G_d$ 则反之，所以两部分的loss符号是相反的（可以再加个系数），即 $G_d$ 正常训练，loss符号不变，但是反传梯度的时候，希望 $G_f$ 分离不了样本，梯度是反过来的。</p>
<p>反应在BP的过程中就是，梯度从最后 $G_d$ 的输出 $L_d$ 开始计算，先正常反传 $G_d$ 上的梯度 $\dfrac{\partial L_d}{\partial \theta_d}$，经过GRL时加个负号（与正系数 $\lambda$），所以在 $G_f$ 上的梯度就是 $\dfrac{\partial L_d}{\partial \theta_f}$。</p>
<p>进一步地，反应在损失函数中，让我们康康原文 $(18)$ 式：</p>
<script type="math/tex; mode=display">\displaystyle E = \dfrac{1}{n}\sum_{i=1}^n \mathcal{L}_y (G_y (G_f (x_i; \theta_f); \theta_y), y_i) - \lambda \left(\dfrac{1}{n}\sum_{i=1}^n \mathcal{L}_d (G_d (GRL(G_f (x_i; \theta_f)); \theta_d), d_i) + \dfrac{1}{n^\prime}\sum_{i=n+1}^N \mathcal{L}_d (G_d (GRL(G_f (x_i; \theta_f)); \theta_d), d_i)\right).</script><p>这样 $E$ 就和上面的DANN的三部分分析对上了，$E$ 右端第一项就是分类器 $G_y$ 的分类误差，梯度是正常反传的，所以loss也是正常的形式。$E$ 右端第二项是域分类器的分类误差，但是分为了两部分，括号里第一项中index为从 $1$ 到 $n$ 的求和是源域的样本分类误差，第二项中index为从 $n+1$ 到 $N$ 的求和是目标域的样本分类误差，其中都经过了GRL的作用，此时要求特征提取器 $G_f$性能不行，因此该误差反传时前面是负数。</p>
<p>现在loss的组成分析完了，但是<strong>对抗体现在loss的哪里</strong>呢？我想了好一会儿…哎呀还是我菜了，对抗体现在 $E$ 的左右两项啊，第一项是正常的，第二项的反过来的嘛，<strong>对抗就是体现在中间那个负号</strong>上~</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Meta Learning</tag>
        <tag>Machine Learning</tag>
        <tag>Domain Adaptation</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读19】Do We Need Zero Training Loss After Achieving Zero Training Error</title>
    <url>/2020/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB19%E3%80%91Do-We-Need-Zero-Training-Loss-After-Achieving-Zero-Training-Error/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>一种新的正则方法flooding，相比以往直接考虑控制模型复杂度，flooding考虑的是控制训练loss的水平</p>
</blockquote>
<a id="more"></a>
<img src="/2020/09/16/【论文阅读19】Do-We-Need-Zero-Training-Loss-After-Achieving-Zero-Training-Error/Zero_Training_Loss_No_1.png" title="Title">
<h2 id="为什么读"><a href="#为什么读" class="headerlink" title="为什么读"></a>为什么读</h2><p>文献及相关介绍的链接：</p>
<ol>
<li><p><a href="https://arxiv.org/abs/2002.08709" target="_blank" rel="noopener">https://arxiv.org/abs/2002.08709</a></p>
</li>
<li><p><a href="https://blog.csdn.net/oYeZhou" target="_blank" rel="noopener">叶舟</a>的CSDN博客<a href="https://blog.csdn.net/oYeZhou/article/details/107667317" target="_blank" rel="noopener">一种新的正则化方法——flooding，只需一行代码即可使用</a></p>
</li>
<li><p>新智元的知乎文章<a href="https://zhuanlan.zhihu.com/p/203417649" target="_blank" rel="noopener">【论文】一行代码发一篇ICML？</a></p>
</li>
</ol>
<p>至于为什么读这篇文章？🤭，因为看起来简单啊，一行代码写出来的，思想上一定是很有意思的，事实也是如此。只是这个有关训练loss和训练error的讨论让我有点迷糊，放在这里，若干纪元以后再来探访~</p>
<h2 id="从标题到文章主题"><a href="#从标题到文章主题" class="headerlink" title="从标题到文章主题"></a>从标题到文章主题</h2><p>标题点明了文章讨论的两个概念：<strong>训练loss</strong>和<strong>训练error</strong>之间的关系</p>
<p>为什么会注意到这两个概念呢？从我个人的观点，<strong>前者有一点泛化</strong>的意思在里面，因为loss大了可能欠拟合，小了可能过拟合，合适的loss水平确实对各种训练方法有点影响（个人经验）；后者其实也有一点泛化的味道，但我更喜欢说它<strong>代表拟合</strong>，理由差不多，error太小了，说明模型把训练数据给背下来了，是大概率过拟合的，毕竟极其强大的网络可以拟合数据，甚至“记住”所有训练数据，达到<strong>训练error=0</strong>，但这显然有点问题，大概率会过拟合。</p>
<blockquote>
<p>文章里有一句是别人工作的结论，我觉得不太对劲：</p>
<p>“learning until zero training error is meaningful to achieve a lower generalization error”</p>
<p>对此我保留意见，文章略过了这一点，直接开始思考🤔…</p>
</blockquote>
<p>这就引出了文章标题的思考：</p>
<blockquote>
<p>Q: Do we need zero training loss after achieving zero training error?</p>
</blockquote>
<h2 id="从传统正则到针对loss水平"><a href="#从传统正则到针对loss水平" class="headerlink" title="从传统正则到针对loss水平"></a>从传统正则到针对loss水平</h2><p>一般我们会用正则控制模型复杂度（但不是完全控制loss，只是加了一项），但它的出发点不是避免0<strong>训练loss</strong>，所以它其实也控制不了<strong>训练loss</strong>的水平，即往往最后的<strong>训练loss</strong>也会有偏大或偏小的现象，达不到合适的水平。</p>
<p>另外，一般来说<strong>训练error</strong> $\rightarrow$ 0是可以的，但是当前者达到时，<strong>训练loss</strong>还需要继续下降么，还需要继续训练模型么？</p>
<blockquote>
<p>even if we add fooding, we can still memorize the training data</p>
<p>flooding剑指当<strong>训练error</strong>-&gt;0时，<strong>训练loss</strong>维持在比较小的值而不是0</p>
<p>假设之前希望<strong>训练error</strong>-&gt;0的那句结论的说法是对的，那么控制<strong>训练loss</strong>的水平确实有点意思，但还要确定这个水平是多少</p>
</blockquote>
<h2 id="Flooding正则"><a href="#Flooding正则" class="headerlink" title="Flooding正则"></a>Flooding正则</h2><img src="/2020/09/16/【论文阅读19】Do-We-Need-Zero-Training-Loss-After-Achieving-Zero-Training-Error/Zero_Training_Loss_No_2.png" title="形象的模型描述">
<p>针对这一点，提出flooding的方法，如上图很直观啦，这个洪水的水面高度就相当于最终<strong>训练loss</strong>的水平，称为fooding level。思想很简单，就是控制<strong>训练loss</strong>的水平，当loss较高就正常SGD，loss偏低就<strong>反向进行梯度上升</strong>。思路简单的结果是实现极其简单，如下式，变量flood就是新的loss，变量b就是fooding level。一行代码即可且可应用于<strong>一切</strong>以往基于loss梯度下降的方法。</p>
<script type="math/tex; mode=display">flood = (loss-b).abs()+b</script><p>这样就有个麻烦，这个fooding level<strong>到底是多少</strong>？我们把<strong>训练loss</strong>控制到这个level<strong>为什么能有效</strong>？</p>
<blockquote>
<p>In Appendix F, we show that during this period of random walk, there is an increase in fatness of the loss function</p>
<p>附录F并没有提及理论上为何有效，似乎在附录A有定理证明</p>
<p>附录F的图看了几遍也没有看明白，可能需要阅读flatness的相关知识吧</p>
<p>附录A的证明很清晰，表明这种操作是有效的√</p>
</blockquote>
<p>那么现在只差fooding level<strong>到底是多少</strong>这个问题了。文章里表示这个可以手动设置，也可以设置验证（文中的实验就是这么做的），或者当作超参数进行超参优化；个人以为还是最后的舒服。</p>
<h2 id="注"><a href="#注" class="headerlink" title="注"></a>注</h2><p>这篇文章有个<strong>有意思的知识点</strong>表一，是许多正则方法的比较，以后需要的话可以看看 √</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Machine Learning</tag>
        <tag>Regularization</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读18】Meta-Learning Symmetries by Reparameterization——利用元学习学习保持运算等变性的结构</title>
    <url>/2020/08/31/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB18%E3%80%91Meta-Learning-Symmetries-by-Reparameterization%E2%80%94%E2%80%94%E5%88%A9%E7%94%A8%E5%85%83%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E4%BF%9D%E6%8C%81%E8%BF%90%E7%AE%97%E7%AD%89%E5%8F%98%E6%80%A7%E7%9A%84%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>这个文章可是难到我了，谁让年轻的时候抽🐘代数学得菜呢？</p>
<p>好在可以举例子明白其中的道理┭┮﹏┭┮</p>
</blockquote>
<a id="more"></a>
<img src="/2020/08/31/【论文阅读18】Meta-Learning-Symmetries-by-Reparameterization——利用元学习学习保持运算等变性的结构/MSR.png" title="MSR">
<h1 id="为什么读"><a href="#为什么读" class="headerlink" title="为什么读"></a>为什么读</h1><ul>
<li>元学习新作</li>
<li>作者之一是Finn啊doge</li>
</ul>
<p>文献链接：<a href="https://arxiv.org/abs/2007.02933" target="_blank" rel="noopener">Meta-Learning Symmetries by Reparameterization</a>，原作者提交于2020/07，挂出来了。</p>
<h1 id="文献总结"><a href="#文献总结" class="headerlink" title="文献总结"></a>文献总结</h1><p>用强大的神经网络去学习一些参数，它们表示某种变换（如卷积），能对特定的任务（如图像相关）满足某种等变性。</p>
<p>其实就是<font color="#0000FF">重参数化网络的隐层，学习像卷积这样的运算的模式pattern，可以满足像对图像平移变换的等变性，并利用元学习进一步学习此pattern</font></p>
<h1 id="文献内容"><a href="#文献内容" class="headerlink" title="文献内容"></a>文献内容</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>和文献总结一致！</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>背景一方面是CNN，可以认为卷积满足对图像平移变换的等变性，如果像卷积这样的运算可以推广，并满足各种特定运算的等变性，那一定是很🐂🍺。</p>
<p>另一方面是文章里的related work，不过我暂时不感兴趣没看。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>说实话我感觉不好总结，姑且分两个部分，一部分讲等变性对应的代数基础，另一部分讲网络参数设置的形式（以使用元学习训练）。</p>
<h3 id="等变性的代数理论"><a href="#等变性的代数理论" class="headerlink" title="等变性的代数理论"></a>等变性的代数理论</h3><p>3.2节提到了群卷积是什么，我不（代）太（数）明（太）白（差），举个例子吧。</p>
<p>网络隐层的输入输出都是空间 $X$ 的函数。比如是 $R^n$ 中一个向量，它相当于一个映射，从指标集 $X={1, \cdots, n}$ 映射到对应的 $n$ 个实数。</p>
<p>$X$ 上的实值函数集合定义为 $F_X$ ，群 $G$ 作用在 $F_X$ 上，通过 $\pi:G\rightarrow GL(F_X)$，这个 $\pi$ 称为representation，把群 $G$ 中的元素 $g$ 映为 $F_X$ 中的一个可逆线性变换。</p>
<p>注意 $G$ 对 $X$ 的作用是一个 $g\rightarrow F_X$ 的映射，</p>
<p>即 $F_X$ 中的可逆线性变换 $f$，当然 $f$ 作用的元素是 $X$ 中的元素 $x$ ，因为 $f$ 是 $X$ 上的实值函数嘛，现在 $\pi(g)$ 是一个可逆线性变换，假设就是 $f$ 的逆变换，所以 $\pi(g)$ 复合 $f$ 是一个函数且是恒等映射，即 $(\pi(g)f)(x) = I(x) = x$</p>
<p>文章写的符号我没看懂，但是看例子懂了， $G=X$ 时，定义作用为 $R^2$ 上的加法平移，则 $g^{-1}x$ 是 $x-g$ ， $gx$ 是 $x+g$ ，</p>
<img src="/2020/08/31/【论文阅读18】Meta-Learning-Symmetries-by-Reparameterization——利用元学习学习保持运算等变性的结构/MSR2.png" title="举个例子我就懂啦！">
<p>3.3节表示，定义网络隐层为一个函数 $\phi:F_X\rightarrow F_Y$， $G$ 有等变性，举个简单例子， $X$ ， $Y$ 是输入图片网格和输出特征图的网格，所以， $F_X$ 中的元素就是网格大小的图片， $F_Y$ 中的元素就是特征图大小的图片；3式左边就是先对隐层输入做一个平移变换，再输出特征图，右边相当于先输出特征图，再做输出空间上对应的平移变换。</p>
<p>一个特例是，如果 $\pi$ 是id恒等映射，即 $X$ 、 $Y$ 空间上平移是一致，一样的，那么就叫不变性，先平移再映射和先映射再平移是一样的；注意这里的两个平移现在是数值上相等，但空间是其实分别是 $X$ 和 $Y$ 。</p>
<p>由于等变性对于函数复合可以保持，那么如果所有隐层都有等变性，那么整个网络也有。</p>
<p>线性层是基本的层，希望用它学习参数的共同pattern，达到使之有等变性（像群卷积）。</p>
<p>最后重述一下MSR方法吧，即学习和编码等变性，这个学习就是学习隐层参数的共享pattern，编码是指在代数上找到上文所述的一个灵活的representation。</p>
<h3 id="网络参数形式"><a href="#网络参数形式" class="headerlink" title="网络参数形式"></a>网络参数形式</h3><p>其实具体的例子参考文中的图1即可。</p>
<p>4.1节表示，隐层权重矩阵是图1那样形式叠加的矩阵，这就是一种constraint，就是一种pattern，结果是对平移有等变性。现在希望推广到其它的变换，旋转，镜像什么的，但是咱们没有思路怎么去做，所以就用简单的FC层模拟，设置 $y=\phi(x)=Wx, W\in \mathbb{R}^{m\times n}$ ，然后学习 $W$ 就是在学习这个pattern了。</p>
<p>接着阐释了这个pattern不是直接去学习 $W$ ，这样也看不出有什么pattern。我猜是参考了矩阵分解的概念， $vec(W)=Uv, v\in\mathbb{R}^k, U\in\mathbb{R}^{mn\times k}$ ， $W$ 分解为伪对称矩阵 $U$ 和滤波参数向量 $v$ 的积， $U$ 的维度就是 $W$ 完全展开，再复制个 $k$ 份， $k$ 就是参量 $v$ 的维度，应该可以自行设置。所以pattern实际上就是 $U$ ， $U$ 可以限制最后 $W$ 的形式，滤波参数 $v$也要根据实际任务训练。</p>
<blockquote>
<p>For example, the sharing pattern of standard convolution guarantees that the weight matrix is constant along any diagonal, reducing the number of per-task parameters</p>
</blockquote>
<p>显然 $U$ 的维度会很高，怎么办？Kronecker factorization，就是参数化的方式，和我之前看的文章和做的实验都是<strong>一致</strong>的想法！</p>
<p>4.2节继续，那么 $U$ 具体怎么设置呢？和输入的形式有一些关系…举例子吧，从基本的网格空间 $X$ 上选位置，得到输入 $f$ ，即 $f:X\rightarrow R^c$ ，这个输入是向量（矩阵或者矩阵展开应该也行）。假定 $f$ 是 $s$ 维的， $\bar{f}\in\mathbb{R}^s$ ，其中 $\bar{f}_i=f(x_i)$  。引理就是保证了可以由这个 $U$ 保持群的作用，即保持某种运算的等变性；引理分两部分，一部分是给定群后，群的作用就定了，即运算定了，该运算的等变性存在 $U$ 来保持；第二部分是此 $U$ 存在时，任何 $G$ 卷积都可以由 $vec(W)$ 表示出来。所以 $U$ 就设置成一个固定长度的参数向量，把它参数化为矩阵或者向量本身都可以，进一步得到pattern的形式，这样就是能训练的。证明估计我也看不懂了…</p>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>优点：</p>
<ul>
<li>优点：学习保持等变量性质的参数pattern—&gt;泛化，减少参数用量，适用小样本</li>
<li>从卷积的应用开始，等变性很重要—&gt;是未来发展的方向之一</li>
<li>未来自动寻找潜在的特殊结构的算子？像卷积这样的带有等变性的</li>
<li>quickly discovering symmetries的应用</li>
</ul>
<p>缺点：</p>
<ul>
<li>我也不知道，可能要根据等变性做监督，训练得到 $U$ 具体的形式，这算不算一种手动设置呢。好吧我其实在杠，还是暂时挑不出毛病</li>
</ul>
<h1 id="附录——“快乐”的阅读体验"><a href="#附录——“快乐”的阅读体验" class="headerlink" title="附录——“快乐”的阅读体验"></a>附录——“快乐”的阅读体验</h1><p>我的阅读历程😭😥😪😵，仅个人记录于此，不建议看官阅读…应该看不懂的，只是一些个人阅读思路。<strong>和上文基（是）本（水）重（字）叠（数）</strong>。</p>
<ul>
<li><p>标题中什么叫元学习的对称性？</p>
<p>这里应该<strong>不是</strong>指“元学习有对称性”，而是一个广泛的概念，是要学习<strong>某种</strong>对称性，包括数据本身性质、任务之间关联性，以及某些对数据运算的性质（如旋转镜像这样的运算算子）</p>
</li>
<li><p>摘要中：</p>
<ul>
<li><p>等变量equivariances是什么意思？</p>
<p>参考链接<a href="https://zhuanlan.zhihu.com/p/34288976" target="_blank" rel="noopener">CNN 中等变性（equivariant）与不变性（invariant）的关系</a>即可，确实是，经过某些特定运算，特定的输入<strong>模式</strong>能够保持不变</p>
</li>
<li><p>感觉文章的<strong>卖点</strong>是从数据（含某种对称性）中（元）学习这个所谓的等变量，学习的方式是用网络<strong>学习等变量对应的parameter sharing patterns</strong>，用参数化形式学习表达出这种模式！</p>
</li>
</ul>
</li>
<li><p>引言中：</p>
<ul>
<li><strong>等变量的起源——CNN</strong>，利用等变量做了某种对称变换，来保护参数，提高泛化能力。什么意思？大概就是卷积（层）这个操作，这个变换允许输入输出的对应变化（平移）有不变性，叫做translation equivariant，即输入（图像）经过特定的平移变换后输出也会经过类似的变换。</li>
<li>第一段提到了translation equivariant，这是个名词，专指<strong>平移不变性</strong>，不是混淆为广泛的变化！知乎<a href="https://zhuanlan.zhihu.com/p/34288976" target="_blank" rel="noopener">CNN 中等变性（equivariant）与不变性（invariant）的关系</a>讲得很好！</li>
<li>上面的指CNN利用的卷积，其本身有这种平移不变性，因此在特定任务上（图像相关）表现很好。现在本文试图找其它的内在的等变量。</li>
<li><strong>目的</strong>出现！引言第3段，在神经网络中<strong>自动学习等变量</strong>。有利于迁移，不需手动设计像卷积这样有平移等变性的算子，直接去寻找等变量。怎么自动学习？<font color="#0000FF">重参数化网络的隐层，以学习表示共享的pattern，并利用元学习进一步学习共同的pattern</font>。</li>
<li>引言最后一段讲的什么？网络怎么等价于有限对称群？这对称性和拓扑怎么关系上了？抽🐘代数🐕表示很淦…</li>
</ul>
</li>
<li><p>相关工作看不懂，现在懒得看</p>
</li>
<li><p>先修知识章节表示3.2、3.3节会给出等变量和群卷积的定义…<strong>早说啊</strong></p>
<ul>
<li><p>3.1回顾MAML，这是典型的元学习，MAML也是这样么，和我记的不太一样啊？我要回去重看MAML了，我就记得它是学一个好的初始参数，具体怎么学的忘了</p>
</li>
<li><p>3.2，对称性和等变性是群对集合作用的性质…复习一堆拓扑代数知识hahaha…群的定义，加法群…同态，从群 $G$ 映到 $X$ 的自同构群…自同构群，是群 $X$ 到自己的同构变换，且双射…</p>
</li>
<li><p>3.2，群卷积是什么，隐层的输入输出都是空间 $X$ 的函数。比如说 $R^n$ 中一个向量相当于一个映射，从指标集 $X={1, \cdots, n}$ 映射到对应的 $n$ 个实数。</p>
<img src="/2020/08/31/【论文阅读18】Meta-Learning-Symmetries-by-Reparameterization——利用元学习学习保持运算等变性的结构/MSR2.png" title="举个例子我就懂啦！">
<p>$X$ 上的实值函数集合定义为 $F_X$ ，群 $G$ 作用在 $F_X$ 上，通过 $\pi:G\rightarrow GL(F_X)$，这个 $\pi$ 称为representation，把群 $G$ 中的元素 $g$ 映为 $F_X$ 中的一个可逆线性变换。</p>
<p>注意 $G$ 对 $X$ 的作用是一个 $g\rightarrow F_X$ 的映射，</p>
<p>即 $F_X$ 中的可逆线性变换 $f$，当然 $f$ 作用的元素是 $X$ 中的元素 $x$ ，因为 $f$ 是 $X$ 上的实值函数嘛，现在 $\pi(g)$ 是一个可逆线性变换，假设就是 $f$ 的逆变换，所以 $\pi(g)$ 复合 $f$ 是一个函数且是恒等映射，即 $(\pi(g)f)(x) = I(x) = x$</p>
<p>文章写的符号我没看懂，但是看例子懂了， $G=X$ 时，定义作用为 $R^2$ 上的加法平移，则 $g^{-1}x$ 是 $x-g$ ， $gx$ 是 $x+g$ ，</p>
</li>
<li><p>3.3，定义网络隐层为一个函数 $\phi:F_X\rightarrow F_Y$， $G$ 等变性，举个简单例子， $X$ ， $Y$ 是输入图片网格和输出特征图的网格，所以， $F_X$ 中的元素就是网格大小的图片， $F_Y$ 中的元素就是特征图大小的图片；3式左边就是先对隐层输入做一个平移变换，再输出特征图，右边相当于先输出特征图，再做输出空间上对应的平移变换。</p>
<p>如果pi_2是id恒等映射，即 $X$ 、 $Y$ 空间上平移是一致一样的，那么就叫不变性，先平移再映射和先映射再平移是一样的；注意这里的两个平移现在是数值上相等，但空间是其实分别是 $X$ 和 $Y$ 。</p>
</li>
<li><p>3.3，等变性对于函数复合可以保持，所以如果所有隐层都有等变性，那么整个网络也有。</p>
</li>
<li><p>3.3，线性层，研究基本的层，怎么学习参数的共同pattern，达到使之有等变性（群卷积）。</p>
</li>
<li><p>3.3，4式什么意思？没细看了，和之前写的3.2的式子应该是一致的</p>
</li>
<li><p>MSR方法干什么的：学习和编码等变性，这个学习就是学习隐层参数的共享pattern，编码是指在代数上找到一个灵活的representation</p>
</li>
</ul>
</li>
<li><p>讲方法了：</p>
<ul>
<li><p>4.1，FC层，权重矩阵是那样叠加的矩阵，这就是一种constraint，就是一种pattern，结果是对平移有等变性。现在希望推广到其它的变换，旋转，镜像什么的，但是咱们没有思路怎么去做，所以就用FC层模拟，设置 $y=\phi(x)=Wx, W\in \mathbb{R}^{m\times n}$ ，然后学习 $W$ 就是在学习这个pattern了。</p>
</li>
<li><p>接着阐释了这个pattern不是直接去学习 $W$ ，这样也看不出有什么pattern，我猜是参考矩阵分解的概念， $vec(W)=Uv, v\in\mathbb{R}^k, U\in\mathbb{R}^{mn\times k}$ ， $W$ 分解为伪对称矩阵 $U$ 和滤波参数向量 $v$ 的积， $U$ 的维度就是 $W$ 完全展开，再复制个 $k$ 份， $k$ 就是参量 $v$ 的维度，应该可以自行设置。所以pattern实际上就是 $U$ ， $U$ 可以限制最后 $W$ 的形式，滤波参数 $v$也要根据实际任务训练。</p>
<blockquote>
<p>For example, the sharing pattern of standard convolution guarantees that the weight matrix is constant along any diagonal, reducing the number of per-task parameters</p>
</blockquote>
</li>
<li><p>显然 $U$ 的维度会很高，怎么办？Kronecker factorization，就是参数化的方式，和之前看的文章和做的实验都是<strong>一致</strong>的想法！</p>
</li>
<li><p>4.2，那么 $U$ 具体怎么设置呢？输入的形式…举例子吧，从基本的网格空间 $X$ 上选位置，得到输入 $f$ ，即 $f:X\rightarrow R^c$ ，这个输入是向量（矩阵或者矩阵展开应该也行）。假定 $f$ 是 $s$ 维的， $\bar{f}\in\mathbb{R}^s$ ，其中 $\bar{f}_i=f(x_i)$  。引理就是保证了可以由这个 $U$ 保持群的作用，即保持某种运算的等变性；引理分两部分，一部分是给定群后，群的作用就定了，即运算定了，该运算的等变性存在 $U$ 来保持；第二部分是此 $U$ 存在时，任何 $G$ 卷积都可以由 $vec(W)$ 表示出来。反正就是能训练的。证明估计我也看不懂</p>
</li>
</ul>
</li>
<li><p>实验日常不想看，但是说实话还是得康康代码的</p>
</li>
<li><p>结论：</p>
<ul>
<li>优点：学习保持等变量性质的参数pattern—&gt;泛化，减少参数用量，适用小样本</li>
<li>从卷积的应用开始，等变性很重要—&gt;是未来发展的方向之一</li>
<li>未来自动寻找潜在的特殊结构的算子？像卷积这样的带有等变性的</li>
<li>quickly discovering symmetries的应用</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Meta Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>【NBA2K13】暑期修改历程暂结</title>
    <url>/2020/08/30/%E3%80%90NBA2K13%E3%80%91%E6%9A%91%E6%9C%9F%E4%BF%AE%E6%94%B9%E5%8E%86%E7%A8%8B%E6%9A%82%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>暑期真的花了很多时间在整理更新数据上，修改的过程真的快乐，但也不能老是沉迷其中啦😂</p>
</blockquote>
<a id="more"></a>
<h1 id="个人感想"><a href="#个人感想" class="headerlink" title="个人感想"></a>个人感想</h1><p>最近相当长的一段时间内，我很癫狂地玩起了NBA2K13（Android），并大量修改其中的球员数据。不得不说，这款老游戏深得我的喜爱，大概是单纯由于我喜欢模拟类、角色扮演类的游戏，以及这款游戏的引擎恰巧很符合我的口味吧。临近开学季，我恐怕不能再抽出这么多时间来体验<font color="#0000FF">修改</font>（其实是无root移植）的乐趣了，因此打算稍微总结这近2个月来的游戏体验。</p>
<p>总的来说，这2个月以来，基于KM2BY的2.20数据包和4.05存档，在无root条件下，我先后一共完成了独行侠、鹰、鹈鹕、太阳、掘金、篮网、76人、快船、湖人、森林狼和猛龙共11支球队绝大多数球员的更新修改，以及魔术和开拓者24球队少部分核心球员的数据更新。</p>
<p>修改完每支球队后，我都会自己开一个赛季模式动手打几场比赛（均为全明星难度），来测试修改后的可玩性（不包括模拟的真实性），然而直到现在，只有5支球队的游戏体验良好，分别是76人、独行侠、湖人、开拓者和猛龙，这其中，76人和开拓者完全是核心球员刷数据带飞；独行侠和湖人由于角色球员较强，相对来说可玩性还是稍强一些；最后单独拎出猛龙是因为目前我只上手玩了一把，这个球队防守强度太高了，导致游戏比较轻松😂，有点“兄弟篮球，人人🐂🐸”那味儿了，后续我会多测试几次来确认可玩性。</p>
<p>一般来说，修改后的阵容有这样的优缺点（对比）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>自己操作</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>核心球员在自己家比较给力，一般能稳住局势</td>
<td>对面球队有且存在不只1个核心并无限单人打爆自己…手段包括但不限于无限轻松上篮得分（或造犯规）、高频率抢断+盖帽…</td>
</tr>
<tr>
<td></td>
<td>拥有极强核心的强队游戏难度低</td>
<td>这样强队玩几局就腻了</td>
</tr>
<tr>
<td></td>
<td>有3分强球员的球队为所欲为</td>
<td>这些球员在自己手里容易n中0，在对面容易超神</td>
</tr>
<tr>
<td></td>
<td>大量角色球员会增强真实感</td>
<td>角色球员在自己家大部分都拉跨，在对面就很无敌…</td>
</tr>
<tr>
<td>模拟</td>
<td>真实性可以大大增强</td>
<td>需要手动调整球员倾向，比较麻烦</td>
</tr>
</tbody>
</table>
</div>
<p>一时间只想出来这么些体验方面的对比，就先放在这里。玩几个强队的时候都还行，只要雪球滚起来，分差拉开，后面就会轻松，但也经常会被对面打出各种小高潮以及末节究极被追分；玩弱队的时候体验极差，各种无限被抢被掏被帽被造犯规被轻松突破，自己进攻总是进攻犯规或者常常强打被按下来，或者干脆就攻不进去。玩了好几个这样的球队，甚至包括部分所谓的强队也有这样的感受，我觉得重要的原因是这些球队球员修改的进攻防守意识数值都不够，另外是普遍移动速度太慢，最后是没有稳定射手或稳定内线大闸。没有第一点，玩起来就总是核心刷数据；没有第二点，玩起来就是根本突不进去，尤其包括快攻的进攻效率低下；没有第三点，进攻端或者防守端必有一端拉跨，降低游戏体验。</p>
<h1 id="实战演示"><a href="#实战演示" class="headerlink" title="实战演示"></a>实战演示</h1><p>由于测试的次数不多且时间间隔有点长了，所以只列两个最近修改的球队，森林狼（弱队代表）和猛龙（强队代表）的比赛感受。</p>
<p>首先是森林狼，核心包括唐斯、拉塞尔、马利克比斯利和詹姆斯约翰逊，其他人要拉跨很多；主要弱点有2，一是射手太少，只有比斯利能顶一下，二是球队球员移动速度普遍较低（掘金、鹰、太阳也是这样）。自己上手比赛，和13年的球队比较，目前只赢过一把，其它全部头被🔨爆…</p>
<img src="/2020/08/30/【NBA2K13】暑期修改历程暂结/1.jpg" title="13版尼克斯VS20版森林狼——尼克斯数据">
<img src="/2020/08/30/【NBA2K13】暑期修改历程暂结/2.jpg" title="13版尼克斯VS20版森林狼——森林狼数据">
<p>只有这一把打赢了，赢的因素（对比后来输的比赛可以看看被打得多惨😭）主要在于：</p>
<ul>
<li>自己的球队因素：<ul>
<li>绝对核心唐斯在攻防两端表现稳定，拉塞尔没有受到犯规困扰，组织上没有被频繁断球</li>
<li>角色球员有所发挥，前期约翰逊强打内线稳住了比分，中后期比斯利抓住了几次快攻机会没有被帽</li>
<li>底层的替补竟然得分能超过5分了😱（气抖冷doge！我替补板凳何时在游戏里站起来！）</li>
</ul>
</li>
<li>对面球队因素：<ul>
<li>甜瓜由于在开拓者，能力值已经根据存档削弱，这次得分不算很高了</li>
<li>基德前期犯了好几次规，第一节让我滚了点雪球</li>
<li>其它球员都算是角色球员，没有那么恐怖…</li>
</ul>
</li>
</ul>
<p>输的局反正就是要么被对面n核心直接🔨爆，要么就是被无限突破+抢断+盖帽…被玩坏了…</p>
<p>———————————————- 手动分割线 ———————————————-</p>
<p>最后是刚弄的猛龙，虽然核心球员的能力值都不算顶尖，但是有不少能高于80了；角色球员的防守能力、意识都相对很不错，球队整体防守目前来看非常好；球队有一堆射手啦，虽然实战投射前期雪崩，后面才慢慢能投一些…</p>
<img src="/2020/08/30/【NBA2K13】暑期修改历程暂结/3.jpg" title="20版开拓者（只有利拉德）VS20版猛龙——前3节猛龙部分数据">
<img src="/2020/08/30/【NBA2K13】暑期修改历程暂结/4.jpg" title="20版开拓者（只有利拉德）VS20版猛龙——前3节猛龙射手数据">
<img src="/2020/08/30/【NBA2K13】暑期修改历程暂结/4.jpg" title="20版开拓者（只有利拉德）VS20版猛龙——全场统计">
<p>这场测试比赛的感受就是，哇塞，防守太舒服了！球队的篮板球实在是舒服，从半场的篮板看就…是不是猛得过头了…当然前几节射手的效率实在是…有空位竟然也投不进…可能是和身高有点关系吧，游戏里范弗里特的这三分是惊到我了…</p>
<p>哎呀呀，总之就是修改游戏的过程很快乐，但是手打的体验没有那么好😂。接下来要收手了，再着迷在这上面就没有工作学习的时间啦🤾‍♂️</p>
]]></content>
      <tags>
        <tag>NBA2K13</tag>
        <tag>DIY</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读17】ODE-Net——从离散化的ResNet到连续化ODE的模拟</title>
    <url>/2020/08/21/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB17%E3%80%91ODE-Net%E2%80%94%E2%80%94%E4%BB%8E%E7%A6%BB%E6%95%A3%E5%8C%96%E7%9A%84ResNet%E5%88%B0%E8%BF%9E%E7%BB%AD%E5%8C%96ODE%E7%9A%84%E6%A8%A1%E6%8B%9F/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>读不懂PDE-Net，所以想先操作下ODE-Net</p>
<p>结果是瞎读文章真心难读也读不明白😭😭😭</p>
</blockquote>
<a id="more"></a>
<img src="/2020/08/21/【论文阅读17】ODE-Net——从离散化的ResNet到连续化ODE的模拟/ODE-Net.png" title="ODE-Net">
<h1 id="为什么读"><a href="#为什么读" class="headerlink" title="为什么读"></a>为什么读</h1><ul>
<li>在阅读PDE-Net的时候被卡了，读得比较不明白，在搜资料的时候发现了知乎上ODE-Net的<a href="https://www.zhihu.com/question/313064079" target="_blank" rel="noopener">讨论</a>，好介绍哇，觉得现在先看ODE-Net是个不错的选择</li>
<li>搜资料的时候才发现这是NIP2018最佳论文</li>
</ul>
<p>文献链接：<a href="https://arxiv.org/abs/1806.07366" target="_blank" rel="noopener">Neural Ordinary Differential Equations</a>，原作者提交于2018/01，最后修改于2019/12，可见打磨的功夫还是下了不少的。</p>
<h1 id="文献总结"><a href="#文献总结" class="headerlink" title="文献总结"></a>文献总结</h1><p>将ODE与神经网络结合，用神经网络来模拟ODE的动态性质。</p>
<p>思想很简单，联想神经网络的隐层嵌套性，联系ResNet的残差（差分）形式并推广，就得到了利用ResNet模拟ODE的思想。细节就麻烦多了看不动看不动…</p>
<h1 id="文献内容"><a href="#文献内容" class="headerlink" title="文献内容"></a>文献内容</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>将ODE与神经网络结合，用神经网络来模拟ODE的动态性质，和总结一致。</p>
<p>另外如果能有办法用网络模拟ODE系统，那一定是应用超级友好的！</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>好像没啥背景，我理解的背景就是这个思想的来源，就够了。文章里的related work我暂时不感兴趣。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>这要从ODE与神经网络的一个共性讲起：</p>
<p>神经网络的嵌套隐层结构可以表示为$h_{t+1} = f(h_t, \theta_t)$，每一层都由前一层前向传播得到，现在用ResNet的结构则有意思起来了：$h_{t+1} = h_t + f(h_t, \theta_t)$，这种残差的形式与ODE的差分形式很“相似”：</p>
<script type="math/tex; mode=display">\Delta h(t) = h_{t+1} - h_t = f(h(t), t, \theta) \approx \dfrac{dh(t)}{dt}</script><p>这样ResNet的每一层$f$都相当于当前时刻$t$的一个差分，如果这个$t$不再离散，而是连续化，那么就可以近似为导数，残差神经网络也可以近似为ODE了。当然<strong>前提</strong>是（1）连续化可行；（2）连续化后能保持ODE的基本衍化性质；（3）不知道能不能证明其它性质诸如求解等等。</p>
<p>那么现在基本的模型其实已经构思好了，下面是实现的技术细节。要使得ResNet成为ODE的近似，首先要考虑可训练的性质，其监督信息由已知的ODEsolver求解，这些solver目前一般都是精度可控的，网络本身作为一个黑箱ODEsolver来训练；前向传播应该比较容易，关键是反向传播会比较麻烦，因为损失的回传需要对参数求导，这些势必要涉及大量参数，会引起高昂的计算代价。</p>
<p>那么该文怎么进行反向传播的呢，直接算？不是，它绕过去了。</p>
<p>先看看损失函数，直接定义为：</p>
<script type="math/tex; mode=display">L(z(t_1)) \approx L\left(z(t_0) + \int_{t_0}^{t_1} f(z(t), t, \theta)dt \right) = L(ODESolve(z(t_0), f, t_0, t_1, \theta))</script><p>其中把ODE最后的target函数记为$z(t)$，ODE初值记为$z(t_0)$，待预测的时刻$t_1$时的值为$z(t_1)$。左边相当于监督信息，用标准的值，右边两个式子都用黑箱网络来近似就可以了。</p>
<p>由于现在考虑把ResNet变成连续化的模型，因此此时不好用差分的形式，而要采用积分的形式，所以现在预测值是和时刻$t_0$和$t_1$中每个值都要有关系，这提示我们采用链式法则寻找这些值之间的关系。当然这些值太多乃至无穷，我们也只能尽可能多地进行离散近似，其实就是网络的深度（层数），有几层，就意味着我们考虑了时刻$t_0$和$t_1$之间几个中间状态。</p>
<p>这种近似考虑的方式称为<strong>adjoint sensitivity method</strong>（1962老文章🐂🍺+🐂🐸！），如下的原文图2所示，这里为了表示方便，记号与上文不同，图中的$z(t_N)$相当于上文的$z(t_1)$，意思大家一定能明白，就是多考虑上文中的中间时刻状态。</p>
<img src="/2020/08/21/【论文阅读17】ODE-Net——从离散化的ResNet到连续化ODE的模拟/ODE-Net2.png" title="Adjoint Sensitivity Method">
<p>感觉是这样推导的，我们想要的目标是损失$L$对网络参数$\theta$的导数：</p>
<script type="math/tex; mode=display">\dfrac{dL}{d\theta} = \dfrac{dL}{dz(t_i)}\dfrac{dz(t_i)}{d\theta} = \dfrac{dL}{dz(t_i)}\dfrac{df(z(t_i), t_i, \theta)}{d\theta} \triangleq a(t_i)\dfrac{df(z(t_i), t_i, \theta)}{d\theta}</script><p>其中第一个等号用链式法则和爱因斯坦求和约定表示；第二个等号是因为$z(t_i) \approx z(t_0) + \int_{t_0}^{t_i} f(z(t), t, \theta)dt$，然后直接求导就得了，其中积分从哪里开始都可以，不一定要$t_0$；第三个等号搞了新定义，把$a(t)$叫做伴随adjoint，它怎么求呢，文章给了$\dfrac{da(t)}{dt} = -a(t)^T\dfrac{\partial f(z(t), t, \theta)}{\partial z}$，但我没看懂，我猜是1962那篇文章的结论。</p>
<p>上面分析了主要的思路，应该还可以了，下面开始水😂，日常写烂尾文章/(ㄒoㄒ)/~~</p>
<p>上面的推导就是原文的(5)式，具体计算不是直接BP，因为考虑连续性，和中间状态都有关系了，中间层现在是可以很多的，BP压力很大。现在直接看完整版的算法2，直接想办法计一个积分算梯度（参考了机器之心的文章加深理解），不过很可惜我<strong>没太看明白</strong>具体计算的过程，可能是直接自动求导，只是不再一层一层计算梯度了？这样真的能减少计算量么？</p>
<p>后面的流模型和生成式模型没有看了，留着看苏剑林的博客吧，感觉和那个关系比较大。</p>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>我理解的优点：</p>
<ul>
<li>应用必定是真正有用的应用，且范围很广，涉及ODE的领域实在是太多啦</li>
<li>思路很棒！从ResNet的差分形式过渡到ODE的模拟，我很喜欢</li>
<li>文章提到的内存占用小的优点，不过有人杠这一点，我目前没看明白算法2，只好持保留态度了</li>
</ul>
<p>缺点：</p>
<ul>
<li>有人杠他们的代码和文章不一致（知乎），我先吃瓜瓜</li>
<li>算法写的有一点不明白，起码我看了几遍没明白具体是怎么算法（不考虑伪代码）</li>
<li>原作者之一自己提的缺点，见参考资料<a href="https://www.sohu.com/a/405309910_129720" target="_blank" rel="noopener">「神经常微分方程」提出者之一David Duvenaud：如何利用深度微分方程模型处理连续时间动态</a></li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>看不动，和参考资料里的是一样的啊，而且有评论说作者放出来的代码和文章不一致，那么轻松复现是不存在的了…</p>
<p>扫了一眼，文章的实验主要是模拟了一些不那么复杂，但也不是很简单的ODE系统。</p>
<h1 id="补充资料——参考链接阅读"><a href="#补充资料——参考链接阅读" class="headerlink" title="补充资料——参考链接阅读"></a>补充资料——参考链接阅读</h1><p>在略读了文章第一遍之后有些疑惑，因此搜了一些相关的网页看一看，觉得其中大部分讲的都不错，因此记录于此。</p>
<ul>
<li>看了一遍作者<a href="https://www.jiqizhixin.com/users/b0254ca0-165f-4fd3-a041-232d39a848d2" target="_blank" rel="noopener">思源</a>发布在机器之心的文章<a href="https://www.jiqizhixin.com/articles/122302" target="_blank" rel="noopener">硬核NeruIPS 2018最佳论文，一个神经了的常微分方程</a>，前面的解释比较足，有了大致的了解，还有不少细节是不太明白的</li>
<li>看了知乎上的<a href="https://www.zhihu.com/question/306937011/answer/561576636" target="_blank" rel="noopener">如何评价ODENet？</a>，反对的意见要注意，我看文章的时候觉得它说避免了存储中间计算的值，减少内存使用，而算法中反向计算的时候其实还要算一遍，这个说法很奇怪。目前我感觉确实是代码中，这部分计算没有交给网络，而是交给scipy了。</li>
<li>看了机器之心在搜狐上的文章<a href="https://www.sohu.com/a/405309910_129720" target="_blank" rel="noopener">「神经常微分方程」提出者之一David Duvenaud：如何利用深度微分方程模型处理连续时间动态</a>，了解了一些该团队最新进展，包括可逆残差网络、设计廉价的可微算子计算梯度、随机微分方程的应用；并看到了有趣的“内幕”哈哈，担心侵权问题就不放原文了，大概意思是他们这篇文章动机很现实，而且有一些数学上比较严肃的问题：</li>
<li><a href="https://www.toutiao.com/a6703302712311677452/" target="_blank" rel="noopener">一个深度学习突破的方向：神经常微分方程ODE</a>这个文章<strong>内容很多</strong>啊，有不少自己尝试的实验，只简单介绍了ODE-Net的思想，然后自己做了不少拟合的实验，最后给了自己的结论太棒了！它的结论是目前该方法只在小任务上表现良好，实际应用或者稍复杂的例子就会gg😀</li>
<li>本来想读<a href="https://blog.csdn.net/hanss2/article/details/81301343" target="_blank" rel="noopener">【随感】ODE的欧拉解法实际上就是RNN的一个特例</a>这篇文章，结果发现是转载苏剑林的博客<a href="https://kexue.fm/archives/5643" target="_blank" rel="noopener">貌离神合的RNN与ODE：花式RNN简介</a>，除了开头的时候提了一小句来源，其它是一模一样的，不过我发现这个作者其它内容写得很有趣，很有想法啊👍。<strong>这个RNN的很有意思，有空要读一读在这里做个笔记</strong>。</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt and David Duvenaud. Neural Ordinary Differential Equations[EB/OL]. <a href="https://arxiv.org/abs/1806.07366" target="_blank" rel="noopener">https://arxiv.org/abs/1806.07366</a>, 2018.</p>
<p>[2] 思源.硬核NeruIPS 2018最佳论文，一个神经了的常微分方程[EB/OL]. <a href="https://www.jiqizhixin.com/articles/122302" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/122302</a>, 2018/12/23.</p>
<p>[3] 匿名用户. 如何评价ODENet？[EB/OL]. <a href="https://www.zhihu.com/question/306937011/answer/561576636" target="_blank" rel="noopener">https://www.zhihu.com/question/306937011/answer/561576636</a>, 2018-12-30.</p>
<p>[4] 机器之心Pro. 「神经常微分方程」提出者之一David Duvenaud：如何利用深度微分方程模型处理连续时间动态[EB/OL]. <a href="https://www.sohu.com/a/405309910_129720" target="_blank" rel="noopener">https://www.sohu.com/a/405309910_129720</a>, 2020-07-02.</p>
<p>[5] AI火箭营. 一个深度学习突破的方向：神经常微分方程ODE[EB/OL]. <a href="https://www.toutiao.com/a6703302712311677452/" target="_blank" rel="noopener">https://www.toutiao.com/a6703302712311677452/</a>, 2019-06-17.</p>
<p>[6] 苏剑林. (2018, Jun 23). 《貌离神合的RNN与ODE：花式RNN简介 》[Blog post]. Retrieved from <a href="https://kexue.fm/archives/5643" target="_blank" rel="noopener">https://kexue.fm/archives/5643</a></p>
<p>最后一条按照原文提供的参考文献格式了~</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Machine Learning</tag>
        <tag>PDE</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读16】Isolation Forest——孤立的异常检测方式</title>
    <url>/2020/08/19/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB16%E3%80%91Isolation-Forest%E2%80%94%E2%80%94%E5%AD%A4%E7%AB%8B%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>这次在网上学习别人记笔记的方式，感觉很有意思，不知道我能从中学到多少。可惜markdown做富文本表格太麻烦了…但是用别的，比如word、pdf，都不方便即时打开。。。算了吧，选择了markdown还是按标题目录来整理笔记</p>
</blockquote>
<a id="more"></a>
<img src="/2020/08/19/【论文阅读16】Isolation-Forest——孤立的异常检测方式/1.png" title="文章简介">
<h1 id="为什么读此文章"><a href="#为什么读此文章" class="headerlink" title="为什么读此文章"></a>为什么读此文章</h1><p>原因有二：</p>
<ul>
<li>是我最早接触的论文之一，当年没看，现在拿来读一下，本来想略读，然后就忍不住多看了几遍…</li>
<li>周志华是作者之一，应该很顶</li>
</ul>
<p>文献链接：<a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest" target="_blank" rel="noopener">Isolation Forest</a></p>
<h1 id="文献总结"><a href="#文献总结" class="headerlink" title="文献总结"></a>文献总结</h1><p>孤立森林是一种异常检测方法，侧重以孤立的方式直接寻找异常而非比对正常样本。</p>
<p>这个孤立的思想很棒，少了很多计算的步骤，但同时有很棒的准确性，毕竟基于的是异常本身的重要性质。</p>
<h1 id="文献内容"><a href="#文献内容" class="headerlink" title="文献内容"></a>文献内容</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>给异常检测领域提供一种新方法——孤立森林。</p>
<p>该方法来源于以往没人用过的孤立的思想来做异常检测，且可以发现由此提出的孤立森林有诸多优点。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul>
<li>大方向是<strong>异常检测</strong>，看着就知道很重要</li>
<li>传统的（模型驱动的）异常检测方法把测试样本与已知正常样本对比，不符即为异常。这个想法很基础很实在√</li>
<li>上面方法的缺点有2，1是<strong>泛化不好</strong>，因为方法效果与正常样本库相关；2是<strong>计算代价大</strong>，往往只能用于低维小规模数据，因为这些方法常常用到距离等度量方式</li>
<li>在此提出并使用<strong>孤立</strong>的概念，把异常点孤立于其它正常样本。思想是利用了异常的2个性质，1是它们真的只是少数；2是特征与正常样本必是不同的。由此思想，用树来孤立异常数据，异常数据会在树根部被孤立，正常样本只会在深处被孤立。</li>
</ul>
<h2 id="方法——从孤立树到孤立森林"><a href="#方法——从孤立树到孤立森林" class="headerlink" title="方法——从孤立树到孤立森林"></a>方法——从孤立树到孤立森林</h2><ul>
<li>孤立森林是什么：<ul>
<li>背景提到了孤立树的概念，集成起来就是孤立森林。孤立森林方法只有2个变量，1是树的个数，2是sub-sampling size，一开始我猜这个size翻译为二次采样的size，做一次全分割用的样本数；后来我发现我猜错了，在文章第3章中说明了这个是可以设置多个树，相当于多个异常检测专家，分别随机取一部分数据来检测异常，这些数据的size叫sub-sampling size，至于为什么将在优缺点部分介绍。</li>
<li>异常的2个性质体现在孤立分割上就是，异常越少，所需分割次数就越少；以及异常会早早被割出来，即异常的平均路径长度average path lengths短，图1很形象。</li>
</ul>
</li>
<li>孤立树是什么：<ul>
<li>先给出基本数据假设：数据集$X=\{x_1, \cdots, x_n\}$，$n$个样本，每个样本是$d$维的，有$d$个分量。</li>
<li>首先是一棵树，然后在这里，是特定的一种树。用节点来描述，它的节点$T$只有两种，一种是外部节点，就是接下来没有分支了；第二种是内部节点，且分支有且只有两个子节点$(T_l, T_r)$，即左右子节点。</li>
<li>那么数据集X的孤立方式是，每次随机孤立一个分量，如第$q$个，孤立的分割阈值是$p$，这样把数据分为两部分。多次孤立的停止条件有：<ul>
<li>树达到临界高度</li>
<li>最后只有一个点了，即$|X|=1$</li>
<li>所有数据都只有一个值，没法孤立</li>
</ul>
</li>
</ul>
</li>
<li><p>异常的判定——异常score：</p>
<ul>
<li>有了方法的概念和方法的构成单元，下面说方法的度量，即怎么样算是异常。基本的想法是path（路径）长度：样本$x$的路径长度记为$h(x)$，在树上，$x$被孤立出来需要的分割次数。但是这个$h(x)$不好，因为这个$h(x)$算不算异常也要看整个树的深度、样本量什么的；因此需要一般性的异常score作为度量。</li>
<li>这个score来源于BST，二元搜索树，因为发现孤立树和BST有相似的性质。借BST文章的结论，其不成功搜索的路径长度$c(n)$与这里的$h(x)$类似，于是采用(1, 2)式的形式计算了此异常score $s(x, n)$。个人感觉就是$h(x)$不general，引用$c(n)$给它做了某种标准化。图2和图3解释得非常好，一目了然，分别介绍了定性的$s(x, n)$合理性和类似等高线的异常判断方式。</li>
</ul>
</li>
</ul>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><ul>
<li>孤立森林与已有方法主要区别：<ul>
<li>它可以利用sub-sampling，<strong>这个sub-sampling指什么？</strong>，是只用一部分数据就可以了，体现在孤立森林的局部：孤立树的使用上；如果数据多，往往树越深，正常样本也多，往往难以检测出异常。具体有两种情形——swamping and masking，分别指错把正常样本识别为异常，和异常点太多，比如扎堆出现，一看还以为都是正常的，其异常性被数量掩盖了。针对它们，只要取一部分数据就可以得到更好的效果，文章第3章画了两张图，非常形象地阐述了机理👍！</li>
<li>不需要计算什么距离、密度，大大降低计算复杂度</li>
<li>由集成的优点，大的森林可以处理大量数据了，包括高维情形</li>
</ul>
</li>
<li>孤立森林的优点：<ul>
<li>线性时间的复杂度，对运算内存的要求为很低的常数。1个原因是不需要计算什么距离、密度，<strong>另一个原因</strong>是按照方法中孤立的方式，即使树是完全树，最多（每次只孤立1个点）的内部节点有$n-1$个。最多的全部节点有$n-1+n=2n-1$个（自己画个图就知道了），所以对变量的内存要求是有界的，与样本量呈线性关系。</li>
<li>首次把孤立应用于异常检测，似乎用了二次采样，其它已有方法不行</li>
<li>它的2个变量都比较小，就可以表现优异</li>
</ul>
</li>
</ul>
<h2 id="实验——如何训练孤立森林"><a href="#实验——如何训练孤立森林" class="headerlink" title="实验——如何训练孤立森林"></a>实验——如何训练孤立森林</h2><p>和一般的机器学习方法类似，先训练出一个孤立森林模型，再拿测试数据测试。</p>
<ul>
<li><p>训练总体的算法是孤立树的集成——孤立森林，为算法1，递归式地分割数据集</p>
<ul>
<li>算法1中包括算法2，单颗孤立树的训练，这些都已经说过了。</li>
<li>算法1、2中有3个参数要提前设置：子采样数、树的最大深度和树的棵树，这些都根据实验经验确定，子采样数默认$\psi=256$，深度默认$l=ceiling(\log_2 \psi)$，棵树默认$t=100$。</li>
</ul>
</li>
<li><p>最后测试的时候计算的score要一直到被孤立出去，且使用递归的方式一点点计算。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Machine Learning</tag>
        <tag>Abnormality Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读15】Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation</title>
    <url>/2020/07/31/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB15%E3%80%91Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>论文阅读</p>
</blockquote>
<a id="more"></a>
<h2 id="文章简介"><a href="#文章简介" class="headerlink" title="文章简介"></a>文章简介</h2><p>这篇文章[3]是在ECCV2020的日程[1, 2]中发现的，拿来一读。</p>
<p>原作者的部分讲解视频在B站BAAI发布的<a href="https://www.bilibili.com/video/BV1wZ4y1M7qw?p=4" target="_blank" rel="noopener">ECCV 2020中国预会议回放丨计算机视觉、目标检测、图像分类、特征提取…55场前沿学术报告集萃</a>P4的181:40-192:00。</p>
<p>从标题上看是提出了一个对抗网络的变种，<font color="#0000FF">双重对抗网络DANet</font>，针对真实场景，<strong>同时建模（生成）噪声以及去噪</strong>。</p>
<img src="/2020/07/31/【论文阅读15】Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation/1.png" title="文章简介">
<h2 id="主要内容——DANet"><a href="#主要内容——DANet" class="headerlink" title="主要内容——DANet"></a>主要内容——DANet</h2><p>ps:</p>
<blockquote>
<p>主要内容直接参考B站视频或者原文即可。</p>
<p>个人不太想看相关工作的部分，只看了噪声生成的介绍，模拟噪声分布的目的之一是解决数据对的成本问题。其中传统方法GAN的难度在于如何找到好的噪声生成器。</p>
</blockquote>
<h3 id="模型提出"><a href="#模型提出" class="headerlink" title="模型提出"></a>模型提出</h3><p>本文的主要思想与之前的<a href="[https://maxliu245.github.io/2020/06/27/10-VDN-Variational-Denoising-Network-%E9%98%85%E8%AF%BB/#more](https://maxliu245.github.io/2020/06/27/10-VDN-Variational-Denoising-Network-阅读/#more">VDN</a>)[4]如出一辙，感觉是顺着VDN在贝叶斯框架下同时进行图像噪声估计和图像降噪的思路继续做的。主要内容是提出了双重对抗网络<strong>DANet</strong>，下面具体介绍一下。</p>
<p>相比VDN直接贝叶斯框架完成两种噪声相关任务，这次是间接地、从另一个角度地对干净图$x$和带噪图$y$的联合分布$p(x,y)$进行估计，并分两种分解形式。因此从两个方面分别建模，从$y$开始建模，生成$x$的过程就是训练了降噪器$R$，反之就是训练了噪声生成器$G$；两种方式逼近联合分布后通过判别器$D$做监督。这种方式称为标题的“双重对抗”，$R$和$G$相互影响，共同逼近联合分布。</p>
<p>$x$，$y$联合分布的两种分解很自然，从联合分布定义即可，实际意义参考视频中的图即可理解：</p>
<img src="/2020/07/31/【论文阅读15】Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation/2.png" title="联合分布$p(x,y)$的两种分解意义">
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>接下来确定对抗网络的结构：</p>
<img src="/2020/07/31/【论文阅读15】Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation/3.png" title="网络结构">
<p>其中降噪器denoiser记为$R$，噪声生成器generator记为$G$，都用UNet，并加了残差策略；最后判别器discriminator记为$D$，采用5个卷积层和一个FC。</p>
<h3 id="部分训练细节"><a href="#部分训练细节" class="headerlink" title="部分训练细节"></a>部分训练细节</h3><p>训练的目标函数是Triple-GAN中提出的损失，加上两个正则项（对应于两种分解）。前者是干净图的$L_1$损失，可以优化$R$；后者由于噪声随机性，采用高斯滤波后的某种$L_1$，优化$G$，具体形式参考原文(6)式。</p>
<p>网络训练的过程参考原文算法1即可，其训练的思路在于向后传播分两步，交替优化$R$和$G$。</p>
<h3 id="部分亮点"><a href="#部分亮点" class="headerlink" title="部分亮点"></a>部分亮点</h3><p>首先是继承VDN的思路，同时完成两个噪声相关任务。</p>
<p>其次是在概率框架下从两种联合分布分解形式的角度设计了DANet。</p>
<p>再者是设计了两种噪声生成质量的度量，比较了真实带噪图和生成带噪图的相似性：</p>
<ul>
<li><p><strong>PGap </strong>(PSNR Gap)：借用降噪指标PSNR设计。已知训练集$\mathcal{D}$和测试集$\mathcal{T}$，分别是一堆成对干净图和带噪图，用准备好的噪声生成器$G$，对训练集进行加噪，得到生成的新训练集$\mathcal{D}_G$，准备原始训练数据上训练得到的降噪器$R_1$和在新训练集$\mathcal{D}_G$上得到的降噪器$R_2$，分别计算测试集上的PSNR，其差值小，代表噪声生成得越好，越接近真是噪声分布。</p>
<script type="math/tex; mode=display">PGap = PSNR(R_1(\mathcal{T})) - PSNR(R_2(\mathcal{T}))</script></li>
<li><p><strong>AKLD</strong> (Average KL Divergence)：这个比较直接，希望生成噪声的分布与其真实分布接近。真实的分布用了VDN的结论，然后采样；这里的分布是由生成器$G$对应的新分布，都通过采样近似。</p>
</li>
</ul>
<h2 id="DANet-V-S-VDN"><a href="#DANet-V-S-VDN" class="headerlink" title="DANet V.S. VDN"></a>DANet V.S. VDN</h2><p>与VDN的区别？难道是VDN传统贝叶斯框架条件后验，这个直接联合分布？主要区别好像是的耶~</p>
<blockquote>
<p> 参考摘要</p>
<p>Instead of only inferring the posteriori distribution of the latent clean image conditioned on the observed noisy image in traditional MAP framework, our proposed method learns the joint distribution of the clean-noisy image pairs.</p>
</blockquote>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>[1] BAAIBeijing. 计算机视觉顶会ECCV 2020中国预会议：日程公开，注册有奖[EB/OL]. <a href="https://blog.csdn.net/BAAIBeijing/article/details/107588080" target="_blank" rel="noopener">https://blog.csdn.net/BAAIBeijing/article/details/107588080</a>, 2020-07-25.</p>
<p>[2] 智源社区. 智源社区活动[EB/OL]. <a href="https://event.baai.ac.cn/event/53#section-four" target="_blank" rel="noopener">https://event.baai.ac.cn/event/53#section-four</a>, 2020.</p>
<p>[3] Zongsheng Yue, Qian Zhao, Lei Zhang, and Deyu Meng. Dual adversarial network: Toward real-world noise removal and noise generation, 2020.</p>
<p>[4] Yue, Zongsheng, et al. Variational denoising network: Toward blind noise modeling and removal, <em>Advances in neural information processing systems</em>, 2019.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Denoising</tag>
        <tag>Machine Learning</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title>【Windows技巧2】用画图编辑图片并保存时出现【发生了共享冲突】</title>
    <url>/2020/07/30/%E3%80%90Windows%E6%8A%80%E5%B7%A72%E3%80%91%E7%94%A8%E7%94%BB%E5%9B%BE%E7%BC%96%E8%BE%91%E5%9B%BE%E7%89%87%E5%B9%B6%E4%BF%9D%E5%AD%98%E6%97%B6%E5%87%BA%E7%8E%B0%E3%80%90%E5%8F%91%E7%94%9F%E4%BA%86%E5%85%B1%E4%BA%AB%E5%86%B2%E7%AA%81%E3%80%91/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>用画图软件编辑图片并保存时出现bug（背景是正在用<code>python</code>导入图片）</p>
<p><code>发生了共享冲突</code></p>
</blockquote>
<a id="more"></a>
<h2 id="bug简介"><a href="#bug简介" class="headerlink" title="bug简介"></a>bug简介</h2><p>不多说，标题已经很明确了，直接上图：</p>
<img src="/2020/07/30/【Windows技巧2】用画图编辑图片并保存时出现【发生了共享冲突】/1.png" title="发生了共享冲突（请忽略卑微马赛克🐕）">
<h2 id="经典解法"><a href="#经典解法" class="headerlink" title="经典解法"></a>经典解法</h2><p>网上有诸多解决方法，其原理都是认为原始图片和自己编辑过的图片，后者并没有在内存中覆盖前者，二者在存储的时候会冲突。</p>
<p>本文<strong>唯一引用文献</strong>，这篇文章讲得就还好<a href="https://www.beihaiting.com/a/YNJD/XTGZ/2016/0620/8922.html" target="_blank" rel="noopener">Win10画图保存编辑的图片时提示发生了共享冲突怎么办?</a></p>
<h2 id="我的问题与解法"><a href="#我的问题与解法" class="headerlink" title="我的问题与解法"></a>我的问题与解法</h2><p>但是！这个方法对我不管用，因为我删除原始图片时会显示<strong>画图软件占用</strong>：</p>
<img src="/2020/07/30/【Windows技巧2】用画图编辑图片并保存时出现【发生了共享冲突】/2.png" title="画图软件占用">
<p>这时哥发现了端倪，平时编辑图片也没这问题啊？问题指向了<code>jupyter notebook</code>界面的<code>python</code>代码，它导入图片失败，我正调试bug……</p>
<p>于是我直接关闭编辑中的画图软件，直接删除图片，果然，<strong>python软件也在占用</strong>：</p>
<img src="/2020/07/30/【Windows技巧2】用画图编辑图片并保存时出现【发生了共享冲突】/3.png" title="`python`软件占用">
<p>所以！我的解决方法是关闭当前<code>jupyter notebook</code>的<code>kernel</code>，🆗了</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>咸鱼生活需要bug滋润……</p>
]]></content>
      <tags>
        <tag>Windows</tag>
        <tag>Bug</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读14】MFM——针对重尾分布的元特征调节器</title>
    <url>/2020/07/23/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB14%E3%80%91MFM%E2%80%94%E2%80%94%E9%92%88%E5%AF%B9%E9%87%8D%E5%B0%BE%E5%88%86%E5%B8%83%E7%9A%84%E5%85%83%E7%89%B9%E5%BE%81%E8%B0%83%E8%8A%82%E5%99%A8/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Please input password to continue...</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="2d7b5b9cef4a173e9dc29c2fda836be7d45b8bd304f6cfd43c1d5b58bebd2d0c">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Meta Learner</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读13】PDE-Net: Learning PDEs from Data</title>
    <url>/2020/07/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB13%E3%80%91PDE-Net-Learning-PDEs-from-Data/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>早就想了解PDE相关的方法啦，来凎！</p>
</blockquote>
<a id="more"></a>
<h2 id="我的早年想法"><a href="#我的早年想法" class="headerlink" title="我的早年想法"></a>我的早年想法</h2><p>其实很早我就有过这种想法，从一堆奇怪的数字序列中自动得到背后的数学规律，这个规律就是PDE啦。</p>
<blockquote>
<p>ps：记得当时是看蚂蚁活动突然有这样的想法？？？</p>
</blockquote>
<p>但是当时对神经网络了解不深，不知道怎么去模拟PDE中的算子。n年后开始磕盐才发现已经有不少这样的研究了，今天就来正式看一看！</p>
<h2 id="PDE-Net"><a href="#PDE-Net" class="headerlink" title="PDE-Net"></a>PDE-Net</h2><p>元学习器，即<code>meta learner</code>，在我理解是机器学习从传统的<code>训练-验证-测试</code>向元学习<code>训练-元数据-泛化</code>转变的一种产物。</p>
<p>以MAML为例，它的核心在于能够找到一个通用的初始参数，能用于一系列存在联系的任务上，使得相关联的新任务使用该初始参数后就能快速收敛，达到较优的性能。但是我对此有</p>
<h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>大背景是PDE应用广泛$\checkmark$，但是现代应用场景下的复杂系统遵循什么样的PDE规律我们并不清楚。我们清楚的是可以收集到很多场景下的数据，因此以数据驱动方式，设计前馈神经网络PDE-Net，能同时预测复杂系统行为，并揭示潜在的PDE模型。</p>
<p>其设计方式是用卷积核拟合微分算子，其实就是离散地逼近非线性可微算子。逼近的时候，会通过微分算子的阶和卷积核加和规则的阶去合理地限制卷积核。</p>
<p>PDE-Net的优点：</p>
<p>预测系统行为的时间长，即便数据有noise</p>
<p>对卷积核的限制合理，既保持逼近的微分算子性质又保持相当的向后预测能力；很灵活，即又逼近潜在PDE，又能预测。</p>
<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>复杂系统的PDE不知道啊，只能通过数据瞎jb学了。</p>
<p>列举的前人工作最早09年，多集中在16，17年。</p>
<p>启发之一，ResNet与PDE有关。</p>
<p>The main objective那一段的思想很棒！</p>
<p>我tm看不懂了</p>
<ul>
<li><a href="https://www.zhihu.com/pin/974609149418479616" target="_blank" rel="noopener">https://www.zhihu.com/pin/974609149418479616</a></li>
<li><a href="https://blog.csdn.net/qq_30883339/article/details/86022111" target="_blank" rel="noopener">https://blog.csdn.net/qq_30883339/article/details/86022111</a></li>
<li><a href="https://www.zhihu.com/question/266075899/answer/363848527" target="_blank" rel="noopener">https://www.zhihu.com/question/266075899/answer/363848527</a></li>
<li><a href="https://www.bilibili.com/video/av65293637/" target="_blank" rel="noopener">https://www.bilibili.com/video/av65293637/</a> 40min</li>
</ul>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>PDE</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读12】Meta Label Corrector——设计元学习器完成错误标签修改</title>
    <url>/2020/07/21/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB12%E3%80%91Meta-Label-Corrector%E2%80%94%E2%80%94%E8%AE%BE%E8%AE%A1%E5%85%83%E5%AD%A6%E4%B9%A0%E5%99%A8%E5%AE%8C%E6%88%90%E9%94%99%E8%AF%AF%E6%A0%87%E7%AD%BE%E4%BF%AE%E6%94%B9/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Please input password to continue...</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="3594e7eaa513efc4c7dc9235bd2667257d90bda111adaf0df9729ac6cac61fd5">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <tags>
        <tag>Paper Reading</tag>
      </tags>
  </entry>
  <entry>
    <title>11 Automated Identification of Chromosome Segments 利用染色体核型图像和条带信息分析易位</title>
    <url>/2020/07/05/11-Automated-Identification-of-Chromosome-Segments-%E5%88%A9%E7%94%A8%E6%9F%93%E8%89%B2%E4%BD%93%E6%A0%B8%E5%9E%8B%E5%9B%BE%E5%83%8F%E5%92%8C%E6%9D%A1%E5%B8%A6%E4%BF%A1%E6%81%AF%E5%88%86%E6%9E%90%E6%98%93%E4%BD%8D/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>该文利用DTW算法以及染色体密度谱曲线判断易位信息</p>
</blockquote>
<a id="more"></a>
<img src="/2020/07/05/11-Automated-Identification-of-Chromosome-Segments-利用染色体核型图像和条带信息分析易位/11.png" title="DTW and Chromosome density profile">
<p>这次读论文尝试了新的记笔记方式，然而现实是残酷的。$\LaTeX{}$写着是真心慢呜呜呜…</p>
<p>附上笔记么么哒~，参考文献和链接一并记录在pdf文件中。此外，笔记仅供参考，<strong>请勿修改此pdf</strong>。</p>


	<div class="row">
    <embed src="20200707-DTW-blog-pdf.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>DTW</tag>
      </tags>
  </entry>
  <entry>
    <title>10 VDN-Variational Denoising Network 阅读</title>
    <url>/2020/06/27/10-VDN-Variational-Denoising-Network-%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>VDN阅读笔记</p>
</blockquote>
<a id="more"></a>
<h2 id="文献简介"><a href="#文献简介" class="headerlink" title="文献简介"></a>文献简介</h2><img src="/2020/06/27/10-VDN-Variational-Denoising-Network-阅读/10.png" title="VDN">
<ul>
<li>这是一篇2019年<a href="http://papers.nips.cc/paper/8446-variational-denoising-network-toward-blind-noise-modeling-and-removal" target="_blank" rel="noopener">NIPS</a>的文章</li>
<li>截至目前，2020/06/29谷歌上的引用量11</li>
</ul>
<h2 id="概括VDN"><a href="#概括VDN" class="headerlink" title="概括VDN"></a>概括VDN</h2><p>VDN方法概括：是VI的方法，在贝叶斯框架下同时进行图像噪声估计和图像降噪。同时基于数据驱动，使用DNN近似图像后验，隐变量为假设的真实干净图像与噪声，进而生成带噪图像。这个图像后验是由DNN输出的参数显示表达的。</p>
<blockquote>
<p>当然你要问，那一般有了大量有监督的带噪图和干净图对，我们可以直接用DNN，或者GAN暴力做一个降噪器，VDN相比于此有何区别呢？除了贝叶斯框架外，最主要的是噪声假设更加general，其先验假设不再是简化的iid噪声，而是与图像上像素位置相关的非iid噪声，是spatial的。</p>
</blockquote>
<p>VDN的优点：</p>
<ul>
<li>使用数据驱动的DNN，其生成模式带来泛化性</li>
<li>是传统DNN方法的general版本，可退化回传统DNN方法（揭示了其机理与缺点）</li>
<li>两种隐变量，干净图和噪声的建模很general，可解释性好</li>
<li>spatial的非iid噪声先验假设</li>
<li>降噪和噪声估计同时进行，且均可显示表达出来</li>
</ul>
<h2 id="文献内容阅读（部分翻译）"><a href="#文献内容阅读（部分翻译）" class="headerlink" title="文献内容阅读（部分翻译）"></a>文献内容阅读（部分翻译）</h2><ul>
<li><p>摘要</p>
<p>介绍个背景<code>blind image denoising</code>，基本概念是收集图像的时候噪声很复杂以至于我们其实真的不清楚噪声具体结构，所以这种降噪称为blind</p>
</li>
<li><p>引言</p>
<ul>
<li>两种传统图像降噪方法：<ul>
<li>贝叶斯框架下的MAP方法，通过稀疏、低秩等先验，得到合理的后验，引出保真的损失和正则。但是缺点是噪声的先验往往与实际复杂且非iid的空间变化的噪声不符；且传统的MAP似乎只能作用于一张图像，对新图像要重新计算。</li>
<li>方法二是DNN端到端模型，利用大量收集的干净带噪图像对训练降噪器。优点是能利用大量图像信息，测试速度快；缺点是易对特定类型的噪声过拟合，且实际噪声复杂导致难以泛化√</li>
</ul>
</li>
<li>本文基于blind image denoising提出了新的VDN<ul>
<li>VDN概念：仍然在贝叶斯框架下，一般性的blind降噪方法。目的是同时估计噪声并降噪，把真实干净图像和噪声估计视为隐变量，从脏图生成干净的后验图像。使用DNN，后验成为参数的显式表示。</li>
<li>对一般方法的改进：<ul>
<li>噪声的估计是非iid的，更真实，与图像的空间位置相关；</li>
<li>生成式模型泛化更好</li>
<li>噪声的估计是生成式的，解释性</li>
<li>DNN框架，<strong>可以退化</strong>为一般的DNN降噪方法</li>
<li>一般方法过拟合的解释：本质上过分强调了拟合干净图的先验，忽略了噪声的空间非iid性，导致测试的时候易被不同类型的噪声影响</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>相关工作</p>
<p>图像降噪的两种方法Model-driven MAP based Methods和Data-driven Deep Learning based Methods。</p>
</li>
<li><p>VDN理论推导过程</p>
<ul>
<li><p>贝叶斯模型建立</p>
<p>数据对集合$D=\{\mathbf{y}_j, \mathbf{x}_j\}_{j=1}^n$，前者是带噪图，后者是对应的真实干净图，下标$j$是样本的index。贝叶斯框架下，隐变量是带噪图条件下的干净图和噪声。假定图像的维度为$d = weight \times height$，单个样本对$(\mathbf{y}, \mathbf{x})=\left(\left[y_1, \cdots, y_d\right]^T, \left[x_1, \cdots, x_d\right]^T\right)$，维度的index则统一用下标$i$表示。那么一张带噪图$\mathbf{y}$的一个分量</p>
<script type="math/tex; mode=display">y_i \sim \mathcal{N}(y_i | z_i, \sigma_i^2), i = 1, 2, \cdots, d \tag{1}</script><p>其中$z_i\in \mathbb{R}^d$，是隐变量，潜在的干净图，以数据对中的干净图为先验，</p>
<script type="math/tex; mode=display">z_i \sim \mathcal{N}(z_i | x_i, \epsilon_0^2), i = 1, 2, \cdots, d \tag{2}</script><p>为共轭的高斯先验。后面的方差项就是指图像上每个像素处的噪声是非iid的，不知道为什么取为对什么共轭的逆高斯分布IG，即</p>
<script type="math/tex; mode=display">\sigma_i^2 \sim IG\left(\sigma_i^2 | \dfrac{p^2}{2} - 1, \dfrac{p^2\xi_i}{2}\right), i = 1, 2, \cdots, d \tag{3}</script><p>其中$\mathbf{\xi} =\mathcal{G}\left(\left(\hat{\mathbf{y}} - \hat{\mathbf{x}} \right)^2; p\right)$，其中$\left(\hat{\mathbf{y}} - \hat{\mathbf{x}} \right)^2$称为variance map，中文可能叫图像的方差映射；$\mathcal{G}(\cdot, p)$指用$p\times p$的卷积核进行高斯滤波。具体有啥用我没看懂鸭…</p>
</li>
<li><p>后验的变分表示</p>
<p>首先明确后验分布是对于隐变量来说的，即$p(z, \sigma^2 | \mathbf{y})$。</p>
<p>那么有自然的后验近似分布$q(z, \sigma^2 | \mathbf{y})$</p>
<p>由平均场假设，近似分布可以分解</p>
<script type="math/tex; mode=display">q(z, \sigma^2 | \mathbf{y}) = q(z | \mathbf{y})q(\sigma^2 | \mathbf{y}) \tag{4}</script><p>由于前面提到“$z$服从共轭的高斯先验”和“方差项为共轭的逆高斯分布IG”，我们用下面两个式子表示$(3)$式中RHS的两项：</p>
<script type="math/tex; mode=display">\displaystyle q(z|\mathbf{y}) = \prod_{i}^{d} \mathcal{N}(z_i | \mu_i (\mathbf{y};W_D), m_i^2 (\mathbf{y}; W_D)) \tag{5}</script><script type="math/tex; mode=display">\displaystyle q(\sigma^2|\mathbf{y}) = \prod_{i}^{d} IG(\sigma_i^2 | \alpha_i (\mathbf{y};W_S), \beta_i^2 (\mathbf{y}; W_S)) \tag{6}</script><p>其中$\mu_i, m_i, \alpha_i, \beta_i$都是预测表示从脏图$\mathbf{y}$到对应参数的映射函数，用NN就可以做到。式$(4)$和式$(5)$分别对应降噪网络D-Net和参数网络Sigma-Net。</p>
</li>
<li><p>变分下界推导：</p>
<p>详细给了后验的分解形式后，来推下变分下界，下面是数据边际似然的对数：</p>
<script type="math/tex; mode=display">\log p(\mathbf{y}; z, \sigma^2) = \mathcal{L}(z,\sigma^2; \mathbf{y}) + D_{KL}(q(z, \sigma^2 | \mathbf{y}) || p(z, \sigma^2 | \mathbf{y})) \tag{7}</script><p>那么其中的$\mathcal{L}$就是变分下界：</p>
<script type="math/tex; mode=display">\mathcal{L}(z,\sigma^2; \mathbf{y}) = \mathbb{E}_{q(z, \sigma^2|y)} [\log p(y | z, \sigma^2)] - D_{KL}(q(z|y)||p(z)) - D_{KL}(q(\sigma^2|y)||p(\sigma^2)) \tag{8}</script><p>至于$(7)(8)$式怎么推导的，那就是一般变分下界的推导方式，对隐变量积分、似然项分解、差分出KL散度。式$(7)$中的每一项<strong>都有解析表示</strong>！下面是第一项</p>
<script type="math/tex; mode=display">\begin{align}\displaystyle \mathbb{E}_{q(z, \sigma^2|y)} [\log p(y | z, \sigma^2)] &\triangleq \int q(z, \sigma^2|y) \log p(y | z, \sigma^2) dzd\sigma^2 \tag{9}\\ &= \sum_{i}^n \int q(z_i, \sigma_i^2|y) \log p(y_i | z_i, \sigma_i^2) dz_id\sigma_i^2 \tag{10}\\ &= \sum_{i}^n \int q(z_i|y) q(\sigma_i^2|y) \left\{-\dfrac{1}{2}\log (2\pi) - \dfrac{1}{2}\log (\sigma_i^2) - \dfrac{(y_i - z_i)^2}{2\sigma_i^2}\right\} dz_id\sigma_i^2 \tag{11}\\ &= \sum_{i}^n \left\{ -\dfrac{1}{2}\log (2\pi) - \dfrac{1}{2} \int q(\sigma_i^2|y) \log (\sigma_i^2)d\sigma_i^2 \int q(z_i^2|y)dz_i - \dfrac{1}{2}\int q(z_i^2|y)(y_i - z_i)^2 dz_i \int q(\sigma_i^2|y)\dfrac{1}{\sigma_i^2}d\sigma_i^2\right\}  \tag{12}\\ &= \sum_{i}^n \left\{ -\dfrac{1}{2}\log (2\pi) - \dfrac{1}{2} \mathbb{E} \left[\log (\sigma_i^2)\right] - \dfrac{1}{2}\mathbb{E}\left[(y_i - z_i)^2\right]\mathbb{E}\left[\dfrac{1}{\sigma_i^2}\right]\right\}  \tag{13}\\ &= \sum_{i}^n \left\{ -\dfrac{1}{2}\log (2\pi) - \dfrac{1}{2} \left(\log \beta_i - \psi (\alpha_i)\right) - \dfrac{\alpha_i}{2\beta_i} \left[(y_i - \mu_i)^2 + m_i^2\right]\right\} \tag{14}\end{align}</script><p>后面两项我着实捉虾了，以及从$(13)$到$(14)$还是令我费解的。补充材料里都推导了，但可惜我这各种分布不太熟悉，这些期望具体怎么算的我还要多学…✊</p>
<p>然后就可以把变分下界当成目标函数，或者说loss来训练了。</p>
</li>
<li><p>网络，D-Net和Sigma-Net的训练似乎比较轻松，不需要重参数化什么的，直接BP开干就完事了。注意这里网络的结构怎么设计先不管了，有需要再进行阅读</p>
<blockquote>
<p>留一段原文结构机理在这里</p>
<p>During the BP training process, the gradient information from the likelihood term of<br>Eq. (10) is used for updating both the parameters of D-Net and S-Net simultaneously, implying that<br>the inference for the latent clean image z and sigma^2 is guided to be learned from each other.</p>
</blockquote>
<p>训练好两个网络之后，把测试脏图输入降噪的D-Net，得到的均值$\mu$就是降噪图；输入Sigma-Net，就得到该图噪声的分布参数。</p>
</li>
</ul>
</li>
<li><p>最后给出一个VDN退化到传统DNN的理解：</p>
<p>$(2)$式中如果方差项趋于0，则上面变分下界中的前面似然项期望退化为MSELoss，即传统的方法。传统的方法容易对特定的噪声过拟合就是因为只有这样的MSELoss项控制，缺少噪声随图像上位置变化的描述（往往非iid）</p>
</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] Yue, Zongsheng, et al. “Variational denoising network: Toward blind noise modeling and removal.” <em>Advances in neural information processing systems</em>. 2019.</p>
<p>[2] NIPS Proceedings 2019. Variational Denoising Network: Toward Blind Noise Modeling and Removal[EB/OL]. <a href="http://papers.nips.cc/paper/8446-variational-denoising-network-toward-blind-noise-modeling-and-removal" target="_blank" rel="noopener">http://papers.nips.cc/paper/8446-variational-denoising-network-toward-blind-noise-modeling-and-removal</a>, 2019.</p>
<p>详细推导细节参考[2]提供的补充材料。</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Denoising</tag>
      </tags>
  </entry>
  <entry>
    <title>[Hexo] 2 加密博客尝试</title>
    <url>/2020/06/27/Hexo-2-%E5%8A%A0%E5%AF%86%E5%8D%9A%E5%AE%A2%E5%B0%9D%E8%AF%95/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Please input password to continue...</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="edb5e20c23b715968b8477bf0a982bcb9cc3393bedd954396a4ed8bfe91a9d49">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>多元高斯分布之间的KL散度</title>
    <url>/2020/06/14/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E4%B9%8B%E9%97%B4%E7%9A%84KL%E6%95%A3%E5%BA%A6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>推导一下两个多元高斯分布之间的KL散度公式</p>
</blockquote>
<a id="more"></a>
<h3 id="完整推导过程"><a href="#完整推导过程" class="headerlink" title="完整推导过程"></a>完整推导过程</h3><p>哎，其实我是傻乎乎地想推导两个多元高斯分布之间的$\beta$和$\gamma$散度，后来发现不可行。在这个过程中顺手推导了一般的KL散度，谨记过程如下。</p>
<p>假定两个多元高斯分布分别为$\mathcal{N}(\mu_1, \Sigma_1)$和$\mathcal{N}(\mu_2, \Sigma_2)$，其概率分布分别表示为$p(x)$和$q(x)$。其中$\mu_1$和$\mu_2$为长度$n$的向量，$\Sigma_1$和$\Sigma_2$为维度$n\times n$的协方差阵。那么$p(x)$和$q(x)$之间的差异可以用KL散度表示，定义为：</p>
<script type="math/tex; mode=display">\displaystyle D_{KL}(p(x)||q(x)) = \int p(x)\log \dfrac{p(x)}{q(x)}dx.\tag{1}</script><p>$(1)$式的完整计算过程如下：</p>
<script type="math/tex; mode=display">\begin{align} \displaystyle D_{KL}(p(x)||q(x)) &= \int p(x)\log \dfrac{p(x)}{q(x)}dx\\ &= \int p(x)\log \dfrac{\dfrac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_1|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1)}}{\dfrac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_2|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2)}}dx \tag{2}\\ &= \int p(x)\log \dfrac{|\Sigma_2|^{\frac{1}{2}}}{|\Sigma_1|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) + \frac{1}{2}(x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2)}dx \tag{3} \\&= \dfrac{1}{2}\int p(x)\log \dfrac{|\Sigma_2|}{|\Sigma_1|}dx + \dfrac{1}{2}\int p(x)\left[ -(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) + (x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2)\right]dx \tag{4} \\ &= \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} + \dfrac{1}{2}\int p(x)\left[ -tr\left(\Sigma_1^{-1}(x-\mu_1)(x-\mu_1)^T\right) + tr\left(\Sigma_2^{-1}(x-\mu_2)(x-\mu_2)^T\right)\right]dx \tag{5} \\ &= \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} - \dfrac{1}{2}tr\left[\int p(x)\left[\Sigma_1^{-1}(x-\mu_1)(x-\mu_1)^T\right]dx\right] + \dfrac{1}{2}tr\left[\int p(x)\left[\Sigma_2^{-1}(x-\mu_2)(x-\mu_2)^T\right]dx\right] \tag{6}\\ &= \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} - \dfrac{1}{2}tr\left(\Sigma_1^{-1}\int p(x)(x-\mu_1)(x-\mu_1)^T dx\right) + \dfrac{1}{2}tr\left(\Sigma_2^{-1}\int p(x)(x-\mu_2)(x-\mu_2)^T dx\right) \tag{7} \\ &= \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} - \dfrac{1}{2}tr\left(\Sigma_1^{-1}\mathbb{E}_p\left(xx^T -x\mu_1^T - \mu_1 x^T + \mu_1\mu_1^T\right)\right) + \dfrac{1}{2}tr\left(\Sigma_2^{-1}\mathbb{E}_p\left(xx^T -x\mu_2^T - \mu_2 x^T + \mu_2\mu_2^T\right)\right) \tag{8} \\ &=  \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} - \dfrac{1}{2}tr\left(\Sigma_1^{-1}\Sigma_1\right) + \dfrac{1}{2}tr\left(\Sigma_2^{-1}\left(\Sigma_1 - \mu_1\mu_2^T - \mu_2 \mu_1^T + \mu_2\mu_2^T\right)\right) \tag{9} \end{align}</script><p>Emmm，式子太长，编辑器渲染都卡爆了，所以接着上面再推导：</p>
<script type="math/tex; mode=display">\begin{align} \displaystyle D_{KL}(p(x)||q(x)) &= \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} - \dfrac{1}{2}tr\left(I\right) + \dfrac{1}{2}tr\left(\Sigma_2^{-1}\Sigma_1\right) + \dfrac{1}{2}tr\left[\Sigma_2^{-1}\left(\mu_1\mu_1^T -\mu_1\mu_2^T - \mu_2 \mu_1^T + \mu_2\mu_2^T\right)\right] \tag{10} \\ &= \dfrac{1}{2}\left[ \log \dfrac{|\Sigma_2|}{|\Sigma_1|} - n + tr\left(\Sigma_2^{-1}\Sigma_1\right) + tr\left[\Sigma_2^{-1}\left(\mu_1\mu_1^T -\mu_1\mu_2^T - \mu_2 \mu_1^T + \mu_2\mu_2^T\right)\right]\right] \tag{11}\\ &= \dfrac{1}{2}\left[ \log \dfrac{|\Sigma_2|}{|\Sigma_1|} - n + tr\left(\Sigma_2^{-1}\Sigma_1\right) + tr\left(\mu_1^T\Sigma_2^{-1}\mu_1^T\right) - tr\left(\mu_1^T\Sigma_2^{-1}\mu_2^T\right) - tr\left(\mu_2^T\Sigma_2^{-1}\mu_1^T\right)+ tr\left(\mu_2^T\Sigma_2^{-1}\mu_2^T\right)\right] \tag{12} \\ &= \dfrac{1}{2}\left[ \log \dfrac{|\Sigma_2|}{|\Sigma_1|} - n + tr\left(\Sigma_2^{-1}\Sigma_1\right) + tr\left(\mu_1^T\Sigma_2^{-1}\mu_1^T - 2\mu_1^T\Sigma_2^{-1}\mu_2^T + \mu_2^T\Sigma_2^{-1}\mu_2^T\right)\right] \tag{13} \\ &= \dfrac{1}{2}\left[ \log \dfrac{|\Sigma_2|}{|\Sigma_1|} - n + tr\left(\Sigma_2^{-1}\Sigma_1\right) + tr\left[\left(\mu_2-\mu_1\right)^T\Sigma_2^{-1}\left(\mu_2-\mu_1\right)\right]\right] \tag{14} \\ &= \dfrac{1}{2}\left[ \log \dfrac{|\Sigma_2|}{|\Sigma_1|} - n + tr\left(\Sigma_2^{-1}\Sigma_1\right) + \left(\mu_2-\mu_1\right)^T\Sigma_2^{-1}\left(\mu_2-\mu_1\right)\right]. \tag{15}\end{align}</script><h3 id="部分推导细节"><a href="#部分推导细节" class="headerlink" title="部分推导细节"></a>部分推导细节</h3><p>在上述推导中有几步需要额外注意，我自己已经在纸上推导好了，这里简要提示一下（具体参见参考文献）：</p>
<ul>
<li>$(4)$式到$(5)$式：利用标量$\Leftrightarrow$标量的迹和迹的交换性；</li>
<li>$(5)$式到$(6)$式：举个例子展开写就明白了；</li>
<li>$(8)$式到$(9)$式：其实是$(7)$式到$(9)$式，注意对谁求期望和协方差的定义；</li>
<li>$(14)$式到$(15)$式：标量$\Leftrightarrow$标量的迹。</li>
</ul>
<h3 id="PyTorch官方函数的写法"><a href="#PyTorch官方函数的写法" class="headerlink" title="PyTorch官方函数的写法"></a><code>PyTorch</code>官方函数的写法</h3><p>最后$(15)$式中的结果与<code>PyTorch</code>中多元高斯分布KL散度的表达是一致的，<code>half_term1</code>就是$\dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|}$，<code>term2</code>就是$tr\left(\Sigma_2^{-1}\Sigma_1\right)$，<code>term3</code>就是$\left(\mu_2-\mu_1\right)^T\Sigma_2^{-1}\left(\mu_2-\mu_1\right)$：</p>
<img src="/2020/06/14/多元高斯分布之间的KL散度/KL.png" title="PyTorch中多元高斯分布KL散度函数">
<h3 id="beta-散度的想法"><a href="#beta-散度的想法" class="headerlink" title="$\beta$散度的想法"></a>$\beta$散度的想法</h3><p>其实本来是想计算$\beta$和$\gamma$散度的类似表达的…只是我真的算不出来…估计只能近似了。列个$\beta$散度的定义，有待研究。</p>
<script type="math/tex; mode=display">\displaystyle D_{\beta}(p(x)||q(x)) = \dfrac{1}{\beta}\int p(x)^{1+\beta}dx - \dfrac{\beta + 1}{\beta}\int p(x)q(x)^{beta}dx + \int q(x)^{1+\beta}dx. \tag{16}</script><h3 id="参考链接列表"><a href="#参考链接列表" class="headerlink" title="参考链接列表"></a>参考链接列表</h3><ul>
<li><a href="https://github.com/pytorch/pytorch/blob/master/torch/distributions/kl.py" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/blob/master/torch/distributions/kl.py</a></li>
<li><a href="https://blog.csdn.net/sinat_33598258/article/details/103866549" target="_blank" rel="noopener">https://blog.csdn.net/sinat_33598258/article/details/103866549</a></li>
<li><a href="https://arxiv.org/pdf/1905.09961.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.09961.pdf</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/143124676" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/143124676</a></li>
<li><a href="https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians</a></li>
</ul>
]]></content>
      <tags>
        <tag>KL Divergence</tag>
      </tags>
  </entry>
  <entry>
    <title>NBA2K13鹰重整理</title>
    <url>/2020/06/13/NBA2K13%E9%B9%B0%E9%87%8D%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><a id="more"></a>
<h2 id="鹰🦅修改"><a href="#鹰🦅修改" class="headerlink" title="鹰🦅修改"></a>鹰🦅修改</h2><h3 id="Trae-Young"><a href="#Trae-Young" class="headerlink" title="Trae Young"></a>Trae Young</h3><p>用雄鹿的Beno Udo…。名字54726165 596f756e67。号码11=22=16，肤色03，位置PG/SG=00/01，年龄21岁7e 7c，身高6‘2’=188，体重180，原头99 04=1177，时间35=140=8c。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>90</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>1799b0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>08e80</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close   92 5c</td>
<td>2 med  91 5b</td>
<td>3 3pt 90 5a</td>
<td>4 ft 85 55</td>
<td>5 layup  87 57</td>
<td>6 dunk  58 3a</td>
<td>28 stdnk  50 1e</td>
<td><strong>23  sit</strong>  55 37</td>
<td><strong>22  sod</strong>  69 45</td>
<td>25 hus  72 48</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  92 5c</td>
<td><strong>8  pass</strong>  92 5c</td>
<td>39</td>
<td>32</td>
<td>9 opost  58 3a</td>
<td>10 dpost  60 3c</td>
<td>11 block   60 3c</td>
<td>26 hnd  64 40</td>
<td>12 steal  50 32</td>
<td>15 speed  86 56</td>
<td>16 stam  91 5b</td>
<td>19</td>
<td>21 vert 90 5a</td>
<td>13 oreb  50 21</td>
<td>14 dreb  50 24</td>
<td>17 dur  70 46</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  72 48</td>
<td><strong>19  oawr</strong>  98 62</td>
<td>32</td>
<td>54</td>
<td>27 def 88 58</td>
<td>24 qui 88 58</td>
<td>潜力 89 59</td>
<td>20 str  60 3c</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Clint-Capela"><a href="#Clint-Capela" class="headerlink" title="Clint Capela"></a>Clint Capela</h3><p>用Alexis Ajinca。身高6‘11’=208，体重240，年龄26=2d 7c，号码17=34=22，肤色01，位置C/05=04/05，原头25 06=1573，名字436c696e74 436170656c61，时间33=132=84。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>86</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>181960</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>71700</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close   73 49</td>
<td>2 med  60 3c</td>
<td>3 3pt 50 32</td>
<td>4 ft 54 36</td>
<td>5 layup  76 4c</td>
<td>6 dunk  82 52</td>
<td>28 stdnk  96 60</td>
<td><strong>23  sit</strong>  88 58</td>
<td><strong>22  sod</strong>  70 46</td>
<td>25 hus  82 52</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  70 46</td>
<td><strong>8  pass</strong>  64 40</td>
<td>32</td>
<td>32</td>
<td>9 opost  79 4f</td>
<td>10 dpost  84 54</td>
<td>11 block   81 51</td>
<td>26 hnd  67 43</td>
<td>12 steal  52 34</td>
<td>15 speed  68 44</td>
<td>16 stam  83 53</td>
<td>19</td>
<td>21 vert 71 47</td>
<td>13 oreb  86 56</td>
<td>14 dreb  94 5e</td>
<td>17 dur  80 50</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  84 54</td>
<td><strong>19  oawr</strong>  79 4f</td>
<td>19</td>
<td>4c</td>
<td>27 def 69 45</td>
<td>24 qui 59 3b</td>
<td>潜力 55</td>
<td>20 str  80 50</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Qi-Zhou🤭"><a href="#Qi-Zhou🤭" class="headerlink" title="Qi Zhou🤭"></a>Qi Zhou🤭</h3><p>用凯尔特人Fab Melo改。名字5169 5a686f75，身高7‘1’=217，体重210，年龄24=43 7c，号码9=18=12，肤色04，位置C/PF=04/03，原头a1 09=2465，时间15=60=3c。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM之前</th>
<th>69</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>179dd0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>0d980</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close   87 57</td>
<td>2 med  87 57</td>
<td>3 3pt 82 52</td>
<td>4 ft 88 58</td>
<td>5 layup  75 4b</td>
<td>6 dunk  75 4b</td>
<td>28 stdnk  99 63</td>
<td><strong>23  sit</strong>  67 43</td>
<td><strong>22  sod</strong>  67 43</td>
<td>25 hus  89 59</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  61 3d</td>
<td><strong>8  pass</strong>  50 1d</td>
<td>32</td>
<td>32</td>
<td>9 opost  64 40</td>
<td>10 dpost  61 3d</td>
<td>11 block   90 5a</td>
<td>26 hnd  82 52</td>
<td>12 steal  68 44</td>
<td>15 speed  79 4f</td>
<td>16 stam  90 5a</td>
<td>28</td>
<td>21 vert 80 50</td>
<td>13 oreb  84 54</td>
<td>14 dreb  87 57</td>
<td>17 dur  80 50</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  64 40</td>
<td><strong>19  oawr</strong>  64 40</td>
<td>23</td>
<td>50</td>
<td>27 def 77 4d</td>
<td>24 qui 77 4d</td>
<td>潜力 83 53</td>
<td>20 str  72 48</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Kevin-Huerter"><a href="#Kevin-Huerter" class="headerlink" title="Kevin Huerter"></a>Kevin Huerter</h3><p>位置SG/SF=01/02，</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM</th>
<th>74</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close   87 57</td>
<td>2 med  87 57</td>
<td>3 3pt 82 52</td>
<td>4 ft 88 58</td>
<td>5 layup  75 4b</td>
<td>6 dunk  75 4b</td>
<td>28 stdnk  99 63</td>
<td><strong>23  sit</strong>  67 43</td>
<td><strong>22  sod</strong>  67 43</td>
<td>25 hus  89 59</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  61 3d</td>
<td><strong>8  pass</strong>  50 1d</td>
<td>32</td>
<td>32</td>
<td>9 opost  64 40</td>
<td>10 dpost  61 3d</td>
<td>11 block   90 5a</td>
<td>26 hnd  82 52</td>
<td>12 steal  68 44</td>
<td>15 speed  79 4f</td>
<td>16 stam  90 5a</td>
<td>28</td>
<td>21 vert 80 50</td>
<td>13 oreb  84 54</td>
<td>14 dreb  87 57</td>
<td>17 dur  80 50</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  64 40</td>
<td><strong>19  oawr</strong>  64 40</td>
<td>23</td>
<td>50</td>
<td>27 def 77 4d</td>
<td>24 qui 77 4d</td>
<td>潜力 83 53</td>
<td>20 str  72 48</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <tags>
        <tag>NBA2K13</tag>
        <tag>DIY修改</tag>
      </tags>
  </entry>
  <entry>
    <title>NBA2K13太阳重整理</title>
    <url>/2020/06/12/hexo/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="太阳修改"><a href="#太阳修改" class="headerlink" title="太阳修改"></a>太阳修改</h2><h3 id="Devin-Booker"><a href="#Devin-Booker" class="headerlink" title="Devin Booker"></a>Devin Booker</h3><p>网队Keith Bogans。446576696e 426f6f6b6572。</p>
<p>号码01；位置SG/SF=01/02；身高6‘6’=196，体重210；潜力89=59；时间36=144=90；白人肤色05；面补19 04=25+1024=1049。KM3542</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>86</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17aa30</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>1f860</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  90 5a</td>
<td>2 med  85 55</td>
<td>3 3pt  82 52</td>
<td>4 ft 91 5b</td>
<td>5 layup  87 57</td>
<td>6 dunk  60 3c</td>
<td>28 stdnk  69 45</td>
<td><strong>23  sit</strong> 66 42</td>
<td><strong>22  sod</strong>  67 43</td>
<td>25 hus  68 44</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  86 56</td>
<td><strong>8  pass</strong>  79 4f</td>
<td>40</td>
<td>40</td>
<td>9 opost  60 3c</td>
<td>10 dpost  60 3c</td>
<td>11 block   50 1c</td>
<td>26 hnd  76 4c</td>
<td>12 steal  50 32</td>
<td>15 speed  78 4e</td>
<td>16 stam  79 4f</td>
<td>19</td>
<td>21 vert 54 36</td>
<td>13 oreb  78 4e</td>
<td>14 dreb  63 3f</td>
<td>17 dur  86 56</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  60 3c</td>
<td><strong>19  oawr</strong>  98 62</td>
<td>3c</td>
<td>41</td>
<td>27 def 58 3a</td>
<td>24 qui 75 4b</td>
<td>潜力 59</td>
<td>20 str 50 32</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Deandre-Ayton"><a href="#Deandre-Ayton" class="headerlink" title="Deandre Ayton"></a>Deandre Ayton</h3><p>76人Spencer Hawes。4465616e647265 4179746f6e。</p>
<p>号码22=44=2c；位置C/PF=04/03；身高7‘1’=216.5，体重250；潜力92=5c；时间33=132=84；肤色01；面补8b 05=139+1280=1419；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>85</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>179660</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>05640</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  91 5b</td>
<td>2 med  71 47</td>
<td>3 3pt  60 3c</td>
<td>4 ft 75 4b</td>
<td>5 layup  79 4f</td>
<td>6 dunk  75 4b</td>
<td>28 stdnk  90 5a</td>
<td><strong>23  sit</strong> 65 41</td>
<td><strong>22  sod</strong>  50 2d</td>
<td>25 hus  82 52</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  50 25</td>
<td><strong>8  pass</strong>  50 2a</td>
<td>40</td>
<td>40</td>
<td>9 opost  82 52</td>
<td>10 dpost  85 55</td>
<td>11 block   81 51</td>
<td>26 hnd  66 42</td>
<td>12 steal  50 2a</td>
<td>15 speed  77 4d</td>
<td>16 stam  78 4e</td>
<td>3c</td>
<td>21 vert 52 34</td>
<td>13 oreb  85 55</td>
<td>14 dreb  89 59</td>
<td>17 dur  70 46</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  65 41</td>
<td><strong>19  oawr</strong>  90 5a</td>
<td>41</td>
<td>3b</td>
<td>27 def 62 3e</td>
<td>24 qui 55 37</td>
<td>潜力</td>
<td>20 str 75 4b</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <tags>
        <tag>NBA2K13</tag>
        <tag>DIY</tag>
      </tags>
  </entry>
  <entry>
    <title>NBA2K13森林狼重整理</title>
    <url>/2020/06/12/NBA2K13%E6%A3%AE%E6%9E%97%E7%8B%BC%E9%87%8D%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p> 此文仅整理记录安卓原版2K13非root情况下可用于修改DIY的球员名单😜。</p>
<p> 之前的大名单整理内容实在是太多了，整理的时候很容易弄混，因此开始分解名单更新。</p>
<p> 如有不当的地方，请联系我删除。</p>
</blockquote>
<a id="more"></a>
<h2 id="森林狼修改"><a href="#森林狼修改" class="headerlink" title="森林狼修改"></a>森林狼修改</h2><p>全队的时间按照stat-nba2020常规赛修改</p>
<h3 id="Karl-Anthony-Towns"><a href="#Karl-Anthony-Towns" class="headerlink" title="Karl-Anthony Towns"></a>Karl-Anthony Towns</h3><div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM405</th>
<th>90</th>
<th>88</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>179620</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>05280</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close   91 5b</td>
<td>2 med  87 57</td>
<td>3 3pt 79 4f</td>
<td>4 ft 83 53</td>
<td>5 layup  87 57</td>
<td>6 dunk  85 55</td>
<td>28 stdnk  99 63</td>
<td><strong>23  sit</strong>  55 37</td>
<td><strong>22  sod</strong>  68 44</td>
<td>25 hus  60 3c</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  71 47</td>
<td><strong>8  pass</strong>  62 3e</td>
<td>32</td>
<td>40</td>
<td>9 opost  86 56</td>
<td>10 dpost  78 4e</td>
<td>11 block   72 48</td>
<td>26 hnd  72 48</td>
<td>12 steal  66 42</td>
<td>15 speed  80 50</td>
<td>16 stam  90 5a</td>
<td>3c</td>
<td>21 vert 72 48</td>
<td>13 oreb  76 4c</td>
<td>14 dreb  89 59</td>
<td>17 dur  78 4e</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  81 51</td>
<td><strong>19  oawr</strong>  82 52</td>
<td>50</td>
<td>5b</td>
<td>27 def 73 49</td>
<td>24 qui 90 5a</td>
<td>潜力 93 5d</td>
<td>20 str  79 4f</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>目前考虑用原76人的Jason改。</p>
<p>名字4b61726c2d416e74686f6e79 546f776e73；位置C，04/05；身高6‘11’，211；体重248；32号-&gt;64-&gt;40；上场时间34min-&gt;136-&gt;88；面补用KM的，原面补0805原来整理好了；年龄24=4c 7c；原肤色00，太黑了，改成03。</p>
<p>注：身高6‘6=198.12，年龄31=da 7b</p>
<h3 id="D’Angelo-Russell"><a href="#D’Angelo-Russell" class="headerlink" title="D’Angelo Russell"></a>D’Angelo Russell</h3><div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM405</th>
<th>84</th>
<th>83</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17a980</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>1e3c0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  81 51</td>
<td>2 med  93 5d</td>
<td>3 3pt  82 52</td>
<td>4 ft 77 4d</td>
<td>5 layup  89 59</td>
<td>6 dunk  60 3c</td>
<td>28 stdnk  50 23</td>
<td><strong>23  sit</strong> 58 3a</td>
<td><strong>22  sod</strong>  55 37</td>
<td>25 hus  81 51</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  86 56</td>
<td><strong>8  pass</strong>  82 52</td>
<td>49</td>
<td>32</td>
<td>9 opost  68 44</td>
<td>10 dpost  69 45</td>
<td>11 block   54 36</td>
<td>26 hnd  74 4a</td>
<td>12 steal  50 32</td>
<td>15 speed  83 53</td>
<td>16 stam  88 58</td>
<td>19</td>
<td>21 vert 70 46</td>
<td>13 oreb  70 46</td>
<td>14 dreb  67 43</td>
<td>17 dur  75 4b</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  74 4a</td>
<td><strong>19  oawr</strong>  84 54</td>
<td>55</td>
<td>5a</td>
<td>27 def 68 44</td>
<td>24 qui 60 3c</td>
<td>潜力 57</td>
<td>20 str 53 35</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<p>换人改，不用比卢普斯，有两个候选，小牛的和太阳的，选了小牛的Rodrigue 526f647269677565 Beaubois。</p>
<p>名字4427416e67656c6f 52757373656c6c；位置PG/SG，00/01；上场时间32.6min-&gt;130-&gt;82；原肤色02，好像不行改成04；原面补65 06=1637，KM的用了即可；身高6‘4’=1.93？不行，还是6‘3’，改成194试试，好了；体重195。号码0号。</p>
<p>投篮动作改成鹰Harris（数据11b20）的。动作的位置在一堆ff上面的08和0c位置。Harris的2c 57。比卢普斯的是0a 17.</p>
<p>变向动作参考利拉德的试一试。不对啊，他的动作也是00 00 00 08 07 00 00 00 07 00 00 00，把08 07删了试试</p>
<p>ps：年龄36 = 8c 7b；身高6’3‘=190.5。原身高6‘1’=187.95，24=4c 7c</p>
<h3 id="Malik-Beasley"><a href="#Malik-Beasley" class="headerlink" title="Malik Beasley"></a>Malik Beasley</h3><p>魔术的Arron Afflalo已经被用了，换一个活塞头号种子Rodney Stuckey 537475636b6579。名字4d616c696b 426561736c6579。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM405</th>
<th>78</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17ac70</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>22ec0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  55 37</td>
<td>2 med  85 55</td>
<td>3 3pt  86 56</td>
<td>4 ft 85 55</td>
<td>5 layup  85 55</td>
<td>6 dunk  85 55</td>
<td>28 stdnk  50 23</td>
<td><strong>23  sit</strong> 72 48</td>
<td><strong>22  sod</strong>  81 51</td>
<td>25 hus  73 49</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  74 4a</td>
<td><strong>8  pass</strong>  60 3c</td>
<td>32</td>
<td>32</td>
<td>9 opost  57 39</td>
<td>10 dpost  59 3b</td>
<td>11 block   50 22</td>
<td>26 hnd  78 4e</td>
<td>12 steal  59 3b</td>
<td>15 speed  77 4d</td>
<td>16 stam  96 60</td>
<td>19</td>
<td>21 vert 82 52</td>
<td>13 oreb  63 3f</td>
<td>14 dreb  63 3f</td>
<td>17 dur  85 55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  60 3c</td>
<td><strong>19  oawr</strong>  85 55</td>
<td>41</td>
<td>5a</td>
<td>27 def 77 4d</td>
<td>24 qui 98 62</td>
<td>潜力 78 4e</td>
<td>20 str 51 33</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<p>身高6‘4’=193.5，体重194，年龄27。号码5号，位置SG=01/05。面补90 05=1424。时间89-&gt;33.1min=132=84。肤色黑黑的02试试，原肤色01.</p>
<p>ps:6’5’=195.58</p>
<h3 id="旧球员-James-Johnson"><a href="#旧球员-James-Johnson" class="headerlink" title="旧球员 James Johnson"></a>旧球员 James Johnson</h3><p>在原国王队，注意要改成PF/05，03/05。号码，16=32=20号。原上场时间02，改为24min=96=60.面补49 06=1609。</p>
<p>名字被之前哪个collins改掉了。。。应该是🦅的Ivan Johnson 4a6f686e736f6e，给了John科林斯。。。17a120</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM最新</th>
<th>76</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17a120</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>18060</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  82 52</td>
<td>2 med  60 3c</td>
<td>3 3pt 79 4f</td>
<td>4 ft 71 47</td>
<td>5 layup  86 56</td>
<td>6 dunk  80 50</td>
<td>28 stdnk  60 3c</td>
<td><strong>23  sit</strong> 76 4c</td>
<td><strong>22  sod</strong>  77 4d</td>
<td>25 hus  76 4c</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  74 4a</td>
<td><strong>8  pass</strong>  70 46</td>
<td>32</td>
<td>32</td>
<td>9 opost  78 4e</td>
<td>10 dpost  75 4b</td>
<td>11 block   80 50</td>
<td>26 hnd  92 5c</td>
<td>12 steal  50 32</td>
<td>15 speed  69 45</td>
<td>16 stam  58 3a</td>
<td>19</td>
<td>21 vert 72 48</td>
<td>13 oreb  50 32</td>
<td>14 dreb  64 40</td>
<td>17 dur  90 5a</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  78 4e</td>
<td><strong>19  oawr</strong>  78 4e</td>
<td>19</td>
<td>54</td>
<td>27 def 68 44</td>
<td>24 qui 84 54</td>
<td>潜力 75 4b</td>
<td>20 str 68 44</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<h3 id="旧球员-Evan-Turner"><a href="#旧球员-Evan-Turner" class="headerlink" title="旧球员 Evan Turner"></a>旧球员 Evan Turner</h3><p>这人好像不在球队了。。。哦还在，改一下吧</p>
<p>原76人队。注意位置SG/05，01/05。号码1号=02。时间由72-&gt;13min=52=34。面补0a 07=1802.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>73</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>05460</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  69 45</td>
<td>2 med  80 50</td>
<td>3 3pt 57 39</td>
<td>4 ft 71 47</td>
<td>5 layup  80 50</td>
<td>6 dunk  64 40</td>
<td>28 stdnk  50 28</td>
<td><strong>23  sit</strong> 68 44</td>
<td><strong>22  sod</strong>  75 4b</td>
<td>25 hus  74 4a</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  77 4d</td>
<td><strong>8  pass</strong>  70 46</td>
<td>32</td>
<td>32</td>
<td>9 opost  67 43</td>
<td>10 dpost  55 37</td>
<td>11 block   62 3e</td>
<td>26 hnd  73 49</td>
<td>12 steal  60 3c</td>
<td>15 speed  76 4c</td>
<td>16 stam  95 5f</td>
<td>19</td>
<td>21 vert 78 4e</td>
<td>13 oreb  68 44</td>
<td>14 dreb  68 44</td>
<td>17 dur  85 55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  72 48</td>
<td><strong>19  oawr</strong>  72 48</td>
<td>32</td>
<td>50</td>
<td>27 def 71 47</td>
<td>24 qui 85 55</td>
<td>潜力 75 4b</td>
<td>20 str 69 45</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Juancho-Hernangomez"><a href="#Juancho-Hernangomez" class="headerlink" title="Juancho Hernangomez"></a>Juancho Hernangomez</h3><p>真的没有合适长度的名字了，我选择用慈世平，first name的长度只能有5，怎么说？哈哈来个Juanc。4a75616e63 4865726e616e676f6d657a。</p>
<p>412数据包里这个面补是亚当斯</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘9’=206</td>
<td>214espn</td>
<td>3d 02=61+512=0573</td>
<td>PF</td>
<td>05</td>
<td>29.4-117.6-76</td>
<td>24</td>
<td>41-82-52</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>74</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名</td>
<td>17a660</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>1a040</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 3d</td>
<td>2 med  37</td>
<td>3 3pt  54</td>
<td>4 ft  4b</td>
<td>5 layup  4a</td>
<td>6 dunk  3e</td>
<td>28 stdnk  4e</td>
<td><strong>23  sit</strong>  47</td>
<td><strong>22  sod</strong>  37</td>
<td>25 hus  48</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  37</td>
<td><strong>8  pass</strong>  39</td>
<td>32</td>
<td>49</td>
<td>9 opost  49</td>
<td>10 dpost  4a</td>
<td>11 block  4d</td>
<td>26 hnd  49</td>
<td>12 steal  32</td>
<td>15 speed  49</td>
<td>16 stam  51</td>
<td>63</td>
<td>21 vert  50</td>
<td>13 oreb  4e</td>
<td>14 dreb  4f</td>
<td>17 dur  32</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  4d</td>
<td><strong>19  oawr</strong>  43</td>
<td>2d</td>
<td>3d</td>
<td>27 def  32</td>
<td>24 qui 33</td>
<td>潜力 4d</td>
<td>20 str  51</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Josh-Okogie"><a href="#Josh-Okogie" class="headerlink" title="Josh Okogie"></a>Josh Okogie</h3><p>网队Donte Greene。</p>
<p>4a6f7368 4f6b6f676965.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘4’=194</td>
<td>212</td>
<td>fd 05=1533</td>
<td>SF/SG</td>
<td>03</td>
<td>24.9-100-64</td>
<td></td>
<td>20-40-28</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>75</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名</td>
<td>17aa40</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>1fa40</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 42</td>
<td>2 med  45</td>
<td>3 3pt  3e</td>
<td>4 ft  46</td>
<td>5 layup  4b</td>
<td>6 dunk  35</td>
<td>28 stdnk  32</td>
<td><strong>23  sit</strong>  3f</td>
<td><strong>22  sod</strong>  4a</td>
<td>25 hus  3e</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  4c</td>
<td><strong>8  pass</strong>  45</td>
<td>32</td>
<td>32</td>
<td>9 opost  4c</td>
<td>10 dpost  48</td>
<td>11 block  35</td>
<td>26 hnd  4e</td>
<td>12 steal  42</td>
<td>15 speed  61</td>
<td>16 stam  58</td>
<td>63</td>
<td>21 vert  50</td>
<td>13 oreb  34</td>
<td>14 dreb  3b</td>
<td>17 dur  55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  50</td>
<td><strong>19  oawr</strong>  50</td>
<td>28</td>
<td>4b</td>
<td>27 def  47</td>
<td>24 qui 5a</td>
<td>潜力3f</td>
<td>20 str 2e</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Omari-Spellman"><a href="#Omari-Spellman" class="headerlink" title="Omari Spellman"></a>Omari Spellman</h3><p>C位没名字了，马刺的PF Tiago 546961676f Splitter。</p>
<p>4f6d617269 5370656c6c6d616e。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘8’=205</td>
<td>245</td>
<td>47 07=1863</td>
<td>C</td>
<td>03</td>
<td>2019勇士18.1-72-48</td>
<td></td>
<td>14-28-1c</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>76</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名</td>
<td>17b000</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>28140</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 4b</td>
<td>2 med  4a</td>
<td>3 3pt  53</td>
<td>4 ft  51</td>
<td>5 layup  46</td>
<td>6 dunk  3c</td>
<td>28 stdnk  46</td>
<td><strong>23  sit</strong>  3e</td>
<td><strong>22  sod</strong>  23</td>
<td>25 hus  4f</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  23</td>
<td><strong>8  pass</strong>  1d</td>
<td>32</td>
<td>32</td>
<td>9 opost  48</td>
<td>10 dpost  4b</td>
<td>11 block  40</td>
<td>26 hnd  4b</td>
<td>12 steal  3f</td>
<td>15 speed  38</td>
<td>16 stam  58</td>
<td>1e</td>
<td>21 vert  34</td>
<td>13 oreb  4b</td>
<td>14 dreb  4e</td>
<td>17 dur  4b</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  4c</td>
<td><strong>19  oawr</strong>  4e</td>
<td>28</td>
<td>50</td>
<td>27 def  34</td>
<td>24 qui 32</td>
<td>潜力 50</td>
<td>20 str 37</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Jake-Layman"><a href="#Jake-Layman" class="headerlink" title="Jake Layman"></a>Jake Layman</h3><p>可用的人不少，字数刚好的比较鸡，是黄蜂的Jeff Taylor 5461796c6f72，这样倾向应该会比较菜。没好的面补!!!</p>
<p>4a616b65 4c61796d616e.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘8‘=205(.8就变成6‘9’了)</td>
<td>214</td>
<td>aa 09=2474</td>
<td>SF=02/05</td>
<td>04</td>
<td>22-88-58</td>
<td>？</td>
<td>10-20-14</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名</td>
<td>1798b0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>7da0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  44</td>
<td>2 med  4e</td>
<td>3 3pt  4c</td>
<td>4 ft  46</td>
<td>5 layup  40</td>
<td>6 dunk  3c</td>
<td>28 stdnk  4a</td>
<td><strong>23  sit</strong>  42</td>
<td><strong>22  sod</strong>  43</td>
<td>25 hus  4e</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  44</td>
<td><strong>8  pass</strong>  41</td>
<td>32</td>
<td>32</td>
<td>9 opost  46</td>
<td>10 dpost  47</td>
<td>11 block  3e</td>
<td>26 hnd  38</td>
<td>12 steal  32</td>
<td>15 speed  46</td>
<td>16 stam  50</td>
<td>28</td>
<td>21 vert  52</td>
<td>13 oreb  30</td>
<td>14 dreb  32</td>
<td>17 dur  4b</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  4d</td>
<td><strong>19  oawr</strong>  4c</td>
<td>23</td>
<td>50</td>
<td>27 def  40</td>
<td>24 qui 3e</td>
<td>潜力 4c</td>
<td>20 str 37</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Jarrett-Culver"><a href="#Jarrett-Culver" class="headerlink" title="Jarrett Culver"></a>Jarrett Culver</h3><div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>=</td>
<td></td>
<td></td>
<td></td>
<td>0</td>
<td></td>
<td>？</td>
<td>-20-14</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close</td>
<td>2 med  4e</td>
<td>3 3pt  4c</td>
<td>4 ft  46</td>
<td>5 layup  40</td>
<td>6 dunk  3c</td>
<td>28 stdnk  4a</td>
<td><strong>23  sit</strong>  42</td>
<td><strong>22  sod</strong>  43</td>
<td>25 hus  4e</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  44</td>
<td><strong>8  pass</strong>  41</td>
<td>32</td>
<td>32</td>
<td>9 opost  46</td>
<td>10 dpost  47</td>
<td>11 block  3e</td>
<td>26 hnd  38</td>
<td>12 steal  32</td>
<td>15 speed  46</td>
<td>16 stam  50</td>
<td>28</td>
<td>21 vert  52</td>
<td>13 oreb  30</td>
<td>14 dreb  32</td>
<td>17 dur  4b</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  4d</td>
<td><strong>19  oawr</strong>  4c</td>
<td>23</td>
<td>50</td>
<td>27 def  40</td>
<td>24 qui 3e</td>
<td>潜力 4c</td>
<td>20 str 37</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Jordan-McLaughlin"><a href="#Jordan-McLaughlin" class="headerlink" title="Jordan McLaughlin"></a><strong>Jordan McLaughlin</strong></h3><p>改的不好，但是实在没名字了，用尼克斯的PF Amar’s 416d61722773 Stoudemire。</p>
<p>4a6f7264616e 4d634c617567686c696e。</p>
<p>身高180不行是5‘10’，180.5差不多</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>5‘11‘=</td>
<td>170</td>
<td>8d 03=0909</td>
<td>PG=00/05</td>
<td>03</td>
<td>19.7-80-50</td>
<td>？</td>
<td>6-12-0c</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>74</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名</td>
<td>17a5a0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>18d80</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close   52</td>
<td>2 med  4a</td>
<td>3 3pt  50</td>
<td>4 ft  42</td>
<td>5 layup  51</td>
<td>6 dunk  32</td>
<td>28 stdnk  42</td>
<td><strong>23  sit</strong>  43</td>
<td><strong>22  sod</strong>  4b</td>
<td>25 hus  4d</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  54</td>
<td><strong>8  pass</strong>  50</td>
<td>3f</td>
<td>35</td>
<td>9 opost  3f</td>
<td>10 dpost  44</td>
<td>11 block  32</td>
<td>26 hnd  3e</td>
<td>12 steal  49</td>
<td>15 speed  4a</td>
<td>16 stam  54</td>
<td>63</td>
<td>21 vert  4d</td>
<td>13 oreb  32</td>
<td>14 dreb  32</td>
<td>17 dur  46</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  43</td>
<td><strong>19  oawr</strong>  4f</td>
<td>50</td>
<td>44</td>
<td>27 def  37</td>
<td>24 qui 3c</td>
<td>潜力 5f</td>
<td>20 str 32</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <tags>
        <tag>NBA2K13</tag>
        <tag>DIY</tag>
      </tags>
  </entry>
  <entry>
    <title>9 Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks 笔记</title>
    <url>/2020/06/12/9-Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks-%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>MAML文献回顾</p>
</blockquote>
<a id="more"></a>
<h2 id="文献简介"><a href="#文献简介" class="headerlink" title="文献简介"></a>文献简介</h2><img src="/2020/06/12/9-Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks-笔记/9.png" title="MAML">
<p>其实寒假的时候就读过这篇文章，<strong>当时的理解</strong>主要是：搞了个新模型去弄原始模型的参数，新模型的目标是使原始模型Loss下一次梯度下降得更多，这样似乎面对新任务新数据时只要几次迭代Loss就能降不少，需要计算迭代的次数因此可能很少。现在希望<strong>回顾</strong>一下。</p>
<ul>
<li>文献地址：<a href="https://arxiv.org/abs/1703.03400或https://dl.acm.org/doi/10.5555/3305381.3305498" target="_blank" rel="noopener">https://arxiv.org/abs/1703.03400或https://dl.acm.org/doi/10.5555/3305381.3305498</a></li>
<li>文献投于ICML2017</li>
<li>Finn作品，另有参考BAIR的博客<a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/" target="_blank" rel="noopener">Learning to Learn</a></li>
<li>截至2020/06/12谷歌学术引用量1664</li>
</ul>
<h2 id="阅读笔记——文章主题整理"><a href="#阅读笔记——文章主题整理" class="headerlink" title="阅读笔记——文章主题整理"></a>阅读笔记——文章主题整理</h2><p>这篇文章提出的是一种经典的元学习方法，记为<code>MAML</code>。其卖点在于<code>模型不可知</code>，对于一般使用梯度下降的模型，都可以以<code>MAML</code>的方法使之能<strong>快速适应新任务</strong>。</p>
<p>MAML的思想来源是元学习，假定有一系列共同或相似内在联系的任务，那么希望机器能把任务之间的关联抽象为所谓的经验，辅助新任务的训练。但是这样一种<strong>general的经验</strong>不好确定啊，而且直观地说，此经验一定是要随着新任务变化而稍有变化的，才能使之合理地成为元学习器。</p>
<p>其思想就是找到这样的经验并用于加强新任务的<strong>泛化</strong>能力，很自然地，这个经验抽象为了模型的初始参数。在元学习的假设下，我们有很多之前的任务$T_i$（经验）和待解决的新任务，它们都可以从任务分布$p(T)$中采样出来。目标是新模型的快速适应能力，即只要很少的迭代次数就能达到较优的效果，那么，初始参数要使得损失的变化最大，只要微调，性能快速变化。</p>
<p>所以考虑模型$f_{\theta}$，其损失$L$。先采样一个过去的任务$T_i\sim p(T)$（也可以采样多个任务，这里以一个为例），在此任务上计算$L_{T_i}(f_{\theta})$，从中提取经验的信息，对初始参数$\theta$求导得$\nabla_{\theta}L_{T_i}(f_{\theta})$。为了使Loss对初始参数的变化更加敏感，对当前初始参数进行一步梯度更新得到$\theta’$。再在重新采样的元任务上确认效果，如何保证Loss对初始参数更敏感？$\theta’$要让经过一次迭代后元任务上的损失就达到最小$\Leftrightarrow$</p>
<script type="math/tex; mode=display">\min\limits_{\theta}\sum\limits_{T_i\sim p(T)}L_{T_i}(f_{\theta'})=\min\limits_{\theta}\sum\limits_{T_i\sim p(T)}L_{T_i}(f_{\theta-\alpha\nabla_{\theta}L_{T_i}(f_{\theta})})</script><p>骚操作，实验不看了，有空研究下代码就更懂了🤭</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>MAML</tag>
        <tag>Meta Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>8 A Generic First-Order Algorithmic Framework for Bi-Level Programming Beyond Lower-Level Singleton 笔记</title>
    <url>/2020/06/10/8-A-Generic-First-Order-Algorithmic-Framework-for-Bi-Level-Programming-Beyond-Lower-Level-Singleton-%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>文献阅读笔记</p>
</blockquote>
<a id="more"></a>
<h2 id="文献简介"><a href="#文献简介" class="headerlink" title="文献简介"></a>文献简介</h2><img src="/2020/06/10/8-A-Generic-First-Order-Algorithmic-Framework-for-Bi-Level-Programming-Beyond-Lower-Level-Singleton-笔记/8.png" title="A Generic First-Order Algorithmic Framework for Bi-Level Programming Beyond Lower-Level Singleton">
<ul>
<li>文献地址：<a href="https://arxiv.org/abs/2006.04045" target="_blank" rel="noopener">https://arxiv.org/abs/2006.04045</a></li>
<li>文献发布于2020年6月</li>
</ul>
<blockquote>
<p>ps：发现06/10的版本有两处书写错误：</p>
<ul>
<li>(9)式上面有个”We”写成了”W e”</li>
<li>5.2节最后一段方法名字写错了…”BAD”</li>
</ul>
</blockquote>
<h2 id="阅读笔记——文章主题整理"><a href="#阅读笔记——文章主题整理" class="headerlink" title="阅读笔记——文章主题整理"></a>阅读笔记——文章主题整理</h2><p>这篇文章研究的对象是<strong>双边优化</strong><code>Bi-Level</code>问题，记为<code>BLP</code>。该类问题虽然早就被提出，但最近一段时间，各种深度学习的应用、元学习中都开始使用它作为目标函数。它的一般形式如下：</p>
<blockquote>
<p> ps：我喜欢的Meta-Weight-Net的目标也属于此类问题</p>
</blockquote>
<script type="math/tex; mode=display">\min_\limits{x\in \mathcal{X}} F(x,y),\ s.t.\ y\in\mathcal{S}(x),\ where\ \mathcal{S}(x):=\arg\min_\limits{y}f(x,y).</script><p>传统的方法是交替迭代优化的方法，但是它们能针对的是上述双边优化问题中的一小类，即条件中的$y$是单一解，而一般双边优化中$y$可以有多解。当底层优化条件退化为单一解时，这个条件<strong>定义为<code>LLS</code></strong>(Lower-Level Singleton)。那么有LLS条件的双边优化问题形式为：</p>
<blockquote>
<p>ps：查阅百度、谷歌、wiki均未查到此LLS，应该是本文先这样称谓</p>
</blockquote>
<script type="math/tex; mode=display">\min_\limits{x\in \mathcal{X}} F(x,y),\ s.t.\ y=\arg\min_\limits{y}f(x,y).</script><p>定义名称，高层优化问题为upper level，底层为lower level，分别记为<strong>UL问题</strong>和<strong>LL子问题</strong>。那么传统方法不细说，就是我了解的那种交替迭代，放张Meta-Weight-Net的图自行体会吧🤭：</p>
<img src="/2020/06/10/8-A-Generic-First-Order-Algorithmic-Framework-for-Bi-Level-Programming-Beyond-Lower-Level-Singleton-笔记/MWN.png" title="体会交替优化方法">
<p>那么传统方法不行啊，因为一般LLS条件是<strong>不满足</strong>的（文中举了反例并实验验证），研究一般的双边优化问题<strong>更有意义</strong>。传统方法不好的原因是UL和LL问题中的变量是相互依存的，直接梯度方法交替迭代不能考虑这种联系，因此理论上不能完全保证收敛正确，一旦不满足LLS条件就会螺旋升天。</p>
<p>基于此提出<strong><code>BDA</code></strong>方法，即Bi-level Descent Aggregation，基本思想就是集合UL和LL问题中变量的联系，力图解决一般的BLP问题。BDA方法的理论我还不完全明白，因为其中包含了很多优化领域的名词、条件，这个暂时没时间一一理解；索性思想是可以明白的，因此大致介绍BDA方法如下：</p>
<ul>
<li>一般的BLP问题可以写成<code>single-level model</code>：</li>
</ul>
<script type="math/tex; mode=display">\min_\limits{x\in \mathcal{X}} \varphi(x),\ with\ \varphi(x):=\inf_\limits{y\in \mathcal{S}(x)}F(x,y).</script><ul>
<li>那么当有数据观测$x$（固定$x$）时，上述问题简化为：</li>
</ul>
<script type="math/tex; mode=display">\min_\limits{y} F(x,y),\ s.t.\ y\in \mathcal{S}(x).</script><ul>
<li>那么LL子问题中$y$的更新要同时考虑$x$与$y$的联系：</li>
</ul>
<script type="math/tex; mode=display">y_{k+1}(x)=\tau_k (x,y_k(x)),\ k=0,\cdots,K-1.</script><p>  其中$\tau_k (x,\cdot)$是更新策略，可以自行调整，且需要初始化$y_0=\tau_0(x)$。这个更新策略要求考虑$x$与$y$的联系！</p>
<ul>
<li>BLP问题的UL更新转化为：</li>
</ul>
<script type="math/tex; mode=display">\min_\limits{x\in\mathcal{X}} \varphi_K(x):=F(x,y_K(x)).</script><ul>
<li><p>其实就是这么个小东西$\tau$需要很多条件支撑，证明内容也很多</p>
<blockquote>
<p>ps：文中表示这些条件不像LLS难以满足，这些条件都是正常的一般条件，易满足</p>
</blockquote>
</li>
</ul>
<p>最终的反例，数据标签重学，元学习的实验结果表示，这个BDA方法很棒：</p>
<ul>
<li>比对比用的RHG(Reverse Hyper-Gradient)方法普遍效果更好，也很稳定</li>
<li>关键是不受LLS条件约束，不会收敛到虚假的极小</li>
<li>在LL子问题的序列长度$K$增大时效果更强</li>
</ul>
<h2 id="阅读的不足之处："><a href="#阅读的不足之处：" class="headerlink" title="阅读的不足之处："></a>阅读的不足之处：</h2><p>这次笔记没有直接翻译，读了之后按自己的理解捋了一遍。但是其实和翻译差不到哪去…有个问题是只知道BDA方法的改进思想是结合UL和LL问题中变量的相关信息，但不知道理论上究竟为何$\tau$就能成功</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Optimization</tag>
        <tag>Bi-Level</tag>
      </tags>
  </entry>
  <entry>
    <title>7 Variational Inference based on Robust Divergences</title>
    <url>/2020/06/05/%C2%967-Variational-Inference-based-on-Robust-Divergences/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>文献阅读记录，这篇稍微读全一点</p>
</blockquote>
<a id="more"></a>
<h2 id="文献简介"><a href="#文献简介" class="headerlink" title="文献简介"></a>文献简介</h2><img src="/2020/06/05/7-Variational-Inference-based-on-Robust-Divergences/7.png" title="Variational Inference based on Robust Divergences">
<ul>
<li>无附录文献地址：<a href="http://proceedings.mlr.press/v84/futami18a/futami18a.pdf" target="_blank" rel="noopener">http://proceedings.mlr.press/v84/futami18a/futami18a.pdf</a></li>
<li>有附录文献地址：<a href="https://arxiv.org/abs/1710.06595" target="_blank" rel="noopener">https://arxiv.org/abs/1710.06595</a></li>
<li>文献发布于2017年</li>
</ul>
<h2 id="摘要-Intro-结论"><a href="#摘要-Intro-结论" class="headerlink" title="摘要+Intro+结论"></a>摘要+Intro+结论</h2><p>本文研究过程：</p>
<ul>
<li>背景：稳健性在机器学习中十分重要</li>
<li>传统标准方法0：基于模型，使用重尾分布，缺点是只能用于少数简单模型</li>
<li>新方法1：Zellner的贝叶斯变分推断，用KL散度，是我熟知的那个</li>
<li>新方法2：基于方法1，使用$\beta$散度，导出伪贝叶斯变分推断，更稳健，但只针对了简单高斯分布做数学证明</li>
<li>本文方法3：基于方法1，使用稳健的$\beta$或者$\gamma$散度，导出伪贝叶斯变分推断。优点如下：<ul>
<li>针对了更复杂的模型，证明了对ReLu激活的DNN的稳健性。对比了只做简单模型验证的新方法2</li>
<li>完成IF分析，在ReLu激活的DNN上对比了方法1的无界IF，因此对特征和标签扰动都是稳健的；第二个对比是我毕设主文献，其IF分析是渐进有界的，此方法则有限样本必有界</li>
</ul>
</li>
<li>未来工作：拓展到更多复杂模型、与其它推断估计方法结合</li>
</ul>
<h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><ul>
<li><p>MLE与其稳健变式：</p>
<ul>
<li>原始MLE很简单，（2）可以用狄拉克函数推导出来</li>
<li>稳健变式即散度的变化，$\beta$和$\gamma$散度都可以退化到KL散度，（8）式有点恶心，但是自己带入就推出来了（已推）</li>
</ul>
</li>
<li><p>贝叶斯变分推断：</p>
<p>Emmm，真的就是介绍下变分推断…其中（12）式没看明白，应该问题不大…</p>
</li>
</ul>
<h2 id="本文方法——基于稳健散度的稳健推断"><a href="#本文方法——基于稳健散度的稳健推断" class="headerlink" title="本文方法——基于稳健散度的稳健推断"></a>本文方法——基于稳健散度的稳健推断</h2><p>总的方法就是在正常变分推断的基础上，把式（14）中的第一项期望中的KL散度项替换为稳健散度。</p>
<p>以$\beta$散度为例，举了例子，近似的参数后验分布有（18）式给出，具体怎么算的话…积分鬼才，本文表示用好用的分布族+重参数化采样近似去计算。</p>
<p>注：其中有个<strong>不对劲</strong>的地方，$\beta$交叉熵，我查阅了原始论文<a href="https://www.duo.uio.no/bitstream/handle/10852/47786/1997-7.pdf" target="_blank" rel="noopener"><em>Robust and efficient estimation by minimising a density power divergence</em></a>中的（4.1）式，本文好像少了一个常数项，虽然求导不影响啦~</p>
<h2 id="IF分析"><a href="#IF分析" class="headerlink" title="IF分析"></a>IF分析</h2><p>本文的IF定义介绍我不是太明白，但是意思我懂了，即数据偏差导致的变化。接下来的定理我也不懂，反正都是给好的结果。接下来的分析我基本上都明白了，<strong>结果也看懂了</strong>，即IF分析表明稳健散度在DNN（包括分类回归）上对特征和标签扰动导致的IF都是有界的即稳健起来了。除了<strong>（26）式又不懂了</strong>，</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验很正常，数据特征和标签的两种打乱方式，按照所提稳健散度引出的稳健VI，分别在人造数据和真实数据上进行了实验。</p>
<ul>
<li><p>人造数据简单验证了稳健性</p>
</li>
<li><p>用UCI数据打乱的程度与测试对数似然的关系验证了稳健性</p>
</li>
<li>用交叉验证的方法获取较优的稳健散度的参数，并以优异结果验证了稳健性</li>
</ul>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Robust</tag>
        <tag>KL Divergence</tag>
        <tag>Bayesian</tag>
      </tags>
  </entry>
  <entry>
    <title>A General Method for Robust Bayesian Modeling 略读</title>
    <url>/2020/05/11/A-General-Method-for-Robust-Bayesian-Modeling-%E7%95%A5%E8%AF%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>论文略读</p>
</blockquote>
<a id="more"></a>
<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近在查阅一些有关稳健贝叶斯的方法，但是目前查到的大多数都是很早之前的文章，亦或是近年来比较specific的研究。比较general的只有一篇16年的<em>A General Method for Robust Bayesian Modeling</em>。读了引言觉得这篇文章讲的其实是<code>localization</code>的思想，因此稍做记录。</p>
<p>其实早一段时间我就读到过<code>localization</code>的概念，但是不知道它具体指的是什么，隐约觉得它是一种鲁棒（稳健）方法，而这篇文章就印证了当时的猜测。</p>
<p>另外，不知道大家如何翻译<code>Localization</code>，这里姑且叫<code>局部化</code>好了。如有不当，日后更正。</p>
<h2 id="Bayesian-Localization"><a href="#Bayesian-Localization" class="headerlink" title="Bayesian + Localization"></a>Bayesian + Localization</h2><p>这篇文章的goal就是在一般贝叶斯概率框架下，说明一种general的稳健手段，减少异常点或者一些采样误差对原始贝叶斯模型的干扰。这种手段就是<code>localization</code>。</p>
<p>它的思想很简单，就是考虑到使原始贝叶斯模型$\displaystyle p(\beta,\mathbf{x}|\alpha) = p(\beta|\alpha)\prod_{i=1}^n p(x_i|\beta)$出现推断误差的数据因素，即往往收集的观测数据中有异常点（outliers），有因收集或者测量误差导致的corrupted数据点。这些因素是数据本身质量的问题。在不知道哪些数据有问题的情况下，为了使模型能容忍它们的存在，可以稍稍改动贝叶斯原始模型的假设。</p>
<p>贝叶斯原始模型的假设是数据$\mathbf{x}$的分布由参数$\beta$生成，而参数$\beta$的分布族由参数$\alpha$生成。$\alpha$作为分布族的参数，保证的是$\beta$的结构，从而数据的生成也有一致的结构。但是异常点或者损坏数据并不符合这种假设，它们与其对应的真实数据（假设存在嘛，坏数据=真实数据+bias）之间有所差异，这种差异可以用方差描述，即它们是对应真实数据的偏离。这样每个数据都可以设置某种分布，由其真实值作为分布中心，同时允许不好的数据与之中心有所偏离。<code>打个比方就是PRML中二维假设线性回归的数据点服从正态分布</code>。这就是<code>localization</code>的概念，每个数据点都各自存在一个这样自己的分布，所以原始模型修饰为</p>
<script type="math/tex; mode=display">\displaystyle p(\beta,\mathbf{x}|\alpha) = \prod_{i=1}^n p(\beta_i|\alpha)\prod_{i=1}^n p(x_i|\beta) = \prod_{i=1}^n p(\beta_i|\alpha)p(x_i|\beta).</script><p>好了，后面正常进行贝叶斯模型的推断和预测，可能要用到MAP或者VI，EM等方法，看情况使用吧$\sim$</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Chong Wang and David M. Blei. <a href="https://arxiv.org/abs/1510.05078?context=stat.ML" target="_blank" rel="noopener">A general method for robust bayesian modeling</a>, 2015.</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Bayesian Machine Learning</tag>
        <tag>Robust</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2020/04/17/NBA2K13%E5%AE%89%E5%8D%93%E5%8E%9F%E7%89%88%E5%8F%AF%E7%94%A8%E7%90%83%E5%91%98%E5%90%8D%E5%8D%95%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p> 此文仅整理记录安卓原版2K13非root情况下可用于修改DIY的球员名单😜。</p>
<p> 如有不当的地方，请联系我删除。</p>
</blockquote>
<a id="more"></a>
<h2 id="NBA2K13安卓原版可用球员名单整理"><a href="#NBA2K13安卓原版可用球员名单整理" class="headerlink" title="NBA2K13安卓原版可用球员名单整理"></a>NBA2K13安卓原版可用球员名单整理</h2><p>本文参考资源仅为安卓原版2K13游戏数据包，表格由Max Liu整理，侵删。目前仅用于个人DIY游戏资源查阅，也欢迎大佬一起交流！</p>
<p>表格说明：</p>
<ul>
<li><code>可用状态</code>栏指球员当前状态，现役或退役或其它状态。其中<code>可用</code>一般指自由球员，大概率已经进入不了NBA的球员，其它赛事，NBA管理岗或G-League成员；<code>暂可</code>指近年被裁，有重返NBA机会的球员。<strong>总之</strong>，本质上只要是非现役球员均可用。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>队名</th>
<th style="text-align:center">球员名</th>
<th>名字长度</th>
<th>位置</th>
<th>可用状态</th>
<th>给谁用</th>
<th>姓名地址</th>
<th>数据地址</th>
<th>面补序号</th>
</tr>
</thead>
<tbody>
<tr>
<td>76人</td>
<td style="text-align:center">Jrue Holiday</td>
<td>4 7</td>
<td>PG</td>
<td>现役鹈鹕</td>
<td></td>
<td>179600</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jason Richardson</td>
<td>12 10</td>
<td>SG</td>
<td>退役</td>
<td>森林狼1唐斯，给76人josh的作废</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Evan Turner</td>
<td>4 6</td>
<td>SG</td>
<td>现役森林狼</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Spencer Hawes</td>
<td>7 5</td>
<td>C</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Andrew Bynum</td>
<td>6 5</td>
<td>C</td>
<td>可用</td>
<td>月机器</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Thaddeus Young</td>
<td>8 5</td>
<td>PF</td>
<td>现役公牛</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Lavoy Allen</td>
<td>5 5</td>
<td>PF</td>
<td>可用</td>
<td>太阳Dario Saric</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Royal Ivey</td>
<td>5 4</td>
<td>PG</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Kwame Brown</td>
<td>5 5</td>
<td>C</td>
<td>退役</td>
<td>组八次</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Dorell Wright</td>
<td>6 6</td>
<td>SF</td>
<td>俄罗斯联赛</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Arnett Moultrie</td>
<td>6 8</td>
<td>PF</td>
<td>现役CBA</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>黄蜂</td>
<td style="text-align:center">Ramon Sessions</td>
<td>5 8</td>
<td>PG</td>
<td>可用</td>
<td></td>
<td>179740</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Gerald Henderson</td>
<td>6 9</td>
<td>SG</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Michael Kidd-Gilchrist</td>
<td>7 14</td>
<td>SF</td>
<td>现役独行侠</td>
<td>只能他雄鹿1字母哥</td>
<td></td>
<td></td>
<td>2445</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Bismack Biyombo</td>
<td>7 7</td>
<td>PF</td>
<td>现役黄蜂</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Brendan Haywood</td>
<td>7 7</td>
<td>C</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Kemba Walker</td>
<td>/</td>
<td>PG</td>
<td>现役绿军</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Ben Gordon</td>
<td>3 6</td>
<td>SG</td>
<td>可用</td>
<td>网哈里斯</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Tyrus Thomas</td>
<td>5 6</td>
<td>PF</td>
<td>GMGB</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Byron Mullens</td>
<td>5 7</td>
<td>C</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Matt Carroll</td>
<td>4 7</td>
<td>SG</td>
<td>可用</td>
<td>掘金哈里斯</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">DeSagana Diop</td>
<td>8 4</td>
<td>C</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Reggie Williams</td>
<td>6 8</td>
<td>SF</td>
<td>不可用！继承超多</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jeff Taylor</td>
<td>4 6</td>
<td>SF</td>
<td>西班牙联赛</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>雄鹿</td>
<td style="text-align:center">Brandon Jennings</td>
<td>7 8</td>
<td>PG</td>
<td>可用</td>
<td></td>
<td>1798c0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Monta Ellis</td>
<td>5 5</td>
<td>SG</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Luc Richard Mbah a Moute</td>
<td>11 12</td>
<td>SF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Ersan Ilyasova</td>
<td>5 8</td>
<td>PF</td>
<td>现役雄鹿</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Samuel Dalembert</td>
<td>6 9</td>
<td>C</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Mike Dunleavy</td>
<td>4 8</td>
<td>SG</td>
<td>可用。但继承Mike姓氏</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Drew Gooden</td>
<td>4 6</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Ekpe Udoh</td>
<td>4 4</td>
<td>PF</td>
<td>CBA</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Beno Udrih</td>
<td>4 5</td>
<td>PG</td>
<td>可用</td>
<td>特雷杨</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Larry Sanders</td>
<td>5 7</td>
<td>PF</td>
<td>可用</td>
<td>还做尼亚</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Joel Przybilla</td>
<td>4 9</td>
<td>C</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">John Henson</td>
<td>4 6</td>
<td>PF</td>
<td>现役活塞</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Tobias Harris</td>
<td>/</td>
<td>SF</td>
<td>现役76人</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>公牛</td>
<td style="text-align:center">Derrick Rose</td>
<td>7 4</td>
<td>PG</td>
<td>现役活塞</td>
<td></td>
<td>179a40</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Richard Hamilton</td>
<td>？</td>
<td>SG</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Luol Deng</td>
<td>4 4</td>
<td>SF</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Carlos Boozer</td>
<td>6 6</td>
<td>PF</td>
<td>可用</td>
<td>西亚卡姆</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Joakim Noah</td>
<td>6 4</td>
<td>C</td>
<td>现役快船</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Kirk Hinrich</td>
<td>4 7</td>
<td>PG</td>
<td>用了之后公牛雪崩</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Taj Gibson</td>
<td>3 6</td>
<td>PF</td>
<td>现役尼克斯</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Marco Belinelli</td>
<td>5 9</td>
<td>SG</td>
<td>现役马刺</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Nazr Mohammed</td>
<td>4 8</td>
<td>C</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Nate Robinson</td>
<td>4 8</td>
<td>PG</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Val大佬给波神</td>
<td>/</td>
<td>PF</td>
<td>√</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jimmy Butler</td>
<td>5 6</td>
<td>SF</td>
<td>现役热火</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>骑士</td>
<td style="text-align:center">Kyrie Irving</td>
<td>/</td>
<td>PG</td>
<td>现役网</td>
<td></td>
<td>179ba0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Dion Waiters</td>
<td>4 7</td>
<td>SG</td>
<td>现役湖人</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">C.J. Miles</td>
<td>4 5</td>
<td>SF</td>
<td>暂可</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Tristan Thompson</td>
<td>7 8</td>
<td>PF</td>
<td>现役骑士</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Anderson Varejao</td>
<td>8 7</td>
<td>C</td>
<td>巴西联赛</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Omri Casspi</td>
<td>4 6</td>
<td>SF</td>
<td>暂可，掘金巴顿</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Tyler Zeller</td>
<td>5 6</td>
<td>C</td>
<td>现役灰熊</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Samardo Samuels</td>
<td>7 7</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Luke Harangody</td>
<td>4 9</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jeremy Pargo</td>
<td>6 5</td>
<td>PG</td>
<td>暂可</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>凯尔特人</td>
<td style="text-align:center">Rajon Rondo</td>
<td>/</td>
<td>PG</td>
<td>现役湖人</td>
<td></td>
<td>179cd0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Avery Bradley</td>
<td>/</td>
<td>SG</td>
<td>现役湖人</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Paul Pierce</td>
<td>4 6</td>
<td>SF</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">加内特名字有问题</td>
<td>10 7</td>
<td>PF</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Courtney Lee</td>
<td>/</td>
<td>/</td>
<td>现役独行侠</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jared Sullinger</td>
<td>5 9</td>
<td>PF</td>
<td>CBA</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Keyon Dooling</td>
<td>5 7</td>
<td>PG</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Chris Wilcox</td>
<td>5 6</td>
<td>PF</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Fab Melo</td>
<td>3 4</td>
<td>C</td>
<td>R.I.P.</td>
<td>来一个周琦</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>快船</td>
<td style="text-align:center">Chauncey Billups</td>
<td>8 7</td>
<td>PG</td>
<td>退役</td>
<td>给哈雷尔了，不能给拉塞尔了，换吧</td>
<td>179df0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Blake Griffin</td>
<td>/</td>
<td>PF</td>
<td>现役活塞</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">DeAndre Jordan</td>
<td>/</td>
<td>C</td>
<td>现役网</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Lamar Odom</td>
<td>5 4</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jamal Crawford</td>
<td>5 8</td>
<td>SG</td>
<td>暂可</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Grant Hill</td>
<td>5 4</td>
<td>SF</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Eric Bledsoe</td>
<td>/</td>
<td>PG</td>
<td>现役雄鹿</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Ryan Hollins</td>
<td>4 7</td>
<td>C</td>
<td>BIG3</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Ronny Turiaf</td>
<td>5 6</td>
<td>C</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Trey Thompkins</td>
<td>4 9</td>
<td>PF</td>
<td>西班牙联赛+新冠</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Travis Leslie</td>
<td>6 6</td>
<td>SG</td>
<td>可用</td>
<td>本队沙梅特</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>灰熊</td>
<td style="text-align:center">Rudy Gay</td>
<td>4 3</td>
<td>SF</td>
<td>现役马刺</td>
<td></td>
<td>179f40</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Zach Randolph</td>
<td>4 8</td>
<td>PF</td>
<td>BIG3</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Marc Casol</td>
<td>4 5</td>
<td>C</td>
<td>现役猛龙</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jerryd Bayless</td>
<td>6 7</td>
<td>PG</td>
<td>CBA</td>
<td>76福尔克马刺</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Darrell Arthur</td>
<td>7 6</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Quincy Pondexter</td>
<td>6 9</td>
<td>SF</td>
<td>可用</td>
<td>雄鹿2米德尔顿</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Marreese Speights</td>
<td>8 8</td>
<td>PF</td>
<td>BIG3</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Wayne Ellington</td>
<td>5 9</td>
<td>SG</td>
<td>现役尼克斯</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Josh Selby</td>
<td>4 5</td>
<td>PG</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Hamed Haddadi</td>
<td>5 7</td>
<td>C</td>
<td>CBA</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>鹰🦅</td>
<td style="text-align:center">Anthony Morrow</td>
<td>7 6</td>
<td>SG</td>
<td>BIG3</td>
<td>不能用，安东尼和甜瓜共用</td>
<td>17a080</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Kyle Korver</td>
<td>4 6</td>
<td>SG</td>
<td>现役雄鹿</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">AI Horford</td>
<td>/</td>
<td>C</td>
<td>现役76人</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Zaza Pachulia</td>
<td>4 8</td>
<td>C</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Ivan Johnson</td>
<td>4 7</td>
<td>PF</td>
<td>可用</td>
<td>🦅John Collins</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Johan Petro</td>
<td>5 5</td>
<td>C</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">DeShawn Stevenson</td>
<td>7 9</td>
<td>SG</td>
<td>可用</td>
<td>本队本部里</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>热火</td>
<td style="text-align:center">Mario Chalmers</td>
<td>5 8</td>
<td>PG</td>
<td>BIG3</td>
<td>范弗利特</td>
<td>17a180</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">韦德名字有问题</td>
<td>13 4</td>
<td>SG</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">LeBron James</td>
<td>6 5</td>
<td>SF</td>
<td>现役战神</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Shane Battier</td>
<td>5 7</td>
<td>SF</td>
<td>退役</td>
<td>太阳Mikal Bridges</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Udonis Haslem</td>
<td>6 6</td>
<td>PF</td>
<td>现役热火</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Rashard Lewis</td>
<td>7 5</td>
<td>SF</td>
<td>可用</td>
<td>热火德里克琼斯</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Norris Cole</td>
<td>6 4</td>
<td>PG</td>
<td>意大利联赛</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Dexter Pittman</td>
<td>6 7</td>
<td>C</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>鹈鹕</td>
<td style="text-align:center">Greivis Vasquez</td>
<td>7 7</td>
<td>PG</td>
<td>可用</td>
<td></td>
<td>17a290</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">AI-Farouq Aminu</td>
<td>9 5</td>
<td>SF</td>
<td>现役魔术</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Austin Rivers</td>
<td>6 6</td>
<td>PG</td>
<td>现役🚀</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Robin Lopez</td>
<td>5 5</td>
<td>C</td>
<td>现役雄鹿</td>
<td>雄鹿替补</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Xavier Henry</td>
<td>6 5</td>
<td>SG</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Hakim Warrick</td>
<td>5 7</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Roger Mason</td>
<td>5 5</td>
<td>SG</td>
<td>可用</td>
<td>太阳偶不累</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>爵士</td>
<td style="text-align:center">Randy Foye</td>
<td>5 4</td>
<td>SG</td>
<td>可用</td>
<td></td>
<td>17a360</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Earl Watson</td>
<td>4 6</td>
<td>PG</td>
<td>退役</td>
<td>湖人卡卢索</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Alec Burks</td>
<td>4 5</td>
<td>SG</td>
<td>现役76人</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Enes Kanter</td>
<td>4 6</td>
<td>C</td>
<td>现役绿军</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jamaal Tinsley</td>
<td>6 7</td>
<td>PG</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Raja Bell</td>
<td>4 4</td>
<td>SG</td>
<td>退役</td>
<td>哈特</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>国王</td>
<td style="text-align:center">Aaron Brooks</td>
<td>5 6</td>
<td>PG</td>
<td>NBL</td>
<td></td>
<td>17a450</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Marcus Thornton</td>
<td>6 8</td>
<td>SG</td>
<td>可用</td>
<td>热火邓肯罗宾逊</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">DeMarcus Cousins</td>
<td>/</td>
<td>C</td>
<td>暂可</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Chuck Hayes</td>
<td>5 5</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jimmer Fredette</td>
<td>6 8</td>
<td>PG</td>
<td>希腊联赛</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Francisco Garcia</td>
<td>9 6</td>
<td>SF</td>
<td>可用</td>
<td>🦅德安德烈亨特</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>尼克斯</td>
<td style="text-align:center">Raymond Felton</td>
<td>7 6</td>
<td>PG</td>
<td>暂可</td>
<td></td>
<td>17a560</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Ronnie Brewer</td>
<td>6 6</td>
<td>SG</td>
<td>可用</td>
<td>🐉Norman Powell</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Amar’s Stoudemire</td>
<td>6 10</td>
<td>PF</td>
<td>以色列联赛</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Tyson Chandler</td>
<td>5 8</td>
<td>C</td>
<td>现役火箭</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Iman Shumpert</td>
<td>4 8</td>
<td>SG</td>
<td>暂可，刚裁</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Steve Novak</td>
<td>5 5</td>
<td>SF</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>湖人</td>
<td style="text-align:center">Kobe Bryant R.I.P.</td>
<td>4 6</td>
<td>SG</td>
<td>R.I.P.</td>
<td></td>
<td>17a640</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">慈世平名字有问题</td>
<td>5 11</td>
<td>SF</td>
<td>BIG3</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Dwight Howard</td>
<td>6 6</td>
<td>C</td>
<td>现役湖人</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Antawn Jamison</td>
<td>6 7</td>
<td>PF</td>
<td>可用</td>
<td>给塔图姆</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jodie Meeks</td>
<td>5 5</td>
<td>SG</td>
<td>暂可</td>
<td>给库子马</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Christian Eyenga</td>
<td>9 6</td>
<td>SF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Darius Morris</td>
<td>6 6</td>
<td>PG</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>魔术</td>
<td style="text-align:center">Jammeer Nelson</td>
<td>6 6</td>
<td>PG</td>
<td>可用</td>
<td></td>
<td>17a760</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Arron Afflalo</td>
<td>5 7</td>
<td>SG</td>
<td>可用</td>
<td>🦅凯文叙尔特</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Hedo Turkoglu</td>
<td>4 8</td>
<td>SF</td>
<td>退役</td>
<td>🦅雷迪什</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Gustavo Ayon</td>
<td>7 4</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">J.J. Redick</td>
<td>4 6</td>
<td>SG</td>
<td>现役鹈鹕</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Nikola Vucevic</td>
<td>6 7</td>
<td>C</td>
<td>现役魔术</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Maurice Harkless</td>
<td>7 8</td>
<td>SF</td>
<td>现役尼克斯</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Justin Harper</td>
<td>6 6</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>小牛（现独行侠）</td>
<td style="text-align:center">老司机</td>
<td>/</td>
<td>PF</td>
<td>致敬</td>
<td></td>
<td>17a8b0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Darren Collison</td>
<td>6 8</td>
<td>PG</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">O.J. Mayo</td>
<td>4 4</td>
<td>SG</td>
<td>CBA</td>
<td>湖人波普</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Shawn Marion</td>
<td>5 6</td>
<td>SF</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Vince Carter</td>
<td>5 6</td>
<td>SF</td>
<td>现役🦅</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Elton Brand</td>
<td>5 5</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Delonte West</td>
<td>7 4</td>
<td>PG</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Rodrigue Beaubois</td>
<td>8 8</td>
<td>PG</td>
<td>土耳其联赛</td>
<td>🐺拉塞尔</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>网</td>
<td style="text-align:center">Kris Humphries</td>
<td>4 9</td>
<td>PF</td>
<td>退役</td>
<td></td>
<td>17a9d0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Keith Bogans</td>
<td>5 6</td>
<td>SG</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Donte Greene</td>
<td>5 6</td>
<td>SF</td>
<td>可用</td>
<td>狼奥克季</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jerry Stackhouse</td>
<td>5 10</td>
<td>SG</td>
<td>退役</td>
<td>开拓者小特伦特</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>掘金</td>
<td style="text-align:center">Ty Lawson</td>
<td>2 6</td>
<td>PG</td>
<td>CBA</td>
<td></td>
<td>17aa90</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">伊戈达拉名字有点问题</td>
<td>5 8</td>
<td>SF</td>
<td>现役热火</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Danilo Gallinari</td>
<td>6 9</td>
<td>SF</td>
<td>现役雷霆</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Kenneth Faried</td>
<td>7 6</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Timofey Mozgov</td>
<td>7 6</td>
<td>C</td>
<td>俄罗斯联赛</td>
<td>网allen</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">JaVale McGee</td>
<td>6 5</td>
<td>C</td>
<td>现役湖人</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Kosta Koufos</td>
<td>5 6</td>
<td>C</td>
<td>俄罗斯联赛</td>
<td>太阳贝恩斯</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>步行者</td>
<td style="text-align:center">Danny Granger</td>
<td>5 7</td>
<td>SF</td>
<td>退役</td>
<td></td>
<td>17ab80</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Roy Hibbert</td>
<td>3 7</td>
<td>C</td>
<td>退役</td>
<td>热火阿德巴约</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">D.J. Augustin</td>
<td>4 8</td>
<td>PG</td>
<td>现役魔术</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Ian Mahinmi</td>
<td>3 7</td>
<td>C</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Lance Stephenson</td>
<td>5 10</td>
<td>SG</td>
<td>CBA</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>活塞</td>
<td style="text-align:center">Rodney Stuckey</td>
<td>6 7</td>
<td>SG</td>
<td>可用</td>
<td>nb，🐺马利克</td>
<td>17ac60</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Tayshaun Prince</td>
<td>8 6</td>
<td>SF</td>
<td>可用</td>
<td>Dorian Finney-Smith</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Greg Monroe</td>
<td>4 6</td>
<td>C</td>
<td>德国联赛</td>
<td>火箭塔克</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jonas Jerebke</td>
<td>5 7</td>
<td>PF</td>
<td>俄罗斯联赛</td>
<td>开拓者zach科林斯</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Charlie Villanueva</td>
<td>7 10</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>猛龙</td>
<td style="text-align:center">DeMar DeRozan</td>
<td>5 7</td>
<td>SG</td>
<td>现役马刺</td>
<td></td>
<td>17ad50</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Landry Fields</td>
<td>6 6</td>
<td>SG</td>
<td>可用</td>
<td>76的milton</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Andrea Bargnani</td>
<td>6 8</td>
<td>PF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Linas Kleiza</td>
<td>5 6</td>
<td>SF</td>
<td>可用</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jose Calderon</td>
<td>4 8</td>
<td>PG</td>
<td>退役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Terrence Ross</td>
<td>8 4</td>
<td>SG</td>
<td>现役魔术</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>火箭🚀</td>
<td style="text-align:center">Patrick Patterson</td>
<td>7 9</td>
<td>PF</td>
<td></td>
<td></td>
<td>17ae50</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Omer Asik</td>
<td>4 4</td>
<td>C</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Royce White</td>
<td>5 5</td>
<td>PF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Toney Douglas</td>
<td>5 7</td>
<td>PG</td>
<td>掘金murray</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Donatas Motiejunas</td>
<td>7 10</td>
<td>PF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Gary Forbes</td>
<td>4 6</td>
<td>SF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jon Brockman</td>
<td>3 8</td>
<td>PF</td>
<td>热火Jae Crowder</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>马刺</td>
<td style="text-align:center">Kawhi Leonard</td>
<td>5 7</td>
<td>SF</td>
<td></td>
<td></td>
<td>17af70</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Boris Diaw</td>
<td>5 4</td>
<td>PF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Tim Duncan</td>
<td>3 6</td>
<td>C</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Manu Ginobili</td>
<td>4 8</td>
<td>SG</td>
<td>C.J. McCollum</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Stephen Jackson</td>
<td>7 7</td>
<td>SF</td>
<td>网Taurean Prince</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Tiago Splitter</td>
<td>5 8</td>
<td>PF</td>
<td>🐺spellman</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">DeJuan Blair</td>
<td>6 5</td>
<td>PF</td>
<td>掘金格兰特</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Cory Joseph</td>
<td>4 6</td>
<td>PG</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>太阳☀</td>
<td style="text-align:center">Goran Dragic</td>
<td>5 6</td>
<td>PG</td>
<td></td>
<td></td>
<td>17b070</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Luis Scola</td>
<td>4 5</td>
<td>PF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Marcin Gortat</td>
<td>6 6</td>
<td>C</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Channing Frye</td>
<td>8 4</td>
<td>PF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Sebastian Telfair</td>
<td>9 7</td>
<td>PG</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Kendall Marshall</td>
<td>7 8</td>
<td>PG</td>
<td>快船贝弗利</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jermaine O’Neal</td>
<td>8 6</td>
<td>C</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>雷霆</td>
<td style="text-align:center">Russell Westbrook</td>
<td>7 9</td>
<td>PG</td>
<td></td>
<td></td>
<td>17b190</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Thabo Sefolosha</td>
<td>5 9</td>
<td>SG</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Serge Ibaka</td>
<td>5 5</td>
<td>PF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Kendrick Perkins</td>
<td>8 7</td>
<td>C</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Daequan Cook</td>
<td>7 4</td>
<td>SG</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Hasheem Thabeet</td>
<td>7 7</td>
<td>C</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>森林狼</td>
<td style="text-align:center">Ricky Rubio</td>
<td>5 5</td>
<td>PG</td>
<td></td>
<td></td>
<td>17b2b0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Andrei Kirilenko</td>
<td>6 9</td>
<td>SF</td>
<td>火箭考文顿</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Chase Budinger</td>
<td>5 8</td>
<td>SF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>开拓者</td>
<td style="text-align:center">Damian Lillard</td>
<td>6 7</td>
<td>PG</td>
<td></td>
<td></td>
<td>17b360</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Nicolas Batum</td>
<td>7 5</td>
<td>SF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">LaMarcus Aldridge</td>
<td>8 8</td>
<td>PF</td>
<td>现役</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Dan Gadzuric</td>
<td>3 8</td>
<td>C</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>勇士</td>
<td style="text-align:center">Jarrett Jack</td>
<td>7 4</td>
<td>PG</td>
<td></td>
<td></td>
<td>17b460</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Andris Biedrins</td>
<td>6 8</td>
<td>C</td>
<td>太阳卡那啥斯基</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Festus Ezeli</td>
<td>6 5</td>
<td>C</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>奇才</td>
<td style="text-align:center">Trevor Ariza</td>
<td>6 5</td>
<td>SF</td>
<td></td>
<td></td>
<td>17b510</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">内内名字有问题</td>
<td>/</td>
<td>/</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Emeka Okafor</td>
<td>5 6</td>
<td>C</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Jan Vesely</td>
<td>3 6</td>
<td>SF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Martell Webster</td>
<td>7 7</td>
<td>SF</td>
<td>掘金小波特</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Shelvin Mack</td>
<td>7 4</td>
<td>PG</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h2 id="🚀修改"><a href="#🚀修改" class="headerlink" title="🚀修改"></a>🚀修改</h2><h3 id="James-Harden"><a href="#James-Harden" class="headerlink" title="James Harden"></a>James Harden</h3><div class="table-container">
<table>
<thead>
<tr>
<th>胡子</th>
<th>KM最新</th>
<th>97</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>/</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>2ac60</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  87 57</td>
<td>2 med  90 5a</td>
<td>3 3pt  84 54</td>
<td>4 ft  86 56</td>
<td>5 layup  98 62</td>
<td>6 dunk  84 54</td>
<td>28 stdnk   55 37</td>
<td><strong>23  sit</strong>  78 4e</td>
<td><strong>22  sod</strong> 90 5a</td>
<td>25 hus  73 49</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  97 61</td>
<td><strong>8  pass</strong>  82 52</td>
<td>40</td>
<td>32</td>
<td>9 opost  97 61</td>
<td>10 dpost  64 40</td>
<td>11 block   67 43</td>
<td>26 hnd  85 55</td>
<td>12 steal  59 3b</td>
<td>15 speed  91 5b</td>
<td>16 stam  94 5e</td>
<td>3c</td>
<td>21 vert  88 58</td>
<td>13 oreb  74 4a</td>
<td>14 dreb  85 55</td>
<td>17 dur  87 57</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  75 4b</td>
<td><strong>19  oawr</strong>  99 63</td>
<td>41</td>
<td>56</td>
<td>27 def  50 32(4d)</td>
<td>24 qui  87 57</td>
<td>潜力 55</td>
<td>20 str  74 4a</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Russell-Westbrook"><a href="#Russell-Westbrook" class="headerlink" title="Russell Westbrook"></a>Russell Westbrook</h3><div class="table-container">
<table>
<thead>
<tr>
<th>威少</th>
<th>KM最新</th>
<th>89</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>/</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>2a300</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  79 4f</td>
<td>2 med  78 4e</td>
<td>3 3pt  60 3c</td>
<td>4 ft  78 4e</td>
<td>5 layup  96 60</td>
<td>6 dunk  90 5a</td>
<td>28 stdnk  62 3e</td>
<td><strong>23  sit</strong>  78 4e</td>
<td><strong>22  sod</strong>  76 4c</td>
<td>25 hus  79 4f</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  91 5b</td>
<td><strong>8  pass</strong>  78 4e</td>
<td>3c</td>
<td>32</td>
<td>9 opost  64 40</td>
<td>10 dpost  74 4a</td>
<td>11 block   50 1f</td>
<td>26 hnd  93 5d</td>
<td>12 steal  63 3f</td>
<td>15 speed  98 62</td>
<td>16 stam  99 63</td>
<td>4b</td>
<td>21 vert  95 5f</td>
<td>13 oreb  61 3d</td>
<td>14 dreb   77 4d</td>
<td>17 dur  96 60</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  85 55</td>
<td><strong>19  oawr</strong>  98 62</td>
<td>55</td>
<td>5e</td>
<td>27 def  88 58</td>
<td>24 qui  96 60</td>
<td>潜力 5e</td>
<td>20 str  50 2e</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Robert-Covington"><a href="#Robert-Covington" class="headerlink" title="Robert Covington"></a>Robert Covington</h3><div class="table-container">
<table>
<thead>
<tr>
<th>考文顿</th>
<th>KM最新</th>
<th>80</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td>Andrei Kirilenko</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  94 5e</td>
<td>2 med  73 49</td>
<td>3 3pt  79 4f</td>
<td>4 ft  83 53</td>
<td>5 layup  68 44</td>
<td>6 dunk  64 40</td>
<td>28 stdnk  80 50</td>
<td><strong>23  sit</strong>  64 40</td>
<td><strong>22  sod</strong>  52 34</td>
<td>25 hus  77 4d</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  71 47</td>
<td><strong>8  pass</strong>  61 3d</td>
<td></td>
<td></td>
<td>9 opost  71 47</td>
<td>10 dpost  82 52</td>
<td>11 block   66 42</td>
<td>26 hnd  70 46</td>
<td>12 steal  83 53</td>
<td>15 speed  72 48</td>
<td>16 stam  81 51</td>
<td></td>
<td>21 vert  66 42</td>
<td>13 oreb  66 42</td>
<td>14 dreb  69 45</td>
<td>17 dur  63 3f</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  80 50</td>
<td><strong>19  oawr</strong>  73 49</td>
<td></td>
<td></td>
<td>27 def  80 50</td>
<td>24 qui  60 3c</td>
<td>潜力 92 5c</td>
<td>20 str  74 4a</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Eric-Gordon"><a href="#Eric-Gordon" class="headerlink" title="Eric Gordon"></a>Eric Gordon</h3><div class="table-container">
<table>
<thead>
<tr>
<th>圆脸</th>
<th>KM最新</th>
<th>79</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>/</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>14280</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  77 4d</td>
<td>2 med  67 43</td>
<td>3 3pt  80 50</td>
<td>4 ft  72 48</td>
<td>5 layup  82 52</td>
<td>6 dunk  70 46</td>
<td>28 stdnk  50 1e</td>
<td><strong>23  sit</strong>  70 46</td>
<td><strong>22  sod</strong>  80 50</td>
<td>25 hus  78 4e</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  79 4f</td>
<td><strong>8  pass</strong>  60 3c</td>
<td>32</td>
<td>32</td>
<td>9 opost  68 44</td>
<td>10 dpost  73 49</td>
<td>11 block   56 38</td>
<td>26 hnd  83 53</td>
<td>12 steal  58 3a</td>
<td>15 speed  86 56</td>
<td>16 stam  92 5c</td>
<td>3c</td>
<td>21 vert  94 5e</td>
<td>13 oreb  67 43</td>
<td>14 dreb  73 49</td>
<td>17 dur  75 4b</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  76 4c</td>
<td><strong>19  oawr</strong>  83 53</td>
<td>4b</td>
<td>5a</td>
<td>27 def  79 4f</td>
<td>24 qui  89 59</td>
<td>潜力 53</td>
<td>20 str  72 48</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="P-J-Tucker"><a href="#P-J-Tucker" class="headerlink" title="P.J. Tucker"></a>P.J. Tucker</h3><div class="table-container">
<table>
<thead>
<tr>
<th>塔克</th>
<th>KM最新</th>
<th>76</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17acc0</td>
<td>活塞Greg Monroe</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>23460</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  95 5f</td>
<td>2 med  74 4a</td>
<td>3 3pt  80 50</td>
<td>4 ft  78 4e</td>
<td>5 layup  60 3c</td>
<td>6 dunk  60 3c</td>
<td>28 stdnk  56 38</td>
<td><strong>23  sit</strong>  65 41</td>
<td><strong>22  sod</strong>  84 54</td>
<td>25 hus  66 42</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  65 41</td>
<td><strong>8  pass</strong>  70 46</td>
<td>32</td>
<td>32</td>
<td>9 opost  78 4e</td>
<td>10 dpost  78 4e</td>
<td>11 block   51 33</td>
<td>26 hnd  90 5a</td>
<td>12 steal  50 32</td>
<td>15 speed  65 41</td>
<td>16 stam  99 63</td>
<td>23</td>
<td>21 vert  75 4b</td>
<td>13 oreb  50 32</td>
<td>14 dreb  66 42</td>
<td>17 dur  55 37</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  80 50</td>
<td><strong>19  oawr</strong>  82 52</td>
<td>4b</td>
<td>50</td>
<td>27 def  69 45</td>
<td>24 qui  79 4f</td>
<td>潜力 55</td>
<td>20 str  57 39</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>面补序号0f 07=15+1792=1807，KM307的2536</p>
<p>塔克245lbs，身高6‘6’</p>
<p>502e4a2e 5475636b6572</p>
<h3 id="Austin-Rivers"><a href="#Austin-Rivers" class="headerlink" title="Austin Rivers"></a>Austin Rivers</h3><div class="table-container">
<table>
<thead>
<tr>
<th>里弗斯</th>
<th>KM最新</th>
<th>75</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>/</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>14a00</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  85 55</td>
<td>2 med  65 41</td>
<td>3 3pt  77 4d</td>
<td>4 ft  70 46</td>
<td>5 layup  80 50</td>
<td>6 dunk  73 49</td>
<td>28 stdnk  50 23</td>
<td><strong>23  sit</strong>  72 48</td>
<td><strong>22  sod</strong>  76 4c</td>
<td>25 hus  78 4e</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  86 56</td>
<td><strong>8  pass</strong>  79 4f</td>
<td>32</td>
<td>32</td>
<td>9 opost  77 4d</td>
<td>10 dpost  78 4e</td>
<td>11 block   50 1b</td>
<td>26 hnd  76 4c</td>
<td>12 steal  50 32</td>
<td>15 speed  79 4f</td>
<td>16 stam  93 5d</td>
<td>2d</td>
<td>21 vert  77 4d</td>
<td>13 oreb  62 3e</td>
<td>14 dreb  50 26</td>
<td>17 dur  80 50</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  77 4d</td>
<td><strong>19  oawr</strong>  79 4f</td>
<td>32</td>
<td>50</td>
<td>27 def  70 46</td>
<td>24 qui  93 5d</td>
<td>潜力 4a</td>
<td>20 str  50 28</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Jeff-Green"><a href="#Jeff-Green" class="headerlink" title="Jeff Green"></a>Jeff Green</h3><div class="table-container">
<table>
<thead>
<tr>
<th>格林</th>
<th>KM最新</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>/</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  93</td>
<td>2 med  72</td>
<td>3 3pt  78</td>
<td>4 ft  80</td>
<td>5 layup  82</td>
<td>6 dunk  85</td>
<td>28 stdnk  99 63</td>
<td><strong>23  sit</strong>  79 4f</td>
<td><strong>22  sod</strong>  95 5f</td>
<td>25 hus  69 45</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  71 47</td>
<td><strong>8  pass</strong>  67 43</td>
<td>32</td>
<td>3c</td>
<td>9 opost  80</td>
<td>10 dpost  85 55</td>
<td>11 block   83</td>
<td>26 hnd  83 53</td>
<td>12 steal  65 41</td>
<td>15 speed  70</td>
<td>16 stam  90 5a</td>
<td>19</td>
<td>21 vert  85 55</td>
<td>13 oreb  72</td>
<td>14 dreb  95</td>
<td>17 dur  68 44</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  78</td>
<td><strong>19  oawr</strong>  75</td>
<td>3c</td>
<td>31</td>
<td>27 def  84 54</td>
<td>24 qui  87 57</td>
<td>潜力 92 5c</td>
<td>20 str  88 58</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Tyson-Chandler"><a href="#Tyson-Chandler" class="headerlink" title="Tyson Chandler"></a>Tyson Chandler</h3><div class="table-container">
<table>
<thead>
<tr>
<th>钱德勒</th>
<th>KM最新</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>/</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  93</td>
<td>2 med  72</td>
<td>3 3pt  78</td>
<td>4 ft  80</td>
<td>5 layup  82</td>
<td>6 dunk  85</td>
<td>28 stdnk  99 63</td>
<td><strong>23  sit</strong>  79 4f</td>
<td><strong>22  sod</strong>  95 5f</td>
<td>25 hus  69 45</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  71 47</td>
<td><strong>8  pass</strong>  67 43</td>
<td>32</td>
<td>3c</td>
<td>9 opost  80</td>
<td>10 dpost  85 55</td>
<td>11 block   83</td>
<td>26 hnd  83 53</td>
<td>12 steal  65 41</td>
<td>15 speed  70</td>
<td>16 stam  90 5a</td>
<td>19</td>
<td>21 vert  85 55</td>
<td>13 oreb  72</td>
<td>14 dreb  95</td>
<td>17 dur  68 44</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  78</td>
<td><strong>19  oawr</strong>  75</td>
<td>3c</td>
<td>31</td>
<td>27 def  84 54</td>
<td>24 qui  87 57</td>
<td>潜力 92 5c</td>
<td>20 str  88 58</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Thabo-Sefolosha"><a href="#Thabo-Sefolosha" class="headerlink" title="Thabo Sefolosha"></a>Thabo Sefolosha</h3><div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>/</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  93</td>
<td>2 med  72</td>
<td>3 3pt  78</td>
<td>4 ft  80</td>
<td>5 layup  82</td>
<td>6 dunk  85</td>
<td>28 stdnk  99 63</td>
<td><strong>23  sit</strong>  79 4f</td>
<td><strong>22  sod</strong>  95 5f</td>
<td>25 hus  69 45</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  71 47</td>
<td><strong>8  pass</strong>  67 43</td>
<td>32</td>
<td>3c</td>
<td>9 opost  80</td>
<td>10 dpost  85 55</td>
<td>11 block   83</td>
<td>26 hnd  83 53</td>
<td>12 steal  65 41</td>
<td>15 speed  70</td>
<td>16 stam  90 5a</td>
<td>19</td>
<td>21 vert  85 55</td>
<td>13 oreb  72</td>
<td>14 dreb  95</td>
<td>17 dur  68 44</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  78</td>
<td><strong>19  oawr</strong>  75</td>
<td>3c</td>
<td>31</td>
<td>27 def  84 54</td>
<td>24 qui  87 57</td>
<td>潜力 92 5c</td>
<td>20 str  88 58</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h2 id="森林狼修改"><a href="#森林狼修改" class="headerlink" title="森林狼修改"></a>森林狼修改</h2><h3 id="Karl-Anthony-Towns"><a href="#Karl-Anthony-Towns" class="headerlink" title="Karl-Anthony Towns"></a>Karl-Anthony Towns</h3><div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>90</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>179620</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>05280</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close   91 5b</td>
<td>2 med  87 57</td>
<td>3 3pt 79 4f</td>
<td>4 ft 83 53</td>
<td>5 layup  87 57</td>
<td>6 dunk  85 55</td>
<td>28 stdnk  99 63</td>
<td><strong>23  sit</strong>  55 37</td>
<td><strong>22  sod</strong>  68 44</td>
<td>25 hus  60 3c</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  71 47</td>
<td><strong>8  pass</strong>  62 3e</td>
<td>32</td>
<td>40</td>
<td>9 opost  86 56</td>
<td>10 dpost  78 4e</td>
<td>11 block   72 48</td>
<td>26 hnd  72 48</td>
<td>12 steal  66 42</td>
<td>15 speed  80 50</td>
<td>16 stam  90 5a</td>
<td>3c</td>
<td>21 vert 72 48</td>
<td>13 oreb  76 4c</td>
<td>14 dreb  89 59</td>
<td>17 dur  78 4e</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  81 51</td>
<td><strong>19  oawr</strong>  82 52</td>
<td>50</td>
<td>5b</td>
<td>27 def 73 49</td>
<td>24 qui 90 5a</td>
<td>潜力 93 5d</td>
<td>20 str  79 4f</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>目前考虑用原76人的Jason改。</p>
<p>名字4b61726c2d416e74686f6e79 546f776e73；位置C/PF，04/03；身高6‘11’，211；体重248；32号-&gt;64-&gt;40；上场时间34min-&gt;136-&gt;88；面补用KM的，原面补0805原来整理好了；年龄24；原肤色00，太黑了，改成03。</p>
<p>注：身高6‘6=198.12，年龄31=da 7b</p>
<h3 id="D’Angelo-Russell"><a href="#D’Angelo-Russell" class="headerlink" title="D’Angelo Russell"></a>D’Angelo Russell</h3><div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>84</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>179e00</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>0df20</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  81 51</td>
<td>2 med  93 5d</td>
<td>3 3pt  82 52</td>
<td>4 ft 77 4d</td>
<td>5 layup  89 59</td>
<td>6 dunk  60 3c</td>
<td>28 stdnk  50 1e</td>
<td><strong>23  sit</strong> 58 3a</td>
<td><strong>22  sod</strong>  55 37</td>
<td>25 hus  81 51</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  86 56</td>
<td><strong>8  pass</strong>  82 52</td>
<td>49</td>
<td>32</td>
<td>9 opost  68 44</td>
<td>10 dpost  69 45</td>
<td>11 block   54 36</td>
<td>26 hnd  74 4a</td>
<td>12 steal  50 32</td>
<td>15 speed  83 53</td>
<td>16 stam  88 58</td>
<td>19</td>
<td>21 vert 70 46</td>
<td>13 oreb  70 46</td>
<td>14 dreb  67 43</td>
<td>17 dur  75 4b</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  74 4a</td>
<td><strong>19  oawr</strong>  84 54</td>
<td>55</td>
<td>5a</td>
<td>27 def 68 44</td>
<td>24 qui 60 3c</td>
<td>潜力 57</td>
<td>20 str 53 35</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<p>目前用原快船比卢普斯。</p>
<p>名字4427416e67656c6f 52757373656c6c；位置PG/SG，00/01；上场时间32min-&gt;128-&gt;80；原肤色03；原面补9d 01=157+256=0413，KM的用了即可；身高1.93；体重198.</p>
<p>ps：年龄36 = 8c 7b；身高6’3‘=190.5。</p>
<h2 id="20200823-湖人修改"><a href="#20200823-湖人修改" class="headerlink" title="20200823-湖人修改"></a>20200823-湖人修改</h2><h3 id="AD405"><a href="#AD405" class="headerlink" title="AD405"></a>AD405</h3><div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘10’=</td>
<td>220</td>
<td>8c 09</td>
<td>PF/C=03/04</td>
<td>03</td>
<td>原始时间81，2020季后赛36.3-145.2-91</td>
<td></td>
<td>3-6-06</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM405</th>
<th>96</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>14820</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  5a</td>
<td>2 med  48</td>
<td>3 3pt  4a</td>
<td>4 ft 56</td>
<td>5 layup  5e</td>
<td>6 dunk  56</td>
<td>28 stdnk  63</td>
<td><strong>23  sit</strong> 4a</td>
<td><strong>22  sod</strong>  52</td>
<td>25 hus  5f</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  47</td>
<td><strong>8  pass</strong>  3d</td>
<td>32</td>
<td>32</td>
<td>9 opost  55</td>
<td>10 dpost  52</td>
<td>11 block   59</td>
<td>26 hnd  4d</td>
<td>12 steal  40</td>
<td>15 speed  51</td>
<td>16 stam  63</td>
<td>37</td>
<td>21 vert 55</td>
<td>13 oreb  4b</td>
<td>14 dreb  51</td>
<td>17 dur  52</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  5f</td>
<td><strong>19  oawr</strong>  62</td>
<td>37</td>
<td>50</td>
<td>27 def 46</td>
<td>24 qui 44</td>
<td>潜力 96 60</td>
<td>20 str 4b</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="LBJ"><a href="#LBJ" class="headerlink" title="LBJ"></a>LBJ</h3><div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘9’=改个206试试</td>
<td>250</td>
<td>f5 03=1013</td>
<td>SF/PF=02/03</td>
<td>02</td>
<td>原始时间9a，2020季后赛34-136-88</td>
<td></td>
<td>23-46-2e</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>97</th>
<th></th>
<th>12c00</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 5a</td>
<td>2 med 50</td>
<td>3 3pt 4e</td>
<td>4 ft 45</td>
<td>5 layup 62</td>
<td>6 dunk 55</td>
<td>28 stdnk 4b</td>
<td>23 sit 55</td>
<td>22 sod 53</td>
<td>25 hus 52</td>
</tr>
<tr>
<td>7 hndl 56</td>
<td>8 pass 5a</td>
<td>5a</td>
<td>50</td>
<td>9 opost 5b</td>
<td>10 dpost 5a</td>
<td>11 block  3e</td>
<td>26 hnd 63</td>
<td>12 steal 35</td>
<td>15 speed 5b</td>
<td>16 stam 63</td>
<td>3c</td>
<td>21 vert 60</td>
<td>13 oreb 32</td>
<td>14 dreb 4e</td>
<td>17 dur 63</td>
</tr>
<tr>
<td>18 dawr 5a</td>
<td>19 oawr 5f</td>
<td>5a</td>
<td>63</td>
<td>27 def 53</td>
<td>24 qui 5a</td>
<td>潜力 63</td>
<td>20 str 58</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="霍华德"><a href="#霍华德" class="headerlink" title="霍华德"></a>霍华德</h3><div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘10’=</td>
<td>265</td>
<td>7e 04=1150</td>
<td>C=04/05</td>
<td>0a</td>
<td>2020季后赛17min=68=44</td>
<td></td>
<td>39-78-4e</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>81</th>
<th></th>
<th>1a400</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 4d</td>
<td>2 med 45</td>
<td>3 3pt 3c</td>
<td>4 ft 31</td>
<td>5 layup 4d</td>
<td>6 dunk 55</td>
<td>28 stdnk 63</td>
<td>23 sit 50</td>
<td>22 sod 34</td>
<td>25 hus 5a</td>
</tr>
<tr>
<td>7 hndl 28</td>
<td>8 pass 26</td>
<td>39</td>
<td>49</td>
<td>9 opost 4e</td>
<td>10 dpost 46</td>
<td>11 block  58</td>
<td>26 hnd 51</td>
<td>12 steal 32</td>
<td>15 speed 48</td>
<td>16 stam 5c</td>
<td>3c</td>
<td>21 vert 48</td>
<td>13 oreb 5b</td>
<td>14 dreb 59</td>
<td>17 dur 59</td>
</tr>
<tr>
<td>18 dawr 50</td>
<td>19 oawr 38</td>
<td>55</td>
<td>37</td>
<td>27 def 41</td>
<td>24 qui 3c</td>
<td>潜力 60</td>
<td>20 str 60</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="麦基"><a href="#麦基" class="headerlink" title="麦基"></a>麦基</h3><div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>7‘0’=</td>
<td>252</td>
<td>f4 05</td>
<td>C</td>
<td>04</td>
<td>2020季后赛13.3-53.2-35，原始时间72</td>
<td></td>
<td>7-14-0e</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>80</th>
<th></th>
<th>20760</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 4f</td>
<td>2 med 40</td>
<td>3 3pt 19</td>
<td>4 ft 42</td>
<td>5 layup 40</td>
<td>6 dunk 5a</td>
<td>28 stdnk 5f</td>
<td>23 sit 34</td>
<td>22 sod 1f</td>
<td>25 hus 53</td>
</tr>
<tr>
<td>7 hndl 23</td>
<td>8 pass 19</td>
<td>32</td>
<td>32</td>
<td>9 opost 46</td>
<td>10 dpost 48</td>
<td>11 block  60</td>
<td>26 hnd 25</td>
<td>12 steal 39</td>
<td>15 speed 43</td>
<td>16 stam 4e</td>
<td>63</td>
<td>21 vert 5d</td>
<td>13 oreb 55</td>
<td>14 dreb 55</td>
<td>17 dur 3f</td>
</tr>
<tr>
<td>18 dawr 40</td>
<td>19 oawr 50</td>
<td>37</td>
<td>41</td>
<td>27 def 46</td>
<td>24 qui 57</td>
<td>潜力4f</td>
<td>20 str 3a</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="rondo"><a href="#rondo" class="headerlink" title="rondo"></a>rondo</h3><div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘1’=</td>
<td>186</td>
<td>4a 05</td>
<td>PG</td>
<td>02</td>
<td>季后赛还没上。。瞎给个时间stat上的20.5-82-52，原始时间95</td>
<td></td>
<td>9-18-12</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>77</th>
<th></th>
<th>c4e0</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 4b</td>
<td>2 med 4c</td>
<td>3 3pt 4f</td>
<td>4 ft 4c</td>
<td>5 layup 49</td>
<td>6 dunk 3b</td>
<td>28 stdnk 39</td>
<td>23 sit 43</td>
<td>22 sod 32</td>
<td>25 hus 61</td>
</tr>
<tr>
<td>7 hndl 56</td>
<td>8 pass 52</td>
<td>39</td>
<td>35</td>
<td>9 opost 44</td>
<td>10 dpost 46</td>
<td>11 block  3d</td>
<td>26 hnd 60</td>
<td>12 steal 35</td>
<td>15 speed 54</td>
<td>16 stam 63</td>
<td>19</td>
<td>21 vert 55</td>
<td>13 oreb 49</td>
<td>14 dreb 4c</td>
<td>17 dur 42</td>
</tr>
<tr>
<td>18 dawr 48</td>
<td>19 oawr 4e</td>
<td>50</td>
<td>5f</td>
<td>27 def 58</td>
<td>60</td>
<td>潜力5c</td>
<td>20 str 25</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="维特斯"><a href="#维特斯" class="headerlink" title="维特斯"></a>维特斯</h3><p>这人先不改，暂时不想找新头，原来不是老队员</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘3’=</td>
<td>215</td>
<td></td>
<td>SG</td>
<td>0</td>
<td>-&gt;120-&gt;78</td>
<td></td>
<td>-20-14</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>76</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 52</td>
<td>2 med 48</td>
<td>3 3pt 51</td>
<td>4 ft 32</td>
<td>5 layup 50</td>
<td>6 dunk 4c</td>
<td>28 stdnk 33</td>
<td>23 sit 48</td>
<td>22 sod 4c</td>
<td>25 hus 4a</td>
</tr>
<tr>
<td>7 hndl 54</td>
<td>8 pass 4b</td>
<td>32</td>
<td></td>
<td>9 opost 48</td>
<td>10 dpost 44</td>
<td>11 block  35</td>
<td>26 hnd 50</td>
<td>12 steal 3c</td>
<td>49</td>
<td>16 stam 5f</td>
<td>19</td>
<td>21 vert 46</td>
<td>13 oreb 32</td>
<td>14 dreb 32</td>
<td>17 dur 55</td>
</tr>
<tr>
<td>18 dawr 44</td>
<td>19 oawr 53</td>
<td></td>
<td></td>
<td>27 def 47</td>
<td>24 qui 54</td>
<td>潜力</td>
<td>20 str 3c</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Danny-Green"><a href="#Danny-Green" class="headerlink" title="Danny Green"></a>Danny Green</h3><div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘6’=</td>
<td>210</td>
<td>74 06</td>
<td>SG/SF</td>
<td>04</td>
<td>2020季后赛25.7-&gt;102.8-&gt;67</td>
<td></td>
<td>14-28-1c</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>76</th>
<th></th>
<th>27600</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 55</td>
<td>2 med 4f</td>
<td>3 3pt 51</td>
<td>4 ft 4b</td>
<td>5 layup 49</td>
<td>6 dunk 44</td>
<td>28 stdnk 3d</td>
<td>23 sit 3f</td>
<td>22 sod 3f</td>
<td>25 hus 52</td>
</tr>
<tr>
<td>7 hndl 4b</td>
<td>8 pass 49</td>
<td>32</td>
<td>32</td>
<td>9 opost 3f</td>
<td>10 dpost 47</td>
<td>11 block  42</td>
<td>26 hnd 46</td>
<td>12 steal 49</td>
<td>15 speed 3d</td>
<td>16 stam 58</td>
<td>19</td>
<td>21 vert 48</td>
<td>13 oreb 3e</td>
<td>14 dreb 3a</td>
<td>17 dur 52</td>
</tr>
<tr>
<td>18 dawr 55</td>
<td>19 oawr 3f</td>
<td>23</td>
<td>3f</td>
<td>27 def 4f</td>
<td>24 qui 47</td>
<td>潜力49</td>
<td>20 str 3c</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="马基夫莫里斯"><a href="#马基夫莫里斯" class="headerlink" title="马基夫莫里斯"></a>马基夫莫里斯</h3><p>在太阳，原来的身高有问题</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘8’=205.8</td>
<td>245</td>
<td>5c 08</td>
<td>PF=03/05</td>
<td>03</td>
<td>2020季后赛17.7-&gt;70.8-&gt;47</td>
<td></td>
<td>88-176-b0</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>75</th>
<th></th>
<th>29d60</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 56</td>
<td>2 med 47</td>
<td>3 3pt 52</td>
<td>4 ft 4b</td>
<td>5 layup 43</td>
<td>6 dunk 43</td>
<td>28 stdnk 41</td>
<td>23 sit 44</td>
<td>22 sod 32</td>
<td>25 hus 46</td>
</tr>
<tr>
<td>7 hndl 24</td>
<td>8 pass 1f</td>
<td>32</td>
<td>32</td>
<td>9 opost 48</td>
<td>10 dpost 4e</td>
<td>11 block  32</td>
<td>26 hnd 31</td>
<td>12 steal 32</td>
<td>15 speed 39</td>
<td>16 stam 4c</td>
<td>19</td>
<td>21 vert 34</td>
<td>13 oreb 32</td>
<td>14 dreb 4a</td>
<td>17 dur 54</td>
</tr>
<tr>
<td>18 dawr 4e</td>
<td>19 oawr 4f</td>
<td>2d</td>
<td>50</td>
<td>27 def 37</td>
<td>24 qui 33</td>
<td>潜力46</td>
<td>20 str 47</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="JR"><a href="#JR" class="headerlink" title="JR"></a>JR</h3><p>用KM405free球员的数据</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘6’</td>
<td>220</td>
<td>8f 04</td>
<td>SG/SF=01/02</td>
<td>02</td>
<td>2020季后赛15-&gt;60-&gt;3c</td>
<td></td>
<td>21-42-2a</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>75</th>
<th></th>
<th>19140</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 32</td>
<td>2 med 3f</td>
<td>3 3pt 54</td>
<td>4 ft 46</td>
<td>5 layup 4b</td>
<td>6 dunk 46</td>
<td>28 stdnk 52</td>
<td>23 sit 41</td>
<td>22 sod 4f</td>
<td>25 hus 3a</td>
</tr>
<tr>
<td>7 hndl 4a</td>
<td>8 pass 39</td>
<td>32</td>
<td>32</td>
<td>9 opost 55</td>
<td>10 dpost 46</td>
<td>11 block  46</td>
<td>26 hnd 44</td>
<td>12 steal 3f</td>
<td>15 speed 4d</td>
<td>16 stam 5d</td>
<td>63</td>
<td>21 vert 5e</td>
<td>13 oreb 21</td>
<td>14 dreb 32</td>
<td>17 dur 3f</td>
</tr>
<tr>
<td>18 dawr 46</td>
<td>19 oawr 58</td>
<td>2d</td>
<td>59</td>
<td>27 def 44</td>
<td>24 qui 5c</td>
<td>潜力4f</td>
<td>20 str 2e</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Alex-Caruso"><a href="#Alex-Caruso" class="headerlink" title="Alex Caruso"></a>Alex Caruso</h3><p>用谁改？查询了ESPN，发现湖人SG过剩，卡卢索当PG打吧</p>
<p>爵士的Earl 4561726c Watson，416c6578 43617275736f。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘5’=195.58</td>
<td>185</td>
<td>4d 03=0845</td>
<td>PG/SG=00/01</td>
<td>05</td>
<td>2020季后赛24.3-&gt;97.2-&gt;61</td>
<td></td>
<td>4-8-08</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>75</th>
<th>17a3d0</th>
<th>16260</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 32</td>
<td>2 med 3c</td>
<td>3 3pt 4f</td>
<td>4 ft 4a</td>
<td>5 layup 4e</td>
<td>6 dunk 50</td>
<td>28 stdnk 3c</td>
<td>23 sit 41</td>
<td>22 sod 4d</td>
<td>25 hus 3a</td>
</tr>
<tr>
<td>7 hndl 4c</td>
<td>8 pass 4d</td>
<td>32</td>
<td>32</td>
<td>9 opost 3c</td>
<td>10 dpost 38</td>
<td>11 block  41</td>
<td>26 hnd 4d</td>
<td>12 steal 53</td>
<td>15 speed 52</td>
<td>16 stam 55</td>
<td>63</td>
<td>21 vert 45</td>
<td>13 oreb 34</td>
<td>14 dreb 39</td>
<td>17 dur 46</td>
</tr>
<tr>
<td>18 dawr 4f</td>
<td>19 oawr 4b</td>
<td>3c</td>
<td>53</td>
<td>27 def 32</td>
<td>24 qui 32</td>
<td>潜力46</td>
<td>20 str 3c</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Kentavious-Caldwell-Pope"><a href="#Kentavious-Caldwell-Pope" class="headerlink" title="Kentavious Caldwell-Pope"></a>Kentavious Caldwell-Pope</h3><p>名字太长抱歉，K.C. Pope</p>
<p>查询了ESPN，发现湖人SG过剩，波普也会客串PG打</p>
<p>小牛的O.J. 4f2e4a2e Mayo，4b2e432e 506f7065.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘5’=195.58</td>
<td>205</td>
<td>e5 05=1509</td>
<td>PG/SG</td>
<td>02</td>
<td>2020季后赛26.3-&gt;105.2-&gt;69</td>
<td></td>
<td>1-2-02</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>75</th>
<th>17a8d0</th>
<th>1d4c0</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 52</td>
<td>2 med 53</td>
<td>3 3pt 55</td>
<td>4 ft 4f</td>
<td>5 layup 49</td>
<td>6 dunk 49</td>
<td>28 stdnk 23</td>
<td>23 sit 38</td>
<td>22 sod 4a</td>
<td>25 hus 3a</td>
</tr>
<tr>
<td>7 hndl 4b</td>
<td>8 pass 4b</td>
<td>32</td>
<td>32</td>
<td>9 opost 43</td>
<td>10 dpost 46</td>
<td>11 block  39</td>
<td>26 hnd 46</td>
<td>12 steal 32</td>
<td>15 speed 57</td>
<td>16 stam 55</td>
<td>3c</td>
<td>21 vert 4e</td>
<td>13 oreb 1f</td>
<td>14 dreb 26</td>
<td>17 dur 46</td>
</tr>
<tr>
<td>18 dawr 46</td>
<td>19 oawr 46</td>
<td>2d</td>
<td>60</td>
<td>27 def 53</td>
<td>24 qui 57</td>
<td>潜力5a</td>
<td>20 str 3c</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Kuzma"><a href="#Kuzma" class="headerlink" title="Kuzma"></a>Kuzma</h3><p>本队Jodie Meeks. 4b796c65 4b757a6d61.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>身高</th>
<th>体重</th>
<th>面补</th>
<th>位置</th>
<th>肤色</th>
<th>上场时间</th>
<th>年龄</th>
<th>号码</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6‘9’=206</td>
<td>223</td>
<td>71 06=1649</td>
<td>PF</td>
<td>04</td>
<td>2020季后赛24.7-&gt;98.8-&gt;63</td>
<td>24=4b 7c</td>
<td>0-0-00</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>KM405</th>
<th>76</th>
<th>17a6e0</th>
<th>1ab80</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close 50</td>
<td>2 med 49</td>
<td>3 3pt 4a</td>
<td>4 ft 48</td>
<td>5 layup 50</td>
<td>6 dunk 50</td>
<td>28 stdnk 53</td>
<td>23 sit 4c</td>
<td>22 sod 4b</td>
<td>25 hus 5b</td>
</tr>
<tr>
<td>7 hndl 47</td>
<td>8 pass 40</td>
<td>32</td>
<td>32</td>
<td>9 opost 4e</td>
<td>10 dpost 4a</td>
<td>11 block  3b</td>
<td>26 hnd 46</td>
<td>12 steal 3a</td>
<td>15 speed 4a</td>
<td>16 stam 5a</td>
<td>19</td>
<td>21 vert 3e</td>
<td>13 oreb 1d</td>
<td>14 dreb 3f</td>
<td>17 dur 4e</td>
</tr>
<tr>
<td>18 dawr 4a</td>
<td>19 oawr 4d</td>
<td>2d</td>
<td>4c</td>
<td>27 def 3b</td>
<td>24 qui 3a</td>
<td>潜力3e</td>
<td>20 str 49</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h2 id="20200806-鹈鹕修改"><a href="#20200806-鹈鹕修改" class="headerlink" title="20200806-鹈鹕修改"></a>20200806-鹈鹕修改</h2><h3 id="Brandon-Ingram"><a href="#Brandon-Ingram" class="headerlink" title="Brandon Ingram"></a>Brandon Ingram</h3><p>高6’10‘=208.28，体重196；?岁；号码14-&gt;28-&gt;1c；肤色04；位置SF/SG=02/01；原头24 07=1828；时间设置为34min-&gt;136-88；</p>
<p>这个人是谁。。。好像是鹈鹕的Vasquez，新名字4272616e646f6e 496e6772616d</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>86</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17a290</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>140a0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  85 55</td>
<td>2 med  79 4f</td>
<td>3 3pt  84 54</td>
<td>4 ft 82 52</td>
<td>5 layup  74 4a</td>
<td>6 dunk  75 4b</td>
<td>28 stdnk  96 60</td>
<td><strong>23  sit</strong> 83 53</td>
<td><strong>22  sod</strong>  87 57</td>
<td>25 hus  79 4f</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  71 47</td>
<td><strong>8  pass</strong>  75 4b</td>
<td>40</td>
<td>32</td>
<td>9 opost  79 4f</td>
<td>10 dpost  76 4c</td>
<td>11 block   64 40</td>
<td>26 hnd  90 5a</td>
<td>12 steal  58 3a</td>
<td>15 speed  78 4e</td>
<td>16 stam  99 63</td>
<td>63</td>
<td>21 vert 86 56</td>
<td>13 oreb  64 40</td>
<td>14 dreb  68 44</td>
<td>17 dur  90 5a</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  59 3b</td>
<td><strong>19  oawr</strong>  98 62</td>
<td>3c</td>
<td>50</td>
<td>27 def 50 32</td>
<td>24 qui 74 4a</td>
<td>潜力 84 54</td>
<td>20 str 67 43</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps:6’6’=198.12，25岁=38 7c</p>
<h3 id="Zion-Williamson"><a href="#Zion-Williamson" class="headerlink" title="Zion Williamson"></a>Zion Williamson</h3><p>高6’6‘=198.12，体重285；?岁；号码1-&gt;2-&gt;02；肤色01；位置PF/SF；原头38 06=1592；时间设置为28min-&gt;112-70；</p>
<p>用自由人的Pops改，新名字5a696f6e 57696c6c69616d736f6e</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>86</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>181ec0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>7a940</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  75 4b</td>
<td>2 med  62 3e</td>
<td>3 3pt  75 4b</td>
<td>4 ft 64 40</td>
<td>5 layup  90 5a</td>
<td>6 dunk  97 61</td>
<td>28 stdnk  95 5f</td>
<td><strong>23  sit</strong> 74 4a</td>
<td><strong>22  sod</strong>  79 4f</td>
<td>25 hus  58 3a</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  71 47</td>
<td><strong>8  pass</strong>  65 41</td>
<td>40</td>
<td>32</td>
<td>9 opost  95 5f</td>
<td>10 dpost  78 4e</td>
<td>11 block   60 3c</td>
<td>26 hnd  84 54</td>
<td>12 steal  51 33</td>
<td>15 speed  88 58</td>
<td>16 stam  99 63</td>
<td>63</td>
<td>21 vert 99 63</td>
<td>13 oreb  88 58</td>
<td>14 dreb  68 44</td>
<td>17 dur  75 4b</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  90 5a</td>
<td><strong>19  oawr</strong>  95 5f</td>
<td>19</td>
<td>49</td>
<td>27 def 80 50</td>
<td>24 qui 95 5f</td>
<td>潜力 96 60</td>
<td>20 str 90 5a</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps：29岁=f3 7b，6‘8’=205.74</p>
<h3 id="Jrue-Holiday"><a href="#Jrue-Holiday" class="headerlink" title="Jrue Holiday"></a>Jrue Holiday</h3><p>高6’3‘=193.04，体重180；?岁；号码11-&gt;22-&gt;16；肤色00??；位置PG=00/05；原头；时间设置为34.8min-&gt;139-8b；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>83</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>50a0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  78 4e</td>
<td>2 med  60 3c</td>
<td>3 3pt  78 4e</td>
<td>4 ft 80 50</td>
<td>5 layup  71 47</td>
<td>6 dunk  50 28</td>
<td>28 stdnk  50 1e</td>
<td><strong>23  sit</strong> 54 36</td>
<td><strong>22  sod</strong>  68 44</td>
<td>25 hus  82 52</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  86 56</td>
<td><strong>8  pass</strong>  75 4b</td>
<td>32</td>
<td>32</td>
<td>9 opost  65 41</td>
<td>10 dpost  50 22</td>
<td>11 block   50 26</td>
<td>26 hnd  86 56</td>
<td>12 steal  70 46</td>
<td>15 speed  81 51</td>
<td>16 stam  96 60</td>
<td>19</td>
<td>21 vert 61 3d</td>
<td>13 oreb  50 26</td>
<td>14 dreb  50 2f</td>
<td>17 dur  74 4a</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  90 5a</td>
<td><strong>19  oawr</strong>  90 5a</td>
<td>50</td>
<td>52</td>
<td>27 def 80 50</td>
<td>24 qui 89 59</td>
<td>潜力 56</td>
<td>20 str 50 31</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps:22岁=66 7c，6‘3’=193.04</p>
<h3 id="JJ-Redick"><a href="#JJ-Redick" class="headerlink" title="JJ Redick"></a>JJ Redick</h3><p>高6’3‘=193.04，体重190；?岁；号码4-&gt;08-&gt;08；肤色05；位置SG；原头；时间设置为26.3min-&gt;105-69；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>80</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>1c3e0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  84 54</td>
<td>2 med  87 57</td>
<td>3 3pt  85 55</td>
<td>4 ft 90 5a</td>
<td>5 layup  74 4a</td>
<td>6 dunk  52 34</td>
<td>28 stdnk  51 33</td>
<td>23  sit 66 42</td>
<td>22 sod 76 4c</td>
<td>25 hus  68 44</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  75 4b</td>
<td><strong>8  pass</strong>  72 48</td>
<td>3c</td>
<td>32</td>
<td>9 opost  57 39</td>
<td>10 dpost  64 40</td>
<td>11 block   68 44</td>
<td>26 hnd  82 52</td>
<td>12 steal  64 40</td>
<td>15 speed  72 48</td>
<td>16 stam  86 56</td>
<td>19</td>
<td>21 vert 60 3c</td>
<td>13 oreb  50 19</td>
<td>14 dreb  50 28</td>
<td>17 dur  87 57</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  74 4a</td>
<td><strong>19  oawr</strong>  78 4e</td>
<td>3c</td>
<td>52</td>
<td>27 def 65 41</td>
<td>24 qui 77 4d</td>
<td>潜力 50</td>
<td>20 str 50 1b</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps:28岁=0c 7c</p>
<h3 id="Lonzo-Ball"><a href="#Lonzo-Ball" class="headerlink" title="Lonzo Ball"></a>Lonzo Ball</h3><p>高6’6’=198.12，体重190；?岁；号码2-&gt;4-&gt;04；肤色04；位置PG/SG；原头e6 04=1254；时间设置为32.2min-&gt;129-81；</p>
<p>76人的Royal Ivey，新名字4c6f6e7a6f 42616c6c</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>79</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>1796d0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>05fa0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  62 3e</td>
<td>2 med  55 37</td>
<td>3 3pt  79 4f</td>
<td>4 ft 54 36</td>
<td>5 layup  74 4a</td>
<td>6 dunk  70 46</td>
<td>28 stdnk  80 50</td>
<td>23  sit 88 58</td>
<td>22 sod 62 3e</td>
<td>25 hus  84 54</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  86 56</td>
<td><strong>8  pass</strong>  89 59</td>
<td>32</td>
<td>32</td>
<td>9 opost  66 42</td>
<td>10 dpost  71 47</td>
<td>11 block   61 3d</td>
<td>26 hnd  85 55</td>
<td>12 steal  56 38</td>
<td>15 speed  87 57</td>
<td>16 stam  99 63</td>
<td>19</td>
<td>21 vert 54 36</td>
<td>13 oreb  65 41</td>
<td>14 dreb  79 4f</td>
<td>17 dur  80 50</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  86 56</td>
<td><strong>19  oawr</strong>  79 4f</td>
<td>3c</td>
<td>41</td>
<td>27 def 66 42</td>
<td>24 qui 83 53</td>
<td>潜力 86 56</td>
<td>20 str 60 3c</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps：31岁=da 7b</p>
<h3 id="Josh-Hart"><a href="#Josh-Hart" class="headerlink" title="Josh Hart"></a>Josh Hart</h3><p>高6’5’=195.58，体重215；?岁；号码3-&gt;06-&gt;06；肤色03；位置SG/SF；原头3c 03=0828；时间设置为27.2min-&gt;109-6d；</p>
<p>jazz的Raja Bell，4a6f7368 48617274</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>75</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17a440</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>16bc0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  88 58</td>
<td>2 med  64 40</td>
<td>3 3pt  79 4f</td>
<td>4 ft 74 4a</td>
<td>5 layup  85 55</td>
<td>6 dunk  64 40</td>
<td>28 stdnk  58 3a</td>
<td>23  sit 57 39</td>
<td>22 sod 66 42</td>
<td>25 hus  76 4c</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  77 4d</td>
<td><strong>8  pass</strong>  73 49</td>
<td>32</td>
<td>32</td>
<td>9 opost  64 40</td>
<td>10 dpost  64 40</td>
<td>11 block   51 33</td>
<td>26 hnd  63 3f</td>
<td>12 steal  53 35</td>
<td>15 speed  83 53</td>
<td>16 stam  85 55</td>
<td>63</td>
<td>21 vert 82 52</td>
<td>13 oreb  80 50</td>
<td>14 dreb  89 59</td>
<td>17 dur  85 55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  65 41</td>
<td><strong>19  oawr</strong>  70 46</td>
<td>3c</td>
<td>3b</td>
<td>27 def 79 4f</td>
<td>24 qui 68 44</td>
<td>潜力 79 4f</td>
<td>20 str 50 31</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps：36岁=89 7b</p>
<h3 id="Derrick-Favors"><a href="#Derrick-Favors" class="headerlink" title="Derrick Favors"></a>Derrick Favors</h3><p>高6’10’=208.28，体重248；?岁；号码22-&gt;44-&gt;2c；肤色02；位置PF/C；原头；时间设置为24.3min-&gt;97-61；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>79</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>15ea0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  90</td>
<td>2 med  79</td>
<td>3 3pt  52</td>
<td>4 ft 59</td>
<td>5 layup  83</td>
<td>6 dunk  80</td>
<td>28 stdnk  95</td>
<td>23  sit 64</td>
<td>22 sod 50</td>
<td>25 hus  64</td>
</tr>
<tr>
<td><strong>7 hndl</strong>  50 24</td>
<td><strong>8  pass</strong>  50 28</td>
<td>32</td>
<td>32</td>
<td>9 opost  79</td>
<td>10 dpost  84</td>
<td>11 block   75</td>
<td>26 hnd  66</td>
<td>12 steal  50 32</td>
<td>15 speed  67</td>
<td>16 stam  93</td>
<td>32</td>
<td>21 vert 80</td>
<td>13 oreb  89</td>
<td>14 dreb  91</td>
<td>17 dur  82</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  65 41</td>
<td><strong>19  oawr</strong>  50 32</td>
<td>3c</td>
<td>50</td>
<td>27 def 54</td>
<td>24 qui 82</td>
<td>潜力 81 51</td>
<td>20 str 72</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps:21岁=77 7c，</p>
<h3 id="E’Twaun-Moore"><a href="#E’Twaun-Moore" class="headerlink" title="E’Twaun Moore"></a>E’Twaun Moore</h3><p>高6’3’=193.04，体重191；?岁；号码55-&gt;110-&gt;6e；肤色01；位置SG；原头；时间设置为18.4min-&gt;74-4a；</p>
<p>在自由人。。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>75</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>7aee0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  79</td>
<td>2 med  79</td>
<td>3 3pt  78</td>
<td>4 ft 82</td>
<td>5 layup  70</td>
<td>6 dunk  60</td>
<td>28 stdnk  55</td>
<td>23  sit 62</td>
<td>22 sod 65</td>
<td>25 hus  66</td>
</tr>
<tr>
<td>7 hndl  77</td>
<td>8  pass 66</td>
<td>32</td>
<td>32</td>
<td>9 opost  69</td>
<td>10 dpost  64</td>
<td>11 block   50 21</td>
<td>26 hnd  57</td>
<td>12 steal  60</td>
<td>15 speed  86</td>
<td>16 stam  83</td>
<td>19</td>
<td>21 vert 52</td>
<td>13 oreb  51</td>
<td>14 dreb  56</td>
<td>17 dur  85</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  69</td>
<td><strong>19  oawr</strong>  74</td>
<td>28</td>
<td>50</td>
<td>27 def 55</td>
<td>24 qui 51</td>
<td>潜力 75</td>
<td>20 str 69</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps：23岁=5c 7c</p>
<h3 id="在这里的一个测试"><a href="#在这里的一个测试" class="headerlink" title="在这里的一个测试"></a>在这里的一个测试</h3><p>上述阵容测试了三把，打弱队无敌，打强队拼到最后会差一线，打一般的球队也很累。</p>
<p>不过一般都是第一节被对面三分两分射崩，胖虎扣篮被疯狂盖帽；后面几节心累追分。</p>
<p>感觉问题出在没有好C上，这样的阵容，胖虎打C很怪，感觉没有发挥锋线的优势。</p>
<h2 id="20200807-魔术"><a href="#20200807-魔术" class="headerlink" title="20200807-魔术"></a>20200807-魔术</h2><p>前面几个只更新数据</p>
<h3 id="Nikola-Vucevic"><a href="#Nikola-Vucevic" class="headerlink" title="Nikola Vucevic"></a>Nikola Vucevic</h3><p>高7’0’=，体重260；?岁；号码9-&gt;18-&gt;12；肤色05；位置C；原头；时间设置为32.4min-&gt;130-82；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>85</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>1c7a0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  58</td>
<td>2 med  48</td>
<td>3 3pt  4e</td>
<td>4 ft 4e</td>
<td>5 layup  4e</td>
<td>6 dunk  3c</td>
<td>28 stdnk  50</td>
<td>23  sit 42</td>
<td>22 sod 3a</td>
<td>25 hus  38</td>
</tr>
<tr>
<td>7 hndl  33</td>
<td>8  pass 35</td>
<td>32</td>
<td>40</td>
<td>9 opost  5a</td>
<td>10 dpost  4a</td>
<td>11 block   43</td>
<td>26 hnd  45</td>
<td>12 steal  22</td>
<td>15 speed  40</td>
<td>16 stam  51</td>
<td>19</td>
<td>21 vert 49</td>
<td>13 oreb  4c</td>
<td>14 dreb  5a</td>
<td>17 dur  55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  49</td>
<td><strong>19  oawr</strong>  60</td>
<td>23</td>
<td>50</td>
<td>27 def 3e</td>
<td>24 qui 3f</td>
<td>潜力 56</td>
<td>20 str 39</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps：22岁=6c 7c,7’0’=213.36</p>
<h3 id="Evan-Fournier"><a href="#Evan-Fournier" class="headerlink" title="Evan Fournier"></a>Evan Fournier</h3><p>高6’7’=201，体重200；?岁；号码10-&gt;20-&gt;14；肤色05；位置SG/SF自带的；原头；时间设置为31.4min-&gt;126-7e；</p>
<p>在当年的掘金</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>81</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>210c0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  58</td>
<td>2 med  58</td>
<td>3 3pt  54</td>
<td>4 ft 51</td>
<td>5 layup  50</td>
<td>6 dunk  3f</td>
<td>28 stdnk  23</td>
<td>23  sit 38</td>
<td>22 sod 48</td>
<td>25 hus  43</td>
</tr>
<tr>
<td>7 hndl  4c</td>
<td>8  pass 4b</td>
<td>32</td>
<td>32</td>
<td>9 opost  4c</td>
<td>10 dpost  4b</td>
<td>11 block   39</td>
<td>26 hnd  48</td>
<td>12 steal  42</td>
<td>15 speed  4a</td>
<td>16 stam  55</td>
<td>41</td>
<td>21 vert 40</td>
<td>13 oreb  45</td>
<td>14 dreb  46</td>
<td>17 dur  43</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  41</td>
<td><strong>19  oawr</strong>  52</td>
<td>28</td>
<td>50</td>
<td>27 def 3c</td>
<td>24 qui 49</td>
<td>潜力 4f</td>
<td>20 str 23</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps：20岁=8e 7c，6‘6’=198.12</p>
<h3 id="Terrence-Ross"><a href="#Terrence-Ross" class="headerlink" title="Terrence Ross"></a>Terrence Ross</h3><p>高6’6’=，体重195；?岁；号码31-&gt;62-&gt;3e；肤色02；位置SG/SF自带的；原头；时间设置为27.3min-&gt;109-6d；</p>
<p>在当年的猛龙</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>78</th>
<th>（77）</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>25260</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  55</td>
<td>2 med  54</td>
<td>3 3pt  50</td>
<td>4 ft 55</td>
<td>5 layup  4f</td>
<td>6 dunk  58</td>
<td>28 stdnk  4c</td>
<td>23  sit 4c</td>
<td>22 sod 4c</td>
<td>25 hus  4d</td>
</tr>
<tr>
<td>7 hndl  4e</td>
<td>8  pass 41</td>
<td>32</td>
<td>32</td>
<td>9 opost  41</td>
<td>10 dpost  34</td>
<td>11 block   43</td>
<td>26 hnd  42</td>
<td>12 steal  46</td>
<td>15 speed  48</td>
<td>16 stam  5d</td>
<td>2d</td>
<td>21 vert 61</td>
<td>13 oreb  37</td>
<td>14 dreb  38</td>
<td>17 dur  50</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  3e</td>
<td><strong>19  oawr</strong>  50</td>
<td>32</td>
<td>50</td>
<td>27 def 42</td>
<td>24 qui 50</td>
<td>潜力 4f</td>
<td>20 str 30</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps：21岁=72 7c</p>
<h3 id="Al-Farouq-Aminu"><a href="#Al-Farouq-Aminu" class="headerlink" title="Al-Farouq Aminu"></a>Al-Farouq Aminu</h3><p>高6’8’=，体重215；?岁；号码2-&gt;4&gt;04；肤色01；位置PF/SF；原头；时间设置21.1min-&gt;84-54；</p>
<p>在当年的水鸟</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>75</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>14460</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  3c</td>
<td>2 med  34</td>
<td>3 3pt  46</td>
<td>4 ft 42</td>
<td>5 layup  48</td>
<td>6 dunk  46</td>
<td>28 stdnk  3c</td>
<td>23  sit 41</td>
<td>22 sod 3c</td>
<td>25 hus  52</td>
</tr>
<tr>
<td>7 hndl  3c</td>
<td>8  pass 41</td>
<td>32</td>
<td>32</td>
<td>9 opost  53</td>
<td>10 dpost  50</td>
<td>11 block   3c</td>
<td>26 hnd  3f</td>
<td>12 steal  46</td>
<td>15 speed  3c</td>
<td>16 stam  55</td>
<td>19</td>
<td>21 vert 50</td>
<td>13 oreb  40</td>
<td>14 dreb  48</td>
<td>17 dur  4b</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  58</td>
<td><strong>19  oawr</strong>  51</td>
<td>28</td>
<td>50</td>
<td>27 def 4b</td>
<td>24 qui 53</td>
<td>潜力 4d</td>
<td>20 str 4b</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps：22岁=6a 7c</p>
<h3 id="D-J-Augustin"><a href="#D-J-Augustin" class="headerlink" title="D.J. Augustin"></a>D.J. Augustin</h3><p>高用自己的6‘0’不改，体重180；?岁；号码14-&gt;28-&gt;1c；肤色04；位置PG/SG自带的；原头；时间设置为24.8min-&gt;100-64；</p>
<p>在当年的步行者</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>77</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>221a0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  3f</td>
<td>2 med  57</td>
<td>3 3pt  4b</td>
<td>4 ft 58</td>
<td>5 layup  4f</td>
<td>6 dunk  24</td>
<td>28 stdnk  19</td>
<td>23  sit 3d</td>
<td>22 sod 4c</td>
<td>25 hus  49</td>
</tr>
<tr>
<td>7 hndl  51</td>
<td>8  pass 4f</td>
<td>32</td>
<td>32</td>
<td>9 opost  44</td>
<td>10 dpost  46</td>
<td>11 block   37</td>
<td>26 hnd  53</td>
<td>12 steal  37</td>
<td>15 speed  55</td>
<td>16 stam  62</td>
<td>3c</td>
<td>21 vert 4f</td>
<td>13 oreb  3a</td>
<td>14 dreb  3f</td>
<td>17 dur  55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  41</td>
<td><strong>19  oawr</strong>  55</td>
<td>46</td>
<td>4c</td>
<td>27 def 47</td>
<td>24 qui 5d</td>
<td>潜力 4c</td>
<td>20 str 1c</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps：25岁=35 7c</p>
<h2 id="20200808掘金"><a href="#20200808掘金" class="headerlink" title="20200808掘金"></a>20200808掘金</h2><h3 id="Nikola-Jokic"><a href="#Nikola-Jokic" class="headerlink" title="Nikola Jokic"></a>Nikola Jokic</h3><p>C用76人的Andrew Bynum，名字42796e756d，新名字4e696b6f6c61 4a6f6b6963</p>
<p>身高7‘0’=213.36一样，体重253；?岁；号码15-&gt;30-&gt;1e；肤色05；位置C；原头f1 04=1265；时间设置为32.3min-&gt;129-81；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>91</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>179680</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>5820</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  62</td>
<td>2 med  54</td>
<td>3 3pt  4f</td>
<td>4 ft 52</td>
<td>5 layup  46</td>
<td>6 dunk  55</td>
<td>28 stdnk  5b</td>
<td>23  sit 56</td>
<td>22 sod 56</td>
<td>25 hus  47</td>
</tr>
<tr>
<td>7 hndl  41</td>
<td>8  pass 50</td>
<td>32</td>
<td>40</td>
<td>9 opost  5f</td>
<td>10 dpost  58</td>
<td>11 block   3a</td>
<td>26 hnd  57</td>
<td>12 steal  36</td>
<td>15 speed  41</td>
<td>16 stam  61</td>
<td>28</td>
<td>21 vert 60</td>
<td>13 oreb  4f</td>
<td>14 dreb  59</td>
<td>17 dur  50</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  3c</td>
<td><strong>19  oawr</strong>  62</td>
<td>50</td>
<td>3e</td>
<td>27 def 50</td>
<td>24 qui 5c</td>
<td>潜力 5c</td>
<td>20 str 40</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps:25岁=3d 7c,7‘0’=213.36</p>
<h3 id="Michael-Porter"><a href="#Michael-Porter" class="headerlink" title="Michael Porter"></a>Michael Porter</h3><p>SF用奇才的Martell Webster，名字57656273746572，新名字4d69636861656c 506f72746572</p>
<p>身高6‘10’用208吧，体重209；?岁；号码1—&gt;02；肤色04；位置SF/SG原来的就好；原头ed 04=1261；时间设置为15.6min-&gt;63-3f；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>81</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17b5b0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>31920</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  47</td>
<td>2 med  4f</td>
<td>3 3pt  57</td>
<td>4 ft 53</td>
<td>5 layup  54</td>
<td>6 dunk  4b</td>
<td>28 stdnk  3c</td>
<td>23  sit 48</td>
<td>22 sod 4c</td>
<td>25 hus  3e</td>
</tr>
<tr>
<td>7 hndl  4e</td>
<td>8  pass 41</td>
<td>32</td>
<td>32</td>
<td>9 opost  4b</td>
<td>10 dpost  4e</td>
<td>11 block   44</td>
<td>26 hnd  4f</td>
<td>12 steal  32</td>
<td>15 speed  52</td>
<td>16 stam  50</td>
<td>19</td>
<td>21 vert 58</td>
<td>13 oreb  42</td>
<td>14 dreb  54</td>
<td>17 dur  55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  42</td>
<td><strong>19  oawr</strong>  4d</td>
<td>41</td>
<td>45</td>
<td>27 def 47</td>
<td>24 qui 54</td>
<td>潜力 56</td>
<td>20 str 4f</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>26岁=22 7c，6‘7’=200.66</p>
<h3 id="Jamal-Murray"><a href="#Jamal-Murray" class="headerlink" title="Jamal Murray"></a>Jamal Murray</h3><p>PG用火箭的Toney Douglas, 名字446f75676c6173，新名字4a616d616c 4d7572726179</p>
<p>身高6‘4’=用193吧，体重207；?岁；号码27—&gt;54—&gt;36；肤色03；位置PG/SG=00/01；原头69 06=1641；时间设置为32.8min-&gt;131-83；</p>
<p>20200907三分改成84</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>84</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17aee0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>26700</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  5b</td>
<td>2 med  50</td>
<td>3 3pt  50</td>
<td>4 ft 5c</td>
<td>5 layup  52</td>
<td>6 dunk  3c</td>
<td>28 stdnk  3f</td>
<td>23  sit 3e</td>
<td>22 sod 3b</td>
<td>25 hus  42</td>
</tr>
<tr>
<td>7 hndl  58</td>
<td>8  pass 53</td>
<td>3c</td>
<td>32</td>
<td>9 opost  4b</td>
<td>10 dpost  46</td>
<td>11 block   3f</td>
<td>26 hnd  3c</td>
<td>12 steal  3b</td>
<td>15 speed  4f</td>
<td>16 stam  5a</td>
<td>3c</td>
<td>21 vert 3c</td>
<td>13 oreb  3d</td>
<td>14 dreb  3f</td>
<td>17 dur  3c</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  4d</td>
<td><strong>19  oawr</strong>  53</td>
<td>41</td>
<td>4f</td>
<td>27 def 32</td>
<td>24 qui 4f</td>
<td>潜力 56</td>
<td>20 str 42</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps:26岁=28 7c，6‘1’=187.96</p>
<h3 id="Monte-Morris"><a href="#Monte-Morris" class="headerlink" title="Monte Morris"></a>Monte Morris</h3><p>PG？？？没想好谁改</p>
<p>身高6‘4’，体重207；?岁；号码27—&gt;54—&gt;36；肤色03；位置PG；原头；时间设置为min-&gt;100-64；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>77</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>221a0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  3f</td>
<td>2 med  57</td>
<td>3 3pt  4b</td>
<td>4 ft 58</td>
<td>5 layup  4f</td>
<td>6 dunk  24</td>
<td>28 stdnk  19</td>
<td>23  sit 3d</td>
<td>22 sod 4c</td>
<td>25 hus  49</td>
</tr>
<tr>
<td>7 hndl  51</td>
<td>8  pass 4f</td>
<td>32</td>
<td>32</td>
<td>9 opost  44</td>
<td>10 dpost  46</td>
<td>11 block   37</td>
<td>26 hnd  53</td>
<td>12 steal  37</td>
<td>15 speed  55</td>
<td>16 stam  62</td>
<td>3c</td>
<td>21 vert 4f</td>
<td>13 oreb  3a</td>
<td>14 dreb  3f</td>
<td>17 dur  55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  41</td>
<td><strong>19  oawr</strong>  55</td>
<td>46</td>
<td>4c</td>
<td>27 def 47</td>
<td>24 qui 5d</td>
<td>潜力 4c</td>
<td>20 str 1c</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Torrey-Craig"><a href="#Torrey-Craig" class="headerlink" title="Torrey Craig"></a>Torrey Craig</h3><p>KM405没这人…</p>
<h3 id="Jerami-Grant"><a href="#Jerami-Grant" class="headerlink" title="Jerami Grant"></a>Jerami Grant</h3><p>PF用马刺DeJuan Blair,名字44654a75616e，新名字4a6572616d69 4772616e74</p>
<p>身高6‘8’=205.74，体重210；?岁；号码9—&gt;18—&gt;；肤色02；位置PF/C；原头4b 06=1611；时间设置为26.5min-&gt;106-6a；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>77</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17b040</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>286e0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  48</td>
<td>2 med  46</td>
<td>3 3pt  53</td>
<td>4 ft 46</td>
<td>5 layup  55</td>
<td>6 dunk  50</td>
<td>28 stdnk  5a</td>
<td>23  sit 4d</td>
<td>22 sod 4e</td>
<td>25 hus  57</td>
</tr>
<tr>
<td>7 hndl  3c</td>
<td>8  pass 3c</td>
<td>32</td>
<td>32</td>
<td>9 opost  46</td>
<td>10 dpost  48</td>
<td>11 block   47</td>
<td>26 hnd  3d</td>
<td>12 steal  32</td>
<td>15 speed  49</td>
<td>16 stam  63</td>
<td>3c</td>
<td>21 vert 50</td>
<td>13 oreb  4b</td>
<td>14 dreb  4c</td>
<td>17 dur  3e</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  46</td>
<td><strong>19  oawr</strong>  45</td>
<td>3c</td>
<td>34</td>
<td>27 def 4a</td>
<td>24 qui 4c</td>
<td>潜力 4d</td>
<td>20 str 38</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>ps:23岁=5b 7c，6‘7’=200.66</p>
<h3 id="Bol-Bol"><a href="#Bol-Bol" class="headerlink" title="Bol Bol"></a>Bol Bol</h3><p>C</p>
<h3 id="Will-Barton"><a href="#Will-Barton" class="headerlink" title="Will Barton"></a>Will Barton</h3><p>SF用骑士的Omri Casspi，名字4f6d7269，新名字57696c6c 426172746f6e</p>
<p>身高6‘5’用196吧，体重175；?岁；号码5—&gt;10—&gt;0a；肤色02；位置SF；原头64 06=1636；时间设置为33min-&gt;132-84；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>80</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>179c30</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>b9a0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  46</td>
<td>2 med  5c</td>
<td>3 3pt  52</td>
<td>4 ft 4b</td>
<td>5 layup  4b</td>
<td>6 dunk  3f</td>
<td>28 stdnk  3e</td>
<td>23  sit 39</td>
<td>22 sod 47</td>
<td>25 hus  46</td>
</tr>
<tr>
<td>7 hndl  4f</td>
<td>8  pass 4b</td>
<td>35</td>
<td>32</td>
<td>9 opost  3c</td>
<td>10 dpost  3c</td>
<td>11 block   35</td>
<td>26 hnd  50</td>
<td>12 steal  31</td>
<td>15 speed  4b</td>
<td>16 stam  58</td>
<td>28</td>
<td>21 vert 36</td>
<td>13 oreb  3c</td>
<td>14 dreb  3f</td>
<td>17 dur  49</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  4b</td>
<td><strong>19  oawr</strong>  55</td>
<td>37</td>
<td>54</td>
<td>27 def 44</td>
<td>24 qui 52</td>
<td>潜力 4f</td>
<td>20 str 29</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>24岁=4b 7c，6‘8’=205.74</p>
<h3 id="Gary-Harris"><a href="#Gary-Harris" class="headerlink" title="Gary Harris"></a>Gary Harris</h3><p>SG用黄蜂卡罗尔Matt Carroll，名字436172726f6c6c，新名字47617279 486172726973</p>
<p>身高6‘4’用193吧，体重210；?岁；号码14—&gt;28—&gt;1c；肤色04；位置SG；原头1d 05=1309；时间设置为min-&gt;100-64；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>76</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>179850</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>7800</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  35</td>
<td>2 med  56</td>
<td>3 3pt  4a</td>
<td>4 ft 53</td>
<td>5 layup  52</td>
<td>6 dunk  3e</td>
<td>28 stdnk  28</td>
<td>23  sit 3d</td>
<td>22 sod 3c</td>
<td>25 hus  53</td>
</tr>
<tr>
<td>7 hndl  4c</td>
<td>8  pass 4b</td>
<td>32</td>
<td>32</td>
<td>9 opost  49</td>
<td>10 dpost  48</td>
<td>11 block   22</td>
<td>26 hnd  49</td>
<td>12 steal  3f</td>
<td>15 speed  48</td>
<td>16 stam  55</td>
<td>19</td>
<td>21 vert 48</td>
<td>13 oreb  4f</td>
<td>14 dreb  46</td>
<td>17 dur  46</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  50</td>
<td><strong>19  oawr</strong>  3c</td>
<td>19</td>
<td>38</td>
<td>27 def 32</td>
<td>24 qui 49</td>
<td>潜力 53</td>
<td>20 str 3a</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>32岁=ce 7b，6‘6’=198.12</p>
<h2 id="网队复兴"><a href="#网队复兴" class="headerlink" title="网队复兴"></a>网队复兴</h2><h3 id="Kyrie-Irving"><a href="#Kyrie-Irving" class="headerlink" title="Kyrie Irving"></a>Kyrie Irving</h3><p>面部打v8补丁，有发带。头像看要不要换绿军的。</p>
<p>身高6‘3’，体重191；?岁；号码11—&gt;22—&gt;16；肤色02；位置PG；原头4a 08=2122；时间设置为33min-&gt;132-84；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>91</th>
<th></th>
<th>ae60</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  5b</td>
<td>2 med  5c</td>
<td>3 3pt  53</td>
<td>4 ft 5e</td>
<td>5 layup  60</td>
<td>6 dunk  3d</td>
<td>28 stdnk  19</td>
<td>23  sit 4c</td>
<td>22 sod 4b</td>
<td>25 hus  52</td>
</tr>
<tr>
<td>7 hndl  63</td>
<td>8  pass 58</td>
<td>40</td>
<td>32</td>
<td>9 opost  40</td>
<td>10 dpost  49</td>
<td>11 block   30</td>
<td>26 hnd  57</td>
<td>12 steal  38</td>
<td>15 speed  57</td>
<td>16 stam  63</td>
<td>23</td>
<td>21 vert 5c</td>
<td>13 oreb  36</td>
<td>14 dreb  35</td>
<td>17 dur  54</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  43</td>
<td><strong>19  oawr</strong>  60</td>
<td>50</td>
<td>50</td>
<td>27 def 44</td>
<td>24 qui 60</td>
<td>潜力 59不改</td>
<td>20 str 25</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Kevin-Durant"><a href="#Kevin-Durant" class="headerlink" title="Kevin Durant"></a>Kevin Durant</h3><p>能力值更新</p>
<p>身高6‘8’，体重；?岁；号码07—&gt;14—&gt;0e；肤色02；位置SF/PF；原头；时间不改了，反正是球队C位</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>96</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>2a6c0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  62</td>
<td>2 med  62</td>
<td>3 3pt  56</td>
<td>4 ft 59</td>
<td>5 layup  5e</td>
<td>6 dunk  55</td>
<td>28 stdnk  56</td>
<td>23  sit 57</td>
<td>22 sod 5f</td>
<td>25 hus  4e</td>
</tr>
<tr>
<td>7 hndl  56</td>
<td>8  pass 4b</td>
<td>5a</td>
<td>32</td>
<td>9 opost  4e</td>
<td>10 dpost  4c</td>
<td>11 block   42</td>
<td>26 hnd  63</td>
<td>12 steal  32</td>
<td>15 speed  52</td>
<td>16 stam  5e</td>
<td>3c</td>
<td>21 vert 49</td>
<td>13 oreb  3c</td>
<td>14 dreb  54</td>
<td>17 dur  55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  50</td>
<td><strong>19  oawr</strong>  5f</td>
<td>63</td>
<td>5f</td>
<td>27 def 4b</td>
<td>24 qui 57</td>
<td>潜力 61</td>
<td>20 str 38</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Spencer-Dinwiddie"><a href="#Spencer-Dinwiddie" class="headerlink" title="Spencer Dinwiddie"></a>Spencer Dinwiddie</h3><p>PG</p>
<p>身高，体重；?岁；号码—&gt;28—&gt;；肤色；位置SF；原头；时间设置为min-&gt;100-64；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  35</td>
<td>2 med  56</td>
<td>3 3pt  4a</td>
<td>4 ft 53</td>
<td>5 layup  52</td>
<td>6 dunk  3e</td>
<td>28 stdnk  28</td>
<td>23  sit 3d</td>
<td>22 sod 3c</td>
<td>25 hus  53</td>
</tr>
<tr>
<td>7 hndl  4c</td>
<td>8  pass 4b</td>
<td>32</td>
<td>32</td>
<td>9 opost  49</td>
<td>10 dpost  48</td>
<td>11 block   22</td>
<td>26 hnd  49</td>
<td>12 steal  3f</td>
<td>15 speed  48</td>
<td>16 stam  55</td>
<td>19</td>
<td>21 vert 48</td>
<td>13 oreb  4f</td>
<td>14 dreb  46</td>
<td>17 dur  46</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  50</td>
<td><strong>19  oawr</strong>  3c</td>
<td>19</td>
<td>38</td>
<td>27 def 32</td>
<td>24 qui 49</td>
<td>潜力 53</td>
<td>20 str 3a</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Caris-LeVert"><a href="#Caris-LeVert" class="headerlink" title="Caris LeVert"></a>Caris LeVert</h3><p>SG</p>
<p>身高，体重；?岁；号码—&gt;28—&gt;；肤色；位置SF；原头；时间设置为min-&gt;100-64；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  35</td>
<td>2 med  56</td>
<td>3 3pt  4a</td>
<td>4 ft 53</td>
<td>5 layup  52</td>
<td>6 dunk  3e</td>
<td>28 stdnk  28</td>
<td>23  sit 3d</td>
<td>22 sod 3c</td>
<td>25 hus  53</td>
</tr>
<tr>
<td>7 hndl  4c</td>
<td>8  pass 4b</td>
<td>32</td>
<td>32</td>
<td>9 opost  49</td>
<td>10 dpost  48</td>
<td>11 block   22</td>
<td>26 hnd  49</td>
<td>12 steal  3f</td>
<td>15 speed  48</td>
<td>16 stam  55</td>
<td>19</td>
<td>21 vert 48</td>
<td>13 oreb  4f</td>
<td>14 dreb  46</td>
<td>17 dur  46</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  50</td>
<td><strong>19  oawr</strong>  3c</td>
<td>19</td>
<td>38</td>
<td>27 def 32</td>
<td>24 qui 49</td>
<td>潜力 53</td>
<td>20 str 3a</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Joe-Harris"><a href="#Joe-Harris" class="headerlink" title="Joe Harris"></a>Joe Harris</h3><p>SG</p>
<p>黄蜂的Ben Gordon。4a6f65 486172726973</p>
<p>身高6’6’=198.12，体重219；?岁；号码12—&gt;24—&gt;18；肤色04；位置SG；原头80 04=1152；时间设置为30.7min-&gt;122.8-7b；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>76</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>179810</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>7260</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  43</td>
<td>2 med  59</td>
<td>3 3pt  55</td>
<td>4 ft 4f</td>
<td>5 layup  42</td>
<td>6 dunk  42</td>
<td>28 stdnk  1e</td>
<td>23  sit 56</td>
<td>22 sod 4a</td>
<td>25 hus  4a</td>
</tr>
<tr>
<td>7 hndl  3c</td>
<td>8  pass 3c</td>
<td>49</td>
<td>32</td>
<td>9 opost  41</td>
<td>10 dpost  44</td>
<td>11 block   3c</td>
<td>26 hnd  43</td>
<td>12 steal  41</td>
<td>15 speed  3d</td>
<td>16 stam  43</td>
<td>19</td>
<td>21 vert 4f</td>
<td>13 oreb  37</td>
<td>14 dreb  41</td>
<td>17 dur  50</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  46</td>
<td><strong>19  oawr</strong>  47</td>
<td>4b</td>
<td>4d</td>
<td>27 def 48</td>
<td>24 qui 48</td>
<td>潜力 4e</td>
<td>20 str 42</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="DeAndre-Jordan"><a href="#DeAndre-Jordan" class="headerlink" title="DeAndre Jordan"></a>DeAndre Jordan</h3><p>C</p>
<p>身高，体重；?岁；号码6—&gt;12—&gt;0c；肤色02；位置C；原头02 06=1538；时间设置为22min-&gt;88-58；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>81</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>e4c0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  4a</td>
<td>2 med  32</td>
<td>3 3pt  19</td>
<td>4 ft 44</td>
<td>5 layup  48</td>
<td>6 dunk  60</td>
<td>28 stdnk  61</td>
<td>23  sit 35</td>
<td>22 sod 39</td>
<td>25 hus  4d</td>
</tr>
<tr>
<td>7 hndl  41</td>
<td>8  pass 39</td>
<td>32</td>
<td>32</td>
<td>9 opost  4d</td>
<td>10 dpost  4f</td>
<td>11 block   4e</td>
<td>26 hnd  48</td>
<td>12 steal  42</td>
<td>15 speed  41</td>
<td>16 stam  60</td>
<td>3c</td>
<td>21 vert 4e</td>
<td>13 oreb  4e</td>
<td>14 dreb  61</td>
<td>17 dur  54</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  50</td>
<td><strong>19  oawr</strong>  48</td>
<td>32</td>
<td>37</td>
<td>27 def 42</td>
<td>24 qui 41</td>
<td>潜力 44不改了</td>
<td>20 str 56</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Taurean-Prince"><a href="#Taurean-Prince" class="headerlink" title="Taurean Prince"></a>Taurean Prince</h3><p>SF</p>
<p>马刺Stephen Jackson。5461757265616e 5072696e6365.</p>
<p>身高6’8’=205.8，体重220；?岁；号码2—&gt;4—&gt;04；肤色03；位置SF/SG；原头5d 00=0093；时间设置为29min-&gt;116-74；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>73</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17afe0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>27f60</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  44</td>
<td>2 med  49</td>
<td>3 3pt  4f</td>
<td>4 ft 4e</td>
<td>5 layup  4c</td>
<td>6 dunk  4b</td>
<td>28 stdnk  57</td>
<td>23  sit 46</td>
<td>22 sod 44</td>
<td>25 hus  52</td>
</tr>
<tr>
<td>7 hndl  46</td>
<td>8  pass 32</td>
<td>49</td>
<td>32</td>
<td>9 opost  49</td>
<td>10 dpost  46</td>
<td>11 block   39</td>
<td>26 hnd  49</td>
<td>12 steal  44</td>
<td>15 speed  52</td>
<td>16 stam  56</td>
<td>5a</td>
<td>21 vert 4f</td>
<td>13 oreb  3d</td>
<td>14 dreb  4b</td>
<td>17 dur  55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  41</td>
<td><strong>19  oawr</strong>  42</td>
<td>46</td>
<td>52</td>
<td>27 def 4c</td>
<td>24 qui 40</td>
<td>潜力 4e</td>
<td>20 str 32</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Jarrett-Allen"><a href="#Jarrett-Allen" class="headerlink" title="Jarrett Allen"></a>Jarrett Allen</h3><p>C</p>
<p>掘金的Timofey Mozgov。4a617272657474 416c6c656e.</p>
<p>身高6’11’=211，体重236；?岁；号码31—&gt;63—&gt;3f；肤色04；位置C；原头49 07=1865；时间设置为26.3min-&gt;105.2-69；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>81</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17ab00</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>20580</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  4c</td>
<td>2 med  36</td>
<td>3 3pt  19</td>
<td>4 ft 41</td>
<td>5 layup  49</td>
<td>6 dunk  4b</td>
<td>28 stdnk  55</td>
<td>23  sit 49</td>
<td>22 sod 3f</td>
<td>25 hus  59</td>
</tr>
<tr>
<td>7 hndl  23</td>
<td>8  pass 19</td>
<td>32</td>
<td>32</td>
<td>9 opost  4b</td>
<td>10 dpost  50</td>
<td>11 block   52</td>
<td>26 hnd  54</td>
<td>12 steal  42</td>
<td>15 speed  4d</td>
<td>16 stam  63</td>
<td>4b</td>
<td>21 vert 3a</td>
<td>13 oreb  51</td>
<td>14 dreb  55</td>
<td>17 dur  50</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  50</td>
<td><strong>19  oawr</strong>  55</td>
<td>19</td>
<td>50</td>
<td>27 def 5a</td>
<td>24 qui 39</td>
<td>潜力 51</td>
<td>20 str 53</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Garrett-Temple"><a href="#Garrett-Temple" class="headerlink" title="Garrett Temple"></a>Garrett Temple</h3><p>SG</p>
<p>🦅的Anthony Morrow。47617272657474 54656d706c65.</p>
<p><strong>此改动取消！这个Anthony和甜瓜的名字是绑定的！</strong></p>
<p>身高6’5’=195.58，体重190；?岁；号码17—&gt;34—&gt;22；肤色03；位置SG/SF；原头26 06=1574；时间设置为27.8min-&gt;111.2-6f；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>71</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td>17a090</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>111c0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  47</td>
<td>2 med  3d</td>
<td>3 3pt  4b</td>
<td>4 ft 4d</td>
<td>5 layup  4c</td>
<td>6 dunk  3d</td>
<td>28 stdnk  33</td>
<td>23  sit 50</td>
<td>22 sod 50</td>
<td>25 hus  49</td>
</tr>
<tr>
<td>7 hndl  4b</td>
<td>8  pass 4b</td>
<td>32</td>
<td>32</td>
<td>9 opost  49</td>
<td>10 dpost  3d</td>
<td>11 block   3a</td>
<td>26 hnd  4c</td>
<td>12 steal  32</td>
<td>15 speed  3b</td>
<td>16 stam  5c</td>
<td>19</td>
<td>21 vert 49</td>
<td>13 oreb  35</td>
<td>14 dreb  39</td>
<td>17 dur  55</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  4b</td>
<td><strong>19  oawr</strong>  4d</td>
<td>3c</td>
<td>56</td>
<td>27 def 4c</td>
<td>24 qui 4d</td>
<td>潜力 4a</td>
<td>20 str 3f</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="Wilson-Chandler"><a href="#Wilson-Chandler" class="headerlink" title="Wilson Chandler"></a>Wilson Chandler</h3><p>PF</p>
<p>身高，体重；?岁；号码21—&gt;41—&gt;29；肤色02；位置SF/PF；原头98 05=1432；时间设置为21min-&gt;84-54；</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>/</th>
<th>KM最新</th>
<th>71</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>姓名地址</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>数据地址</td>
<td>20940</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>00</strong></td>
<td><strong>01</strong></td>
<td><strong>02</strong></td>
<td><strong>03</strong></td>
<td><strong>04</strong></td>
<td><strong>05</strong></td>
<td><strong>06</strong></td>
<td><strong>07</strong></td>
<td><strong>08</strong></td>
<td><strong>09</strong></td>
<td><strong>0a</strong></td>
<td><strong>0b</strong></td>
<td><strong>0c</strong></td>
<td><strong>0d</strong></td>
<td><strong>0e</strong></td>
<td><strong>0f</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>1 close  4f</td>
<td>2 med  4f</td>
<td>3 3pt  47</td>
<td>4 ft 48</td>
<td>5 layup  42</td>
<td>6 dunk  4b</td>
<td>28 stdnk  37</td>
<td>23  sit 40</td>
<td>22 sod 47</td>
<td>25 hus  52</td>
</tr>
<tr>
<td>7 hndl  47</td>
<td>8  pass 3c</td>
<td>32</td>
<td>32</td>
<td>9 opost  48</td>
<td>10 dpost  4c</td>
<td>11 block   38</td>
<td>26 hnd  49</td>
<td>12 steal  3e</td>
<td>15 speed  3d</td>
<td>16 stam  57</td>
<td>19</td>
<td>21 vert 58</td>
<td>13 oreb  32</td>
<td>14 dreb  43</td>
<td>17 dur  52</td>
</tr>
<tr>
<td><strong>18 dawr</strong>  3f</td>
<td><strong>19  oawr</strong>  4d</td>
<td>41</td>
<td>4c</td>
<td>27 def 50</td>
<td>24 qui 4b</td>
<td>潜力 54不改了</td>
<td>20 str 46</td>
<td>？</td>
<td>？</td>
<td>？</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      <tags>
        <tag>NBA2K13</tag>
        <tag>DIY</tag>
      </tags>
  </entry>
  <entry>
    <title>[hexo] 用Hexo d提交博客时出现连接不了的错误</title>
    <url>/2020/02/20/hexo-%E7%94%A8Hexo-d%E6%8F%90%E4%BA%A4%E5%8D%9A%E5%AE%A2%E6%97%B6%E5%87%BA%E7%8E%B0%E8%BF%9E%E6%8E%A5%E4%B8%8D%E4%BA%86%E7%9A%84%E9%94%99%E8%AF%AF/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<ul>
<li>Connection reset</li>
<li>fatal: Could not read from remote repository</li>
<li>Error: Spawn failed</li>
</ul>
</blockquote>
<a id="more"></a>
<p>这次写完博客，准备用<code>hexo d</code>命令上传的时候报了一堆错…</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The file will have its original line endings in your working directory.</span><br><span class="line">On branch master</span><br><span class="line">nothing to commit, working directory clean</span><br><span class="line">Connection reset by xx.xxx.xxx.xxx</span><br><span class="line">fatal: Could not read from remote repository.</span><br><span class="line"></span><br><span class="line">Please make sure you have the correct access rights</span><br><span class="line">and the repository exists.</span><br><span class="line">FATAL Something&apos;s wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.html</span><br><span class="line">Error: Spawn failed</span><br><span class="line">    at ChildProcess.&lt;anonymous&gt; (F:\xxx\Blogs\hexo-doc\node_modules\hexo-deployer-git\node_modules\hexo-util\lib\spawn.js:xx:xx)</span><br><span class="line">    at ChildProcess.emit (events.js:xxx:x)</span><br><span class="line">    at ChildProcess.cp.emit (F:\xxx\Blogs\hexo-doc\node_modules\hexo-deployer-git\node_modules\cross-spawn\lib\enoent.js:xx:xx)</span><br><span class="line">    at Process.ChildProcess._handle.onexit (internal/child_process.js:xxx:xx)</span><br></pre></td></tr></table></figure>
<p>中间我隐去了一些地址什么的，大概的问题包括：</p>
<ul>
<li>Connection reset</li>
<li>fatal: Could not read from remote repository</li>
<li>Error: Spawn failed</li>
</ul>
<p>查阅不少博客之后我的问题解决了，先删除原先的<code>ssh</code>文件，重新生成一个并存到<code>GitHub</code>账户里。然后<strong>—删除本地博客目录下的文件夹<code>.deploy_git</code>—</strong>，好了，重新<code>hexo g</code>+<code>gulp</code>+<code>hexo d</code>就Okay了。</p>
<p>事后我觉得这次bug的原因是我用<code>hexo new &quot;blog title&quot;</code>生成新博客之后对标题不满意，<strong><del>手动在本地修改了<code>md</code>文件的文件名和对应的图片文件夹名</del></strong> ，导致<code>.deploy_git</code>的内容和不知道哪里出现了一些不匹配？<strong>当然这个原因是我猜的~如有高见可以指点我QwQ</strong></p>
]]></content>
      <tags>
        <tag>Hexo</tag>
        <tag>Bugs</tag>
      </tags>
  </entry>
  <entry>
    <title>[LaTeX]ML课程作业模板bugs研究记录</title>
    <url>/2020/02/20/LaTeX-ML%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E6%A8%A1%E6%9D%BFbugs%E7%A0%94%E7%A9%B6%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>在某课程上遇到的$\LaTeX$模板的bugs研究</p>
</blockquote>
<a id="more"></a>
<p>今年头一次正式选了一门ML课程，第一周的作业就是用$\LaTeX$写一份介绍文档。</p>
<p>模板是老师提供的，而凡遇模板必有bug。今天就让哥会会它。</p>
<h2 id="模板一览-关键信息已隐藏"><a href="#模板一览-关键信息已隐藏" class="headerlink" title="模板一览(关键信息已隐藏)"></a>模板一览(关键信息已隐藏)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\documentclass[twoside, openany]&#123;cctbook&#125;</span><br><span class="line">\usepackage&#123;graphicx&#125;</span><br><span class="line">\usepackage&#123;Picins&#125;</span><br><span class="line">\usepackage&#123;amssymb&#125;</span><br><span class="line">\usepackage&#123;makeidx&#125;</span><br><span class="line">\usepackage&#123;mathrsfs&#125;</span><br><span class="line">\usepackage&#123;amsmath&#125;</span><br><span class="line">\usepackage&#123;rotating&#125;</span><br><span class="line">\input vatola.sty</span><br><span class="line">%\input psfig.sty</span><br><span class="line">\input cyracc.def</span><br><span class="line">\input amssymb.sty\TagsOnRight</span><br><span class="line">\font\tencyr=wncyr10</span><br><span class="line">\def\cyr&#123;\tencyr\cyracc&#125;</span><br><span class="line">\newsymbol\wjzhml 203F%空集</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">\begin&#123;figure&#125;[t]</span><br><span class="line">...</span><br><span class="line">\end&#123;figure&#125;</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>拿到这个模板看第一眼我就感觉要gg（最近刚刚从<code>CTEX</code>换成最新的<code>TeXLive</code>+<code>TeXStudio</code>套装）。这模板开头就送我一句：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\usepackage&#123;Picins&#125;</span><br></pre></td></tr></table></figure>
<p>这个包是老版本的包，在<code>CTEX</code>中常见，自从换了新底层<code>TeXLive</code>我就不用它了。</p>
<h2 id="有哪些bugs"><a href="#有哪些bugs" class="headerlink" title="有哪些bugs"></a>有哪些bugs</h2><p>简单地用<code>XeLaTeX</code>编译一下，<code>warnings</code>可以不管，但是一片报错可是太难了。研究过程<strong>艰辛而快乐</strong>，就不写了，下面说说哪些地方bugs比较严重。</p>
<h3 id="老式模板需要更换"><a href="#老式模板需要更换" class="headerlink" title="老式模板需要更换"></a>老式模板需要更换</h3><p>首先是开头的声明就会报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\documentclass[twoside, openany]&#123;cctbook&#125;</span><br></pre></td></tr></table></figure>
<p>原因是<code>cctbook</code>版式适合<code>CTEX</code>，但是新的<code>TeXLive</code>并不会识别这样的老版式，解决方法是更换一个兼容的版式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\documentclass[twoside, openany]&#123;ctexart&#125;</span><br></pre></td></tr></table></figure>
<h3 id="各种宏包的缺失"><a href="#各种宏包的缺失" class="headerlink" title="各种宏包的缺失"></a>各种宏包的缺失</h3><p>这里主要是老版本的包很麻烦，尤其是<code>CTEX</code>比较喜欢用的，这里包括<code>Pincins</code>，<code>psfig</code>和<code>vatola</code>。我选择了快乐的手动下包法，直接上<code>Ctan</code>下载对应的<code>sty</code>文件放在<code>tex</code>文件同目录下即可。</p>
<h3 id="重复调用宏包"><a href="#重复调用宏包" class="headerlink" title="重复调用宏包"></a>重复调用宏包</h3><p>注意模板里有个重复的地方，在已经<code>\usepackage{amssymb}</code>的情况下，下文又<code>\input amssymb.sty</code>，这虽影响不大，但是<code>warning</code>是少不了，直接注释比较舒服：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">%\input amssymb.sty</span><br></pre></td></tr></table></figure>
<h3 id="其它麻烦的小东西"><a href="#其它麻烦的小东西" class="headerlink" title="其它麻烦的小东西"></a>其它麻烦的小东西</h3><ul>
<li><p>第一个是这个奇怪的符号定义<code>\newsymbol\wjzhml 203F%空集</code>，不管它可以，会有个<code>warning</code>。</p>
</li>
<li><p>第二个是省略的<code>...</code>中各种奇怪的定义，很多规则似乎已经落后了，所以会有规则冲突，会报很多<code>warnins</code>，不过<code>TeXStudio</code>直接忽略，能生成<code>pdf</code>就可以。</p>
</li>
<li><p>第三个是<code>\begin{figure}[t]</code>，直接改成<code>\begin{figure}[h]</code>免得图片位置爆炸。</p>
</li>
<li>还有的记不得了，反正哥调好啦~</li>
</ul>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>感谢以下链接的帮助：</p>
<ul>
<li><p>有关<code>cctbook</code>：<a href="http://tieba.baidu.com/p/6251773770" target="_blank" rel="noopener">http://tieba.baidu.com/p/6251773770</a></p>
</li>
<li><p>下包的链接：</p>
<ul>
<li><a href="https://www.ctan.org/tex-archive/macros/latex209/contrib/picins/" target="_blank" rel="noopener">https://www.ctan.org/tex-archive/macros/latex209/contrib/picins/</a></li>
<li><a href="https://www.latexstudio.net/archives/3608" target="_blank" rel="noopener">https://www.latexstudio.net/archives/3608</a></li>
<li><a href="https://www.ctan.org/tex-archive/graphics/psfig/" target="_blank" rel="noopener">https://www.ctan.org/tex-archive/graphics/psfig/</a></li>
</ul>
</li>
<li><p>宏包<code>vatola</code>的介绍：<a href="https://blog.csdn.net/sinat_32547403/article/details/76599126" target="_blank" rel="noopener">https://blog.csdn.net/sinat_32547403/article/details/76599126</a></p>
</li>
</ul>
]]></content>
      <tags>
        <tag>LaTeX</tag>
        <tag>Bugs</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows 10 DPC_WATCHDOG_VIOLATION 问题</title>
    <url>/2020/01/05/Windows-10-DPC-WATCHDOG-VIOLATION-%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>继电池容量不足后发生的电脑蓝屏</p>
<p><code>DPC_WATCHDOG_VIOLATION</code></p>
</blockquote>
<a id="more"></a>
<h2 id="bug背景"><a href="#bug背景" class="headerlink" title="bug背景"></a>bug背景</h2><p>先交代一下bug背景，由于电池容量严重不足，电脑之前的用户配置文件损坏，不得不先创建了一个新的账户继续使用。可能是由于这些个原因跳bug。</p>
<h2 id="bug细节"><a href="#bug细节" class="headerlink" title="bug细节"></a>bug细节</h2><p>某天开机，刚打开浏览器，电脑突然死机，触摸板，键盘，鼠标，显示屏均没有任何反应。等待约5min之后电脑自动蓝屏，提示我出了问题。</p>
<img src="/2020/01/05/Windows-10-DPC-WATCHDOG-VIOLATION-问题/BS.jpg" title="Blue Screen">
<p>Bug名称为<code>DPC_WATCHDOG_VIOLATION</code>。</p>
<h2 id="bug解决"><a href="#bug解决" class="headerlink" title="bug解决"></a>bug解决</h2><p>长按电源键关机重启后，电脑正常开机。等待一小会，360自动弹框提示蓝屏错误（不是打广告）。</p>
<img src="/2020/01/05/Windows-10-DPC-WATCHDOG-VIOLATION-问题/3601.jpg" title="360提示">
<p>它给出的解决方案是</p>
<img src="/2020/01/05/Windows-10-DPC-WATCHDOG-VIOLATION-问题/3602.png" title="360删补丁大法">
<p>觉得删补丁不稳，干脆百度。经查询，大部分解决方案都和[百度经验][<a href="https://jingyan.baidu.com/article/cd4c29795ab8dc756e6e60c5.html]一致，照着来一遍，截至目前bug没有再犯。" target="_blank" rel="noopener">https://jingyan.baidu.com/article/cd4c29795ab8dc756e6e60c5.html]一致，照着来一遍，截至目前bug没有再犯。</a></p>
]]></content>
      <tags>
        <tag>Windows</tag>
        <tag>Bug</tag>
      </tags>
  </entry>
  <entry>
    <title>西北中心研讨班——图像质量评价</title>
    <url>/2019/12/13/%E8%A5%BF%E5%8C%97%E4%B8%AD%E5%BF%83%E7%A0%94%E8%AE%A8%E7%8F%AD%E2%80%94%E2%80%94%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p> 数学西北中心“数据科学与医疗健康”主题年研讨班</p>
<p> 仅记录一些笔记&amp;感想，内容并不详细！</p>
</blockquote>
<a id="more"></a>
<h2 id="开幕简介"><a href="#开幕简介" class="headerlink" title="开幕简介"></a>开幕简介</h2><p>图像模型都是老套的啦，无非是设计loss，根据task的需求设计参数，我更倾向于用MWN嘻嘻。</p>
<p>可以举几个常用的model的例子，VAE，滤波，Baysian推断加正则项。</p>
<p>最后提一下很重要的图像评价指标：</p>
<ul>
<li><strong>通用图像质量评价</strong></li>
<li><strong>面向任务的图像质量评价</strong></li>
</ul>
<h2 id="Bayesian-CT图像重建"><a href="#Bayesian-CT图像重建" class="headerlink" title="Bayesian CT图像重建"></a>Bayesian CT图像重建</h2><p>由纽约石溪大学的教授Jerome Zhengrong Liang给出。由于版权问题，笔记不在此，也不发布。</p>
<h2 id="深度纹理分析-Deep-Texture-Analysis"><a href="#深度纹理分析-Deep-Texture-Analysis" class="headerlink" title="深度纹理分析 Deep Texture Analysis"></a>深度纹理分析 Deep Texture Analysis</h2><p>由纽约石溪大学的教授Jerome Zhengrong Liang给出。由于版权问题，笔记不在此，也不发布。但是可以在这里列一篇参考文献</p>
<ul>
<li>Weiguo Cao, Marc J. Pomeroy, Perry J. Pickhardt, Matthew A. Barish, Samuel Stanley III, Hongbing Lu, Zhengrong Liang, “A pyramid machine learning model for polyp classification via CT colonography,” Proc. SPIE 10950, Medical Imaging 2019: Computer-Aided Diagnosis, 109502U (30 May 2019);<a href="https://doi.org/10.1117/12.2513068" target="_blank" rel="noopener">https://doi.org/10.1117/12.2513068</a></li>
</ul>
<h2 id="图像质量评价IQA中的问题、理论和方法"><a href="#图像质量评价IQA中的问题、理论和方法" class="headerlink" title="图像质量评价IQA中的问题、理论和方法"></a>图像质量评价IQA中的问题、理论和方法</h2><p>由张林教授给出。两种图像质量的评价标准：</p>
<ul>
<li>人类感知效果</li>
<li>具体任务的评价指标</li>
</ul>
<h3 id="基于人类感知"><a href="#基于人类感知" class="headerlink" title="基于人类感知"></a>基于人类感知</h3><ul>
<li>Pearson线性相关系数PLCC</li>
<li>均方误差的平方根RMSE</li>
</ul>
<p>具体公式不会耶，应该是具体问题具体算客观评分和自己的评分</p>
<h4 id="FR-IQR方法"><a href="#FR-IQR方法" class="headerlink" title="FR-IQR方法"></a>FR-IQR方法</h4><p>度量图像之间的距离使之满足人类感知。</p>
<p>MSE不会考虑图像上点的顺序，不会考虑正负偏差，因此与人的感知可能有一些差距。</p>
<p>相对来说SSIM（结构相似性：structural similarity）会好很多，它是个基石性的工作。它会把图像的变化分为有结构化改变和没有结构化改变两种大情况。</p>
<p>举个例子，对原始标准图像施加Gaussian noise后，MSE对应的bias是随机模糊的noise，SSIM对应的bias有结构化的意义在里面。PPT里有图像例子，这里没有拍照。</p>
<p>总之就是，MSE一样时，SSIM能度量结构化的error。</p>
<h4 id="FSIM-FSIMc-Feature-Similarity-Index"><a href="#FSIM-FSIMc-Feature-Similarity-Index" class="headerlink" title="FSIM/FSIMc: Feature Similarity Index"></a>FSIM/FSIMc: Feature Similarity Index</h4><p>核心概念是相位的一致性phase congruency。这个东西可以算的，具体怎么算不写了，反正我的确不是很会QwQ。</p>
<p>不过FSIM的计算是受到SSIM很大的启发的，计算过程比较相似。</p>
<h4 id="GMSD-Gradient-magnitude-similarity-deviation"><a href="#GMSD-Gradient-magnitude-similarity-deviation" class="headerlink" title="GMSD: Gradient magnitude similarity deviation"></a>GMSD: Gradient magnitude similarity deviation</h4><p>梯度模相似性标准差，据说非常有效非常快。主要用的是梯度模的信息。</p>
<h4 id="LPIPS-A-CNN-based-FR-IQA-Index"><a href="#LPIPS-A-CNN-based-FR-IQA-Index" class="headerlink" title="LPIPS: A CNN based FR-IQA Index"></a>LPIPS: A CNN based FR-IQA Index</h4><p>这都是个什么东西。Learned perceptual image patch similarity.大概是衡量两种patch图像和标准图像的相似性。但是以前的指标判断的都不符合人类感知，这里LPIPS可以衡量得很好，而不依赖于具体DeepCNN的结构。</p>
<p>LPIPS训练的是tuple：$\{x,x_0,x_1,h\}$，参考图像，patch0，patch1，h指示哪个patch更好。这个有点意思了。</p>
<h4 id="NR-IQR方法"><a href="#NR-IQR方法" class="headerlink" title="NR-IQR方法"></a>NR-IQR方法</h4><p>FR似乎是指有reference，而NR似乎没啥标准，所以它要自己判断如何符合人类感知。</p>
<p>NR似乎可以用于对抗学习。</p>
<h3 id="基于task"><a href="#基于task" class="headerlink" title="基于task"></a>基于task</h3><p>自建数据集去研究去雾，和师姐很想但没师姐强嘻嘻。</p>
<h2 id="下午实在是困了，没记笔记，再见"><a href="#下午实在是困了，没记笔记，再见" class="headerlink" title="下午实在是困了，没记笔记，再见"></a>下午实在是困了，没记笔记，再见</h2><p>中午被拖堂，没睡好觉。。。</p>
<h2 id="CT系统设计及锥束CT成像方法"><a href="#CT系统设计及锥束CT成像方法" class="headerlink" title="CT系统设计及锥束CT成像方法"></a>CT系统设计及锥束CT成像方法</h2><p>由朱磊教授给出，GaTech校友，他讲的其实非常好，基本上把图像质量评价和CT面临的问题都讲清楚了！</p>
<p>一个重点是把实际需求转化为多目标优化问题。</p>
<p>具体内容只剩一些照片了，不展示在这里。</p>
<h2 id="基于特定任务的医学图像质量评价"><a href="#基于特定任务的医学图像质量评价" class="headerlink" title="基于特定任务的医学图像质量评价"></a>基于特定任务的医学图像质量评价</h2><p>由卢hb教授给出。第二天下午太浪了，没记什么笔记。</p>
<p>讲了讲图像质量评价发展、基于特征保持的CT图像重建及恢复和一些医学图像诊疗性能评价方法，一共三块。</p>
<p>其总的方法都属于<code>影像组学</code> <code>Radiomics</code>，分割啊，特征提取啊再分析这样，影像组学的思路（或者说流程）可以为医学图像处理提供部分定量的研究手段，其实就是能算的一些指标。</p>
<h4 id="基于特征保持的（Ld）CT图像重建及恢复"><a href="#基于特征保持的（Ld）CT图像重建及恢复" class="headerlink" title="基于特征保持的（Ld）CT图像重建及恢复"></a>基于特征保持的（Ld）CT图像重建及恢复</h4><p>举几个例子：</p>
<ul>
<li>针对nonstationary noise的各项异性边缘引导的TV最小化重建方法；</li>
<li>针对photon starvation的高阶微分模型TVS最小重建方法；</li>
<li>针对streak artifacts的基于NLM约束的迭代重建方法；</li>
<li>针对noise/details tradeoff的基于邻域主成分的非局部均值成像算法。</li>
</ul>
<p>还有一个例子好像很有意思，有机会再看吧：</p>
<ul>
<li>Knowledge-Based Bayesian Reconstruction Using Information from Previous Full-dose CT Scan</li>
</ul>
<h4 id="诊疗性能评价方法"><a href="#诊疗性能评价方法" class="headerlink" title="诊疗性能评价方法"></a>诊疗性能评价方法</h4><p>我感觉就是基本的数据分析方法。。。</p>
<ul>
<li>ROC: receiver operating characteristic curve，真阳性率和假阳性率的曲线<ul>
<li>用于两种或以上不同诊断方法对疾病识别能力的比较；</li>
<li>越靠近左上角，工作越准确（纵轴是真阳性率）；</li>
<li>离左上角垂直距离最短的一点，敏感度特异度之和最大，成为最佳临界点，点上的值称为最佳临界值</li>
<li>ROC曲线下面积AUC，越大，诊断价值越高。一般高于80%算好。</li>
</ul>
</li>
<li>C-Index</li>
<li>LASSO</li>
<li>Nomogram for disease risk stratification<ul>
<li>这是用于实现个体化风险预测的诺模图，阐述不同重要特征的预测能力</li>
<li>回归模型都可以绘制诺模图，画法不说了，感觉就是设置某种映射给特征一个特殊的重要性权重（score）</li>
</ul>
</li>
</ul>
<p>一个和我相关的例子：</p>
<ul>
<li>注意灰度梯度共生特征和灰度曲度共生特征这两个说法！毕竟是某种图像纹理特征，应该还是很有用的！</li>
</ul>
<h2 id="Low-Dose-CT-Quality-Enhancement-with-Ethical-AI"><a href="#Low-Dose-CT-Quality-Enhancement-with-Ethical-AI" class="headerlink" title="Low Dose CT Quality Enhancement with Ethical AI"></a>Low Dose CT Quality Enhancement with Ethical AI</h2><p>由Ge Wang教授，CT领域的领军学者之一给出。老师很喜欢深度方法，举了早期的一些介绍性的工作，用网络做data-image-feature的各种转换。老师的想法和我一样耶，ML与医学的结合。</p>
<ul>
<li>这老师好凶啊，感觉是暴力网络（网络集成）图像重建啊！？！？他喵的真的是任务&amp;实验驱动的科研啊。。。</li>
<li>Artifacts尾影问题<ul>
<li>老师主讲的是LdCT，这个方向还是很火的；</li>
<li>早期会把经典的迭代方法编程到网络中，权值共享使得计算复杂度降低？</li>
<li>他的文章里的网络怎么咔咔咔用了5-6个评价指标；</li>
<li>ADN: disentanglement似乎非常nb；<ul>
<li>设计了很多编码器解码器，提取尾影特征；</li>
<li>设计了5种损失，包括对抗损失2种，重建artifacts损失1种，尾影artifacts一致性损失1种，自优化损失1种，目标函数对应minmax问题；</li>
<li>待改进问题：如何使用先验信息？</li>
</ul>
</li>
</ul>
</li>
<li>分辨率和噪影问题<ul>
<li>分辨率的一种定义方法FWHM，用的啥啥啥点扩散函数。也可以用MTF啥的；</li>
<li>究极定义分辨率公理QwQ；</li>
<li>GAN-Circle；</li>
<li>Deep Resemble Learning；</li>
</ul>
</li>
<li>浪了浪了看小说去了qwq</li>
</ul>
<h2 id="深度重建：基于深度学习的CT图像重建"><a href="#深度重建：基于深度学习的CT图像重建" class="headerlink" title="深度重建：基于深度学习的CT图像重建"></a>深度重建：基于深度学习的CT图像重建</h2><p>由四川大学的Yi Zhang老师给出。主要内容区分为single domain单域和cross domain跨域两者方法。这个域指的是image图像域或者data域，所以到底是什么域？</p>
<p>回去复习一下池化层吧，当时学得不细现在也忘了。</p>
<p>后面和组里的越来越像了。</p>
<p>其中一个VGG+GAN的模型，提特征的思路，包括在target上适用性根据效果来判断，和我的一模一样。。。</p>
]]></content>
      <tags>
        <tag>Medical Images</tag>
        <tag>Xiammt</tag>
      </tags>
  </entry>
  <entry>
    <title>My Journey in FGO</title>
    <url>/2019/12/12/My-Journey-in-FGO/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>我非常非常喜欢玩FGO，所以在此记录美妙瞬间！</p>
<p>Posted on 2019-10-17, updated on 2019-12-12</p>
</blockquote>
<a id="more"></a>
<h3 id="第一个-310-我的小茄子-Mash-Kyrielight"><a href="#第一个-310-我的小茄子-Mash-Kyrielight" class="headerlink" title="第一个 310, 我的小茄子 Mash Kyrielight~!"></a>第一个 <strong>310</strong>, 我的<strong>小茄子</strong> <strong>Mash Kyrielight</strong>~!</h3><img src="/2019/12/12/My-Journey-in-FGO/mash.png" title="Mash Kyrielight">
<h3 id="欢迎入住我的Chaldea-Scathach-Skaði"><a href="#欢迎入住我的Chaldea-Scathach-Skaði" class="headerlink" title="欢迎入住我的Chaldea, Scáthach-Skaði!"></a>欢迎入住我的Chaldea, <strong>Scáthach-Skaði</strong>!</h3><img src="/2019/12/12/My-Journey-in-FGO/skaha.png" title="斯卡哈·斯卡蒂">
<h3 id="完成活动Fate-Accel-Zero-Order-LAP-2！"><a href="#完成活动Fate-Accel-Zero-Order-LAP-2！" class="headerlink" title="完成活动Fate/Accel Zero Order -LAP_2！"></a>完成活动<strong>Fate/Accel Zero Order -LAP_2</strong>！</h3><img src="/2019/12/12/My-Journey-in-FGO/fz2019.png" title="完成活动">
<img src="/2019/12/12/My-Journey-in-FGO/unknowngaonan2.png" title="高难无伤通关">
<h3 id="欢迎小黑和女王梅芙"><a href="#欢迎小黑和女王梅芙" class="headerlink" title="欢迎小黑和女王梅芙~"></a>欢迎小黑和女王梅芙~</h3><img src="/2019/12/12/My-Journey-in-FGO/xiaohei.png" title="小黑黑">
<img src="/2019/12/12/My-Journey-in-FGO/meifu.png" title="梅芙大大~">
<h3 id="欢迎2019万圣二期黄金鬼国Caster酒吞！"><a href="#欢迎2019万圣二期黄金鬼国Caster酒吞！" class="headerlink" title="欢迎2019万圣二期黄金鬼国Caster酒吞！"></a>欢迎2019万圣二期黄金鬼国Caster酒吞！</h3><img src="/2019/12/12/My-Journey-in-FGO/casterjiutun.png" title="Caster酒吞">
<h3 id="日服新征程，纪念圣诞无限池第一次出活动礼装"><a href="#日服新征程，纪念圣诞无限池第一次出活动礼装" class="headerlink" title="日服新征程，纪念圣诞无限池第一次出活动礼装"></a>日服新征程，纪念圣诞无限池第一次出活动礼装</h3><img src="/2019/12/12/My-Journey-in-FGO/japanlizhuang.png" title="不知道名字的礼装">
<h3 id="澡堂奇遇！国服万圣三期，冥界复刻9呼符欧爆！（虽然没我艾蕾酱呜呜呜）"><a href="#澡堂奇遇！国服万圣三期，冥界复刻9呼符欧爆！（虽然没我艾蕾酱呜呜呜）" class="headerlink" title="澡堂奇遇！国服万圣三期，冥界复刻9呼符欧爆！（虽然没我艾蕾酱呜呜呜）"></a>澡堂奇遇！国服万圣三期，冥界复刻9呼符欧爆！（虽然没我艾蕾酱呜呜呜）</h3><img src="/2019/12/12/My-Journey-in-FGO/meidusha.png" title="欢迎小美杜莎！">
<img src="/2019/12/12/My-Journey-in-FGO/meidusha2.png" title="欢迎小美杜莎！">
<img src="/2019/12/12/My-Journey-in-FGO/wuwang.png" title="吾王二宝啦啦啦啦啦啦啦啦啦啦啦啦啦">]]></content>
      <tags>
        <tag>FGO</tag>
      </tags>
  </entry>
  <entry>
    <title>Making Deep Networks Robust to Label Noise: a Loss Correction Approach</title>
    <url>/2019/12/04/Making-Deep-Networks-Robust-to-Label-Noise-a-Loss-Correction-Approach/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>A CVPR 2017 paper</p>
<p>Updated on Dec, 06</p>
</blockquote>
<a id="more"></a>
<h1 id="Making-Deep-Networks-Robust-to-Label-Noise-a-Loss-Correction-Approach"><a href="#Making-Deep-Networks-Robust-to-Label-Noise-a-Loss-Correction-Approach" class="headerlink" title="Making Deep Networks Robust to Label Noise: a Loss Correction Approach"></a>Making Deep Networks Robust to Label Noise: a Loss Correction Approach</h1><h2 id="摘要部分"><a href="#摘要部分" class="headerlink" title="摘要部分"></a>摘要部分</h2><p>问题的背景是数据中的label含有noise，且是与类别独立相关的。大前提是已知每一类的label被弄错成另一种类别的概率，相当于有一个概率转移矩阵。因此label的操作可能会有矩阵求逆，矩阵相乘的操作。</p>
<p>提出一种基于理论的方法来训练神经网络，包括递归网络。想法应该是想办法修正loss，试图估计一下label的概率转移情况。通过这种方法减小noise label的影响，做到稳健性。</p>
<p>另一个结论是，如果只有ReLU作为非线性单元，则loss曲线的情况与noise label的情况无关！这个似乎有点恐怖？！</p>
<h2 id="模型setup"><a href="#模型setup" class="headerlink" title="模型setup"></a>模型setup</h2><p>基本记号：</p>
<ul>
<li>c是正整数，定义$[c] = \{1, \cdots, c\}$</li>
<li>粗体表示列向量或者矩阵</li>
<li>c维的单纯形$\Delta^{c-1}\subset [0, 1]^c$</li>
</ul>
<p>数据生成模型setup，c类监督学习，</p>
<ul>
<li>特征空间$\mathcal{X}\subseteq \mathbb{R}^d$，d维数据</li>
<li>label空间$\mathcal{Y} = \{e^i:i\in [c]\}$，$e^i$是c维向量，元素值为0或1，只有第i维是1，其它都是0，这个维度就对应了label值</li>
</ul>
<p>一个数据$(\boldsymbol{x}, \boldsymbol{y})$生成的方式是未知的分布$p(\boldsymbol{x},\boldsymbol{y}) = p(\boldsymbol{y}|\boldsymbol{x})p(\boldsymbol{x})$。</p>
<p>神经网络模型，n层layers，其实就是一模一样的全连接网络做分类，只是这里它说的数学很详细，可以作为不错的介绍材料。（下面可看可不看）</p>
<p>整个网络$\boldsymbol{h}:\mathcal{X}\rightarrow \mathbb{R}^c$，$\boldsymbol{h} = (\boldsymbol{h}^{(n)} \circ \boldsymbol{h}^{(n-1)} \cdots \boldsymbol{h}^{(1)})$，就是网络是n层的。其中前n-1层网络都是正常的全连接及激活（激活只要求保持一致不变就好），最后一层由于分类的需要就不激活了。其中未知的参数包括每一层的权重矩阵和bias向量。后面不写了，实际上是完全能懂的。最后用的softmax去近似分类条件概率$p(\boldsymbol{y}|\boldsymbol{x})$，并且用交叉熵作为损失函数。<strong>如果概率p加了尖上标，表示预测的结果；弯上标表示转移后的观测标签</strong>。其交叉熵损失是这样定义的</p>
<script type="math/tex; mode=display">l(\boldsymbol{e}^i, \hat{p}(\boldsymbol{y}|\boldsymbol{x})) =  - (\boldsymbol{e}^i)^Tlog\ \hat{p}(\boldsymbol{y}|\boldsymbol{x}) = - log\ \hat{p}(\boldsymbol{y} = \boldsymbol{e}^i|\boldsymbol{x})</script><p>把每个label对应的结果组合起来成为loss向量$\boldsymbol{l} \in \mathbb{R}^c$</p>
<p>后面将证明一些温和条件下loss的结论$\sim$</p>
<h2 id="带噪标签和loss的稳健性"><a href="#带噪标签和loss的稳健性" class="headerlink" title="带噪标签和loss的稳健性"></a>带噪标签和loss的稳健性</h2><p>标签的噪声是不对称或者是不均匀asymmetric的，概率转移是指每个标签$\boldsymbol{y}$都可能以一定概率$p(\tilde{\boldsymbol{y}}|\boldsymbol{y})$变成一个别的标签$\tilde{\boldsymbol{y}}$。</p>
<p>注意这里真实标签是$\boldsymbol{y}$但是观测到的样本标签是$\tilde{\boldsymbol{y}}$。所以我们在拟合的其实是分布$p(\boldsymbol{x}, \tilde{\boldsymbol{y}}) = \sum_{\boldsymbol{y}}p(\tilde{\boldsymbol{y}}|\boldsymbol{y})p(\boldsymbol{y}|\boldsymbol{x})p(\boldsymbol{x})$。其中标签转移的概率表示为noise转移矩阵$T\in [0, 1]^{c\times c}$的元素$T_{ij} = p(\tilde{\boldsymbol{y}} = e^j|\boldsymbol{y} = e^i)$。T只需要满足行和为1。</p>
<p>一般来说实际数据中可能有些类别本身很接近，人也不太能精确识别，因此容易出现标签出错的情况。</p>
<p>我们通过两种修正loss函数的方法达到loss稳健性。</p>
<h2 id="修正1：向后修正"><a href="#修正1：向后修正" class="headerlink" title="修正1：向后修正"></a>修正1：向后修正</h2><p>向后修正loss仍然是原loss的无偏估计，即</p>
<p><strong>定理1</strong> 标签转移矩阵T非奇异，已知原loss函数l，定义向后修正的loss为</p>
<script type="math/tex; mode=display">\boldsymbol{\ell}^{\leftarrow}(\hat{p}(\boldsymbol{y}|\boldsymbol{x})) = T^{-1}\boldsymbol{\ell}(\hat{p}(\boldsymbol{y}|\boldsymbol{x}))</script><p>向后loss是对带噪数据预测的loss，原loss是指干净数据上的loss。称之为向后是指本来干净标签y经T变换成为带噪标签，现在loss上给了个T逆，是一步step back。</p>
<p>它的无偏性是指</p>
<script type="math/tex; mode=display">\forall \boldsymbol{x},\ \mathbb{E}_{\tilde{\boldsymbol{y}}|\boldsymbol{x}}\boldsymbol{\ell}^{\leftarrow}(\hat{p}(\boldsymbol{y}|\boldsymbol{x})) = \mathbb{E}_{\boldsymbol{y}|\boldsymbol{x}}\boldsymbol{\ell}(\hat{p}(\boldsymbol{y}|\boldsymbol{x}))</script><p>因此其极小，即神经网络期望的优化结果minimizer相等：</p>
<script type="math/tex; mode=display">\mathop{\arg\min}\limits_{\hat{p}(\boldsymbol{y}|\boldsymbol{x})} \mathbb{E}_{\tilde{\boldsymbol{y}}|\boldsymbol{x}}\boldsymbol{\ell}^{\leftarrow}(\hat{p}(\boldsymbol{y}|\boldsymbol{x})) = \mathop{\arg\min}\limits_{\hat{p}(\boldsymbol{y}|\boldsymbol{x})} \mathbb{E}_{\boldsymbol{y}|\boldsymbol{x}}\boldsymbol{\ell}(\hat{p}(\boldsymbol{y}|\boldsymbol{x}))</script><p><strong>证明</strong> 只需证明无偏性</p>
<script type="math/tex; mode=display">\begin{align} \mathbb{E}_{\tilde{\boldsymbol{y}}|\boldsymbol{x}}\boldsymbol{\ell}^{\leftarrow}(\hat{p}(\boldsymbol{y}|\boldsymbol{x})) &= \mathbb{E}_{\boldsymbol{y}|\boldsymbol{x}} T\boldsymbol{\ell}^{\leftarrow}(\hat{p}(\boldsymbol{y}|\boldsymbol{x}))\\ &= \mathbb{E}_{\boldsymbol{y}|\boldsymbol{x}} T T^{-1}\boldsymbol{\ell}(\hat{p}(\boldsymbol{y}|\boldsymbol{x}))\\ &= \mathbb{E}_{\boldsymbol{y}|\boldsymbol{x}}\boldsymbol{\ell}(\hat{p}(\boldsymbol{y}|\boldsymbol{x}))\end{align}</script><p>式子中第一步是换元，T是两个元之间的转化关系；</p>
<p>第二步是loss的def。$\blacksquare$</p>
<p>注意T逆一般可以算，但是可能条件数很大，比较难算。所以修改一下T，让他好算点比较舒服。</p>
<h2 id="修正2：向前修正"><a href="#修正2：向前修正" class="headerlink" title="修正2：向前修正"></a>修正2：向前修正</h2><p><strong>定理2</strong> 标签转移矩阵T非奇异，已知原loss函数$\boldsymbol{\ell}_{\boldsymbol{\psi}}$且是proper composite的。定义向前修正的loss为</p>
<script type="math/tex; mode=display">\boldsymbol{\ell}^{\rightarrow}_{\boldsymbol{\psi}}(\boldsymbol{h}(\boldsymbol{x})) = \boldsymbol{\ell}(T^{T}\psi^{-1}(\boldsymbol{h}(\boldsymbol{x})))</script><p>向前loss</p>
<p>这里没有了无偏性，但是期望的minimizer仍然相等（一眼看不出来的）</p>
<script type="math/tex; mode=display">\mathop{\arg\min}\limits_{\boldsymbol{h}} \mathbb{E}_{\boldsymbol{x}, \tilde{\boldsymbol{y}}}\boldsymbol{\ell}^{\rightarrow}_{\boldsymbol{\psi}}(\boldsymbol{y}, \boldsymbol{h}(\boldsymbol{x})) = \mathop{\arg\min}\limits_{\boldsymbol{h}} \mathbb{E}_{\boldsymbol{x}, \boldsymbol{y}}\boldsymbol{\ell}_{\boldsymbol{\psi}}(\boldsymbol{y}, \boldsymbol{h}(\boldsymbol{x}))</script><p><strong>证明</strong> 向前loss利用了交叉熵的一个性质</p>
<p>预测标签$\hat{p}(\boldsymbol{y}|\boldsymbol{x})$和任一标签$e^i$之间的交叉熵，真实标签$e^j$，但是观测标签$\tilde{\boldsymbol{y}}$却在干扰</p>
<script type="math/tex; mode=display">\begin{align} \boldsymbol{\ell}(e^i, \hat{p}(\boldsymbol{\tilde{y}}|\boldsymbol{x})) &= - (\boldsymbol{e}^i)^Tlog\ \hat{p}(\boldsymbol{\tilde{y}}|\boldsymbol{x})\\ &= - log\ \hat{p}(\boldsymbol{\tilde{y}} = \boldsymbol{e}^i|\boldsymbol{x})\\ &= -\log \sum_{j=1}^{c} p\left(\tilde{\boldsymbol{y}}=e^{i} | \boldsymbol{y}=e^{j}\right) \hat{p}\left(\boldsymbol{y}=e^{j} | \boldsymbol{x}\right)\\ &= -\log \sum_{j=1}^{c} T_{j i} \hat{p}\left(\boldsymbol{y}=e^{j} | \boldsymbol{x}\right)\end{align}</script><p>用矩阵的形式就是</p>
<script type="math/tex; mode=display">\ell(\hat{p}(\boldsymbol{y} | \boldsymbol{x}))=-\log T^{\top} \hat{p}(\boldsymbol{y} | \boldsymbol{x})</script><p>这样的话，<strong>loss被T处理了，就叫做向前</strong></p>
<p>在定理中，loss是proper composite，带来一个性质：</p>
<p>给定可逆的link函数$\psi: \Delta^{c-1} \rightarrow \mathbb{R}^{c}$，则复合loss定义为$\ell_{\psi}$，$\ell_{\boldsymbol{\psi}}(\boldsymbol{y}, \boldsymbol{h}(\boldsymbol{x}))=\ell\left(\boldsymbol{y}, \boldsymbol{\psi}^{-1}(\boldsymbol{h}(\boldsymbol{x}))\right)$，这里softmax就是$\boldsymbol{\psi}^{-1}$。它对应minimizer的性质$\underset{h}{\operatorname{argmin}} \mathbb{E}_{\boldsymbol{x}, \boldsymbol{y}} \ell_{\boldsymbol{\psi}}(\boldsymbol{y}, \boldsymbol{h}(\boldsymbol{x}))=\psi(p(\boldsymbol{y} | \boldsymbol{x}))$。</p>
<p>$\therefore$由link函数的定义，向前loss表示为</p>
<script type="math/tex; mode=display">\ell \vec{\psi}(\boldsymbol{y}, \boldsymbol{h}(\boldsymbol{x}))=\ell\left(\boldsymbol{y}, T^{\top} \boldsymbol{\psi}^{-1}(\boldsymbol{h}(\boldsymbol{x}))\right)=\ell_{\boldsymbol{\phi}}(\boldsymbol{y}, \boldsymbol{h}(\boldsymbol{x}))</script><p>新的符号$\phi^{-1}=\psi^{-1} \circ T^{\top}$，$\therefore \mathbb{E}_{\boldsymbol{x}, \tilde{\boldsymbol{y}}} \boldsymbol{\ell}^{\rightarrow}_{\boldsymbol{\psi}}(\boldsymbol{y}, \boldsymbol{h}(\boldsymbol{x})) =  \mathbb{E}_{\boldsymbol{x}, \tilde{\boldsymbol{y}}} \ell_{\boldsymbol{\phi}}(\boldsymbol{y}, \boldsymbol{h}(\boldsymbol{x})) $</p>
<p>得到$\underset{\boldsymbol{h}}{\operatorname{argmin}} \mathbb{E}_{\boldsymbol{x}, \tilde{\boldsymbol{y}}} \ell_{\boldsymbol{\phi}}(\boldsymbol{y}, \boldsymbol{h}(\boldsymbol{x}))=\phi(p(\tilde{\boldsymbol{y}} | \boldsymbol{x}))$<br>$=\psi\left(\left(T^{-1}\right)^{\top} p(\tilde{\boldsymbol{y}} | \boldsymbol{x})\right)=\psi(p(\boldsymbol{y} | \boldsymbol{x}))$</p>
<p>$\blacksquare$</p>
<p>结论是向前修正的loss得到的minimizer与是干净数据上的一样。link函数起到了此作用，不用关心noise的水平。</p>
<h2 id="整个算法"><a href="#整个算法" class="headerlink" title="整个算法"></a>整个算法</h2><p>哎呀懒得写了，大概的意思就是说假设每一类都至少有一个正确标签的观测样本，且softmax的确近似了T中的元素。</p>
<p>既然一般T不知道，我们先在noisy数据上xjb练一波，用练好的分类器去试试一些unlabelled的数据，把T估计一下，估计的方法就是默认真实标签是概率最大的那个，其它累死了。</p>
<p>这样有了T之后可以用向前或者向后修正的loss重新刚数据集啦！！</p>
]]></content>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust Probabilistic Modeling with Bayesian Data Reweighting</title>
    <url>/2019/11/27/Robust%20Probabilistic%20Modeling%20with%20Bayesian%20Data%20Reweighting/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>A paper reading</p>
</blockquote>
<a id="more"></a>
<h1 id="Robust-Probabilistic-Modeling-with-Bayesian-Data-Reweighting"><a href="#Robust-Probabilistic-Modeling-with-Bayesian-Data-Reweighting" class="headerlink" title="Robust Probabilistic Modeling with Bayesian Data Reweighting"></a>Robust Probabilistic Modeling with Bayesian Data Reweighting</h1><h2 id="这篇文章大致讲了什么？"><a href="#这篇文章大致讲了什么？" class="headerlink" title="这篇文章大致讲了什么？"></a>这篇文章大致讲了什么？</h2><p>在概率模型PM（这里含隐变量$\beta$）的框架下，现实中很多数据data往往与假设的分布不符或者有些许偏差，导致概率模型做出的推断或者预测效果不太好。因此我们希望能够检测出这种不匹配mismatch的性质，并由此给每个观测observation样本加一个权重，这样模型会对不好的数据有更好的稳健性。权重和隐变量将一起由推断得到（类似meta weight net那样的）。</p>
<h2 id="大概的算法是什么？"><a href="#大概的算法是什么？" class="headerlink" title="大概的算法是什么？"></a>大概的算法是什么？</h2><p>首先提供的数据是有N个独立观测的样本的数据集$y = (y_1, \cdots, y_n)$。</p>
<ul>
<li>首先给定基本的概率模型PM，其对应的联合分布是$p_\beta(\beta)\prod_{n=1}^{N}l(y_n|\beta)$，$l(y_n)$是数据$y_n$的可能性likelihood，由此PM给定，$\beta$是隐变量，它有一个先验$p_\beta(\beta)$。</li>
<li>加权后的模型定义成reweighted probabilistic model，即RPM，权重$w_n$的先验是$p_w(w)$。新的RPM是<script type="math/tex">p(y,\beta, w) = \frac{1}{Z}p_w(w)p_\beta(\beta)\prod_{n=1}^{N}l(y_n|\beta)^{w_n}</script>，其中$Z$是对应的归一化常数。</li>
<li>最后同时推断隐变量和权重参数$\beta$和$w$，对应的后验是$p(\beta, w|y)$。</li>
</ul>
<p>为什么加权加在可能性似然的次幂上？我们考虑对联合分布取对数：</p>
<script type="math/tex; mode=display">log\ p(y,\beta, w) \varpropto log\ p_w(w) + log\ p_\beta(\beta) + \sum_{n=1}^{N}w_nlog\ l(y_n|\beta)</script><p>后验推断其实近似为最大化上面的对数联合概率。当某个观测样本$y_i$不太服从假设分布时，我们认为其可能性$l(y_i)$比较低，趋近于0，那么其对数$log\ l(y_i|\beta)$的绝对值为很小的负数，因此为了增大上式，需要使其权重$w_i$变小，即有问题的数据的权重要小一些；反之亦然。好像还有一种解释是把上式最后一项看成是loss，前面两项看成正则，具体怎么分析我忘啦…</p>
<h2 id="案例研究Case-Study"><a href="#案例研究Case-Study" class="headerlink" title="案例研究Case Study"></a>案例研究Case Study</h2><p>这个案例是Poisson因子分解模型，去做电影推荐。</p>
<h1 id="Automated-Identification-of-Chromosome-SegmentsInvolved-in-Translocations-by-Combining-Spectral-Karyotyping-and-Banding-Analysis"><a href="#Automated-Identification-of-Chromosome-SegmentsInvolved-in-Translocations-by-Combining-Spectral-Karyotyping-and-Banding-Analysis" class="headerlink" title="Automated Identification of Chromosome SegmentsInvolved in Translocations by Combining Spectral Karyotyping and Banding Analysis"></a>Automated Identification of Chromosome SegmentsInvolved in Translocations by Combining Spectral Karyotyping and Banding Analysis</h1><p>这篇文章讲的是DTW，即动态时间规划算法在染色体条带匹配上的应用。</p>
]]></content>
      <tags>
        <tag>Papers Reading</tag>
      </tags>
  </entry>
  <entry>
    <title>Programming Bugs Records</title>
    <url>/2019/11/24/Programming-Bugs-Records/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>单独开一个blog来记录各式各样的bugs…</p>
</blockquote>
<a id="more"></a>
<h1 id="编程bugs记录"><a href="#编程bugs记录" class="headerlink" title="编程bugs记录"></a>编程bugs记录</h1><blockquote>
<p>这篇博客谨记录我所遇到过的bugs。<br>它包括两个部分，一个是bugs记录，另一个是techniques记录。</p>
</blockquote>
<h2 id="Bugs"><a href="#Bugs" class="headerlink" title="Bugs"></a>Bugs</h2><h3 id="LaTeX-Bugs"><a href="#LaTeX-Bugs" class="headerlink" title="$\LaTeX$ Bugs"></a>$\LaTeX$ Bugs</h3><h4 id="1-caption-outside-float"><a href="#1-caption-outside-float" class="headerlink" title="1. caption outside float"></a>1. caption outside float</h4><p>记录于2019/08/25，</p>
<p>为以下代码报的错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\renewcommand&#123;\algorithm&#125;&#123;算法&#125;</span><br></pre></td></tr></table></figure>
<p>且我在为算法添加标题时用了中文标题，然后就报错了…</p>
<p>解决方案是注释（删了）这行，就可以用中文标题了…</p>
<h4 id="2-Something’s-wrong—perhaps-a-missing-item"><a href="#2-Something’s-wrong—perhaps-a-missing-item" class="headerlink" title="2.Something’s wrong—perhaps a missing \item."></a>2.Something’s wrong—perhaps a missing \item.</h4><p>Bug是</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LaTeX Error: Something&apos;s wrong--perhaps a missing \item.</span><br></pre></td></tr></table></figure>
<p>网上有的说法是参考文献的问题，而我的是因为使用列表环境<code>\begin{enumerate}[(1)]</code>时没有添加<code>\usepackage{enumerate}</code>。</p>
<h3 id="CV2-Bugs"><a href="#CV2-Bugs" class="headerlink" title="CV2 Bugs"></a>CV2 Bugs</h3><h4 id="1-FindContours-supports-only-CV-8UC1-images"><a href="#1-FindContours-supports-only-CV-8UC1-images" class="headerlink" title="1. FindContours supports only CV_8UC1 images"></a>1. FindContours supports only CV_8UC1 images</h4><p>记录于2019/09/02，</p>
<p>Bug是</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[start]FindContours supports only CV_8UC1 images when mode != CV_RETR_FLOODFILL otherwise supports CV_32SC1 images only in function cvStartFindContours_Impl</span><br></pre></td></tr></table></figure>
<p>It appears when I want to find a contour using cv2 fuctions after adding some dilation operations, and the solution is just using</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">image = np.clip(image, 0, 255)</span><br><span class="line">image = np.array(image,np.uint8)</span><br></pre></td></tr></table></figure>
<p>The reference comes from <a href="https://blog.csdn.net/u010030977/article/details/81214145" target="_blank" rel="noopener">https://blog.csdn.net/u010030977/article/details/81214145</a></p>
<h4 id="2-UMat-for-argument-‘src’"><a href="#2-UMat-for-argument-‘src’" class="headerlink" title="2. UMat for argument ‘src’"></a>2. UMat for argument ‘src’</h4><p>Recorded on 8th, Aug, 2019.</p>
<p>The bug is</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TypeError: Expected cv::UMat for argument &apos;src&apos;</span><br></pre></td></tr></table></figure>
<p>It appears because I open an image with <code>PIL</code>, then try to convert it to RGB using <code>cv2</code>…</p>
<h2 id="Techniques"><a href="#Techniques" class="headerlink" title="Techniques"></a>Techniques</h2><h3 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h3><h4 id="1-get-class-probabilities"><a href="#1-get-class-probabilities" class="headerlink" title="1. get class probabilities"></a>1. get class probabilities</h4><p>Recorded on 25th, August, 2019.</p>
<p>When designing a DNN for classification, sometimes we want to see what the probability of the issue whether a test sample belongs to a certain class is.</p>
<p>Before this, we need the output of the net, usually a list of numbers.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">net = net.cuda()</span><br><span class="line">input = input.cuda()</span><br><span class="line">outputs = net(input)# the output of the net</span><br><span class="line"></span><br><span class="line">p = torch.nn.functional.softmax(outputs, dim=1)# `p` is what we want and its sum is 1</span><br><span class="line">_, predicted = torch.max(outputs.data, 1)# `predicted` is the order because we get the max in `p`</span><br><span class="line">classes[predicted.cpu()]# the class name</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title>How to Build Your Own Blog as A Newbie</title>
    <url>/2019/10/20/How-to-Build-Your-Own-Blog-as-A-Newbie/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>新手如何在<strong>Windows系统</strong>上搭建自己的hexo主题博客？这篇blog就将展示<a href="https://maxliu245.github.io/">Max Liu’s Blog</a>的搭建历程！</p>
<p>作为<strong>git小白</strong>，我一度对hexo有某种未知的畏惧感，即不敢上手。但是在萌萌哒的二次元插件的诱惑下我还是决定从原先的jekyll过渡为hexo。在搭建的过程中也遇到了很多问题，其中对于我来说关键的比较耗时间查找的困难，我都将一一列出解决方法。</p>
</blockquote>
<a id="more"></a>
<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>即便我是git小白，但是基础的<strong>敲命令行代码的能力</strong>还是有的，因此推荐大家对GitHub比较了解，至少会搜索GitHub再搭建博客，否则很浪费时间，绝对不是3分钟能解决的。</p>
<p>在搭建博客之前，有如下基本要求，这也是我搭建之前已经准备好的：</p>
<ul>
<li>安装git</li>
<li>拥有自己的GitHub账号</li>
</ul>
<h2 id="一步步完善"><a href="#一步步完善" class="headerlink" title="一步步完善"></a>一步步完善</h2><h3 id="在电脑上搭建基本环境"><a href="#在电脑上搭建基本环境" class="headerlink" title="在电脑上搭建基本环境"></a>在电脑上搭建基本环境</h3>]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/10/17/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>This is an initial doc of <strong>hexo</strong>.</p>
</blockquote>
<a id="more"></a>
<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>膨胀与腐蚀法——数学形态学算子</title>
    <url>/2019/07/03/%E8%86%A8%E8%83%80%E4%B8%8E%E8%85%90%E8%9A%80%E6%B3%95%E2%80%94%E2%80%94%E6%95%B0%E5%AD%A6%E5%BD%A2%E6%80%81%E5%AD%A6%E7%AE%97%E5%AD%90/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p> Dilation and erosion intro </p>
</blockquote>
<a id="more"></a>
<h1 id="膨胀与腐蚀法——数学形态学算子"><a href="#膨胀与腐蚀法——数学形态学算子" class="headerlink" title="膨胀与腐蚀法——数学形态学算子"></a>膨胀与腐蚀法——数学形态学算子</h1><blockquote>
<p>Note at the beginning:</p>
<ul>
<li>为什么学习这个方法：这个方法是基于2019 Summer Session的学习所需</li>
<li>资源来自何处：谷歌搜索到的资源<a href="http://www.cs.uu.nl/docs/vakken/ibv/reader/chapter6.pdf" target="_blank" rel="noopener">Chapter 6 Mathematical morphology</a>且只学习了部分内容</li>
<li>其它参考链接：<ul>
<li><a href="https://www.cnblogs.com/slysky/archive/2011/10/16/2214015.htm" target="_blank" rel="noopener">图像的膨胀与腐蚀、细化</a></li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><blockquote>
<p>Studied on 1st, July, posted on 3rd.</p>
</blockquote>
<p>【<strong><code>数学形态学</code></strong>】</p>
<p>数学形态学是研究形态的学科，用集合的方法来描述某种形态。</p>
<p>它被广泛应用在图像处理上，有两点需要注意：</p>
<p>注意它的算子，尤其是膨胀和腐蚀一般都不是线性的。</p>
<p>它研究的图像上的元素需要满足可比较的序关系，这样才能进行相关的运算。</p>
<p>【<strong><code>The Maxima And Minima Image</code></strong>】</p>
<p>“<code>maximum image</code>” 和 “<code>minimum image</code>”是关于两张图片的定义，即新的图片在某像素点位置的值分别取最大和最小。</p>
<script type="math/tex; mode=display">(max\{f, g\})(x) = max\{f(x), g(x)\}</script><script type="math/tex; mode=display">(min\{f, g\})(x) = min\{f(x), g(x)\}</script><p>其中$x$即为对应像素点的位置。</p>
<p>在提及膨胀和腐蚀算子之前先要声明它们属于数学形态学算子。</p>
<p>【<strong><code>数学形态学算子</code></strong>】</p>
<p>它描述的是一张图像与一个结构元素之间的关系。结构元素一般比图像小很多，一般是十字形，方形或者圆形的二值区域，也一般取对称的区域。</p>
<blockquote>
<p>声明：这篇文章使用资源中的约定，即在提及二值图像的时候，黑色像素指的是目标像素，灰度为1；而白色像素指的是背景像素，灰度为0。</p>
</blockquote>
<h2 id="基本算子"><a href="#基本算子" class="headerlink" title="基本算子"></a>基本算子</h2><p>【<strong><code>互补算子</code></strong>】</p>
<p>互补沿用集合论的符号$c$，假设一张图片$X$在像素$(x,y)$位置上的函数值为$f(x,y)$，其范围$range=[L,\cdots,M]$。则$X^c$的对应位置满足$f^c(x,y) = L+M-f(x,y)$。</p>
<blockquote>
<p>一个最简单的例子是二值图像黑白颠倒。</p>
</blockquote>
<p>【<strong><code>膨胀算子</code></strong>】</p>
<p>一张二值图像$X$与一个结构元素$S$的闵科夫斯基和$\oplus$就是一个膨胀操作：</p>
<script type="math/tex; mode=display">\delta(X) = X\oplus S = \{x+s\mid x\in X\ \and\ s\in S\}</script><blockquote>
<p>资源里给的例子算得很清楚，是根据位置操作的。<br>我已经看懂了。但是很容易弄混，建议忘了复习一下。</p>
</blockquote>
<p>【<strong><code>腐蚀算子</code></strong>】</p>
<p>它是膨胀算子的对偶算子。一张二值图像$X$与一个结构元素$S$的闵科夫斯基差$\ominus$是一个腐蚀操作：</p>
<script type="math/tex; mode=display">\epsilon(X) = X\ominus S = \{x\mid \forall s\in S,\ x+s\in X\}</script><blockquote>
<p>资源里给的也是根据位置操作的。<br>虽然资源里没有说，但是我觉得即使原图像不是二值图像也可以用。</p>
</blockquote>
<p>通俗地说：</p>
<p>把图像看作食物，膨胀法相当于在边缘吐出食物，图像会扩大一点点；腐蚀法相当于在边缘吃掉食物，所以图像会缩小一点点。</p>
<p>【<strong><code>算子的对偶</code></strong>】</p>
<p>对偶即<code>dual</code>，两个算子对偶要满足以下式子：</p>
<p>$\{\forall f,\ subject\ to\ \varphi_1(f^c)=(\varphi_2(f))^c\}$</p>
<blockquote>
<p>资源里举了两个例子，这里不写，知道这个式子成立即可。</p>
</blockquote>
<h2 id="组合算子"><a href="#组合算子" class="headerlink" title="组合算子"></a>组合算子</h2><p>组合算子只介绍膨胀和腐蚀算子的组合，即开闭运算。</p>
<p>【<strong><code>开运算</code></strong>】</p>
<p>开运算<code>Opening</code>定义为$\gamma(f) = \delta(\epsilon(f))$。</p>
<p>先腐蚀再膨胀，就是开运算。开运算能消除小的<code>object</code>即孤立点。</p>
<p>【<strong><code>闭运算</code></strong>】</p>
<p>闭运算<code>Closing</code>定义为$\varphi(f) = \epsilon(\delta(f))$</p>
<p>先膨胀再腐蚀，就是闭运算。同理，闭运算能填充图像中的小洞。</p>
<h2 id="代码研究"><a href="#代码研究" class="headerlink" title="代码研究"></a>代码研究</h2><p>将在<code>ipynb</code>文件中进行研究。</p>
<blockquote>
<p>代码及分水岭算法均在3rd, July晚上完成，但尚未上传，计划日后一并上传。</p>
</blockquote>
]]></content>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Mathematical Morphology</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Sustainability: Relevant to Most Modern Fields</title>
    <url>/2019/04/22/Computational-Sustainability-Relevant-to-Most-Modern-Fields/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>A further studying of Computational Sustainability</p>
</blockquote>
<a id="more"></a>
<h2 id="Computational-Sustainability-Relevant-to-Most-Modern-Fields"><a href="#Computational-Sustainability-Relevant-to-Most-Modern-Fields" class="headerlink" title="Computational Sustainability: Relevant to Most Modern Fields"></a>Computational Sustainability: Relevant to Most Modern Fields</h2><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>Computational sustainability, which I have introduced in my previous posting, is a combination of math and sustainability. Since I’m a math student and I love computers, I’m curious about what computers can do for sustainability. Several months ago, I began to search this on Google where I found the definition of computational sustainability. After that, I found this article and obtained more information. From the article, I learned what we really need to do for a better world.</p>
<img src="/2019/04/22/Computational-Sustainability-Relevant-to-Most-Modern-Fields/big-data.jpg" title="Big data">
<h3 id="2-Summary"><a href="#2-Summary" class="headerlink" title="2. Summary"></a>2. Summary</h3><p>In “<a href="https://www.nae.edu/17286/Computational-Sustainability-Computational-Methods-for-a-Sustainable-Environment-Economy-and-Society-" target="_blank" rel="noopener">Computational Sustainability: Computational Methods for a Sustainable Environment, Economy, and Society</a>“ (2011, Ithaca, New York), Carla P. Gomes in Cornell University claimed the idea of computational sustainability in detail. Before proposing the definition, he told us that there was a dramatic growth using natural resources, leading to a rising alarming level. To be honest, people have concerned about this since 1987, even including the United Nations. However, how does the issue concerns with math? In modern times, to make decisions relies on <strong>computers</strong>, which brings a new field, i.e. computational sustainability. It is because modern projects contains a big data and complicated models. To further demonstrate how computers develop now methods to help, the author described several modern cases and I list them as follows:</p>
<ul>
<li>Biodiversity and Species Conservation: <ul>
<li>For example, researchers formulated connection sub-graph problem in order to choose animals’ conservation site optimally with limited resources.</li>
</ul>
</li>
<li>Natural Resource Management: <ul>
<li>Mostly we have to build a special dynamic system, usually nonlinear. In this case, we rely on computers to help us analyze the system in terms of long-term stability and possible bifurcation properties. These are actually the insights of the dynamic system.</li>
</ul>
</li>
<li>Balancing Socioeconomic Needs and the Environment: <ul>
<li>It tends to be a predictive problem. To study such systems, researchers usually developed a machine learning model and generate a huge amount of key parameters, further made predictions quite accurately.</li>
</ul>
</li>
<li>Increasing Energy Efficiency and Renewable Energy (A big part):<ul>
<li>Usually researchers developed a complexed sensor system to collect data. In order to maximize the information, they need to solve some kind of complex optimization problem. Optimization is basically difficult because they need to choose an optimal algorithm and do marvelous calculation.</li>
</ul>
</li>
<li>Other Research Areas and Themes:<ul>
<li>This is a big question because computational sustainability is indeed an interdisciplinary term. Since experts in a wide range of fields come together and work for a same problem, we can foresee that it has still a long road to go.</li>
</ul>
</li>
</ul>
<p>At the end of the article, the author did a brief summary, which told us computational sustainability is a new field , including many fields, math especially. Since we can not avoid the big data or complex system in modern projects, we have to develop new methods or models using computer. Only with a more efficient and powerful method, we can manage to balance the allocation of environmental, economic and social resources.</p>
<blockquote>
<p>Note: Since it’s not a short article, so I write a long section to sort out the roots and leaves (梳理脉络).</p>
</blockquote>
<h3 id="3-Response"><a href="#3-Response" class="headerlink" title="3. Response"></a>3. Response</h3><p>Overall, I consider this article as a simple scientific passage because it mainly introduce some examples from different subjects. Nevertheless, it’s clear enough to explain what computational sustainability is. And what I appreciate a lot is that it tells us about how we apply that into modern projects. These examples illustrate what we can do on earth by combining sustainability with computer and further more, what we’re expected to do in the future.</p>
<p>Before I read about the fantastic combination, I have thought about the high performance algorithms, which is also a key component in achieving computational sustainability. Although there’s still a long way to go, I prefer to work for a better world.</p>
<p>Lastly, I’d like to mention <a href="http://earthday.gatech.edu/" target="_blank" rel="noopener">Georgia Tech’s 22nd Annual Earth Day Festival</a> last Friday. I worked for the festival for 3 hours as a volunteer. I felt happy that more and more people joined the group of sustainability. So why not study more about sustainability? Even though you still don’t know what to do, just search what it concerns about your own major and <a href="https://www.michaeljackson.com/video/heal-world-video/" target="_blank" rel="noopener">make a little better world</a>!</p>
<img src="/2019/04/22/Computational-Sustainability-Relevant-to-Most-Modern-Fields/Michael-Jackson-It-all-begins-with-forgiveness-because-to-heal-the-world-we-first-have-to.jpg" title="Heal the world">
<h3 id="4-References"><a href="#4-References" class="headerlink" title="4. References"></a>4. References</h3><p>[1] Gomes C.P. (2011) Computational Sustainability. In: Gama J., Bradley E., Hollmén J. (eds) Advances in Intelligent Data Analysis X. IDA 2011. Lecture Notes in Computer Science, vol 7014. Springer, Berlin, Heidelberg</p>
<p>[2] Gomes, C.P.: Computational Sustainability: Computational methods for a sustainable environment, economy, and society. The Bridge, National Academy of Engineering 39(4) (Winter 2009)</p>
<p>[3]Liu, M. (2019). A Brief Studying of Computational Sustainability - Max Liu’s Blog | Github Blog. Retrieved from <a href="https://maxliu245.github.io/2019/04/14/A-Brief-Studying-of-Computational-Sustainability/">https://maxliu245.github.io/2019/04/14/A-Brief-Studying-of-Computational-Sustainability/</a></p>
<blockquote>
<p>Notations</p>
<ul>
<li>The big data picture comes from <a href="https://commons.wikimedia.org/wiki/File:Large-895567_1920.jpg" target="_blank" rel="noopener">wikimedia</a>, which is an open source.</li>
<li>The “<a href="https://quotefancy.com/quote/867277/Michael-Jackson-It-all-begins-with-forgiveness-because-to-heal-the-world-we-first-have-to" target="_blank" rel="noopener">MJ heal the world</a>“ picture comes from the Internet, which is an open picture website “quotefancy”.</li>
<li>The header image is free to share given by <a href="http://ramok.tech/blog-header-machine-learning/" target="_blank" rel="noopener">Klevis Ramo</a>.</li>
</ul>
</blockquote>
]]></content>
      <tags>
        <tag>Sustainable Service Learning</tag>
        <tag>Computational Sustainability</tag>
      </tags>
  </entry>
  <entry>
    <title>A Brief Studying of Computational Sustainalility</title>
    <url>/2019/04/14/A%20Brief%20Studying%20of%20Computational%20Sustainalility/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>This passage is my general idea about <strong>computational sustainablility</strong>.</p>
</blockquote>
<a id="more"></a>
<h2 id="A-Brief-Studying-of-Computational-Sustainability"><a href="#A-Brief-Studying-of-Computational-Sustainability" class="headerlink" title="A Brief Studying of Computational Sustainability"></a>A Brief Studying of Computational Sustainability</h2><h3 id="1-Introduction-of-SUS"><a href="#1-Introduction-of-SUS" class="headerlink" title="1. Introduction of SUS"></a>1. Introduction of SUS</h3><p>Sustainability, briefly named as SUS, has been a great issue for a long time. Most people agree that it contains 3 parts: environmental, economic and social sustainability. And this is the most common agreement of SUS.</p>
<p>From my point of view, SUS is quite important because it cares a lot about the properties of human ourselves. To reach the following 3 goals is to help protect ourselves, anyway.</p>
<img src="/2019/04/14/A%20Brief%20Studying%20of%20Computational%20Sustainalility/sus.png" title="SUS">
<h3 id="2-Computational-SUS"><a href="#2-Computational-SUS" class="headerlink" title="2. Computational SUS"></a>2. Computational SUS</h3><p>However, what if we combine sustainability with some certain subjects? This is an interesting question and there are certain definitions on the Internet. This week, I read some articles about computational sustainability and what I concerned most is SUS with mathematics.</p>
<p><a href="https://en.wikipedia.org/wiki/Computational_sustainability" target="_blank" rel="noopener">Computational sustainability</a> is a kind of SUS with mathematics, which is actually a combination of several fields, including mathematics, statistics, information, economy, biology, environment, etc. When designing new sustainable techniques or products to make reasonable decisions in our real life, people are tend to develop complex computational models, methods and tools in order to make decisions over several subjects. During the process, people have to consider government’s policies, the management and allocation of natural resources, and also the mathematical components.</p>
<p>As for now, computational SUS is a popular issue, and it appears in many <a href="https://www.cs.ubc.ca/~mack/ComputationalSustainabilityResources/" target="_blank" rel="noopener">top seminars</a>  and have formed its <a href="https://www.computational-sustainability.org/" target="_blank" rel="noopener">own group</a>. More and more people in these fields began to consider SUS when dealing with their own works, which is a great trend showing that people care more about sustainable development.</p>
<p>In this case, I’d like to focus on the part related with my field, which is algorithm and calculation on the big data, i.e. to deal with a high dimension data with fast and efficient algorithms. Most of the time, we are facing some restrictions from natural resources, energy and expenses. To tell the truth, most missions now require a big data, which needs high powerful calculation machines, superior computer groups. As is mentioned in the text, when dealing with a modern system, like an intelligent city, a well-managed farm, we have to consider</p>
<ul>
<li>energy saving</li>
<li>expenses declining</li>
<li>environment protecting</li>
<li>TBD (to be determined)</li>
</ul>
<p>That is to say, even dealing with a simple modern system, it’s indeed extremely complicated. To obtain an optimal designing of a system, we have to collect lots of data, usually at least several TBs of even PBs. Then we have to analyze the data using computers. However, this require high performance computing, including fast and efficient algorithms and high performance computing groups. The mathematical techniques have to be concluded before we can fetch our research results.</p>
<h3 id="3-An-Example"><a href="#3-An-Example" class="headerlink" title="3. An Example"></a>3. An Example</h3><p>Let’s take the recent <a href="https://www.weibo.com/ttarticle/p/show?id=2309351000044359683002479630&amp;u=6033196789&amp;m=4361221496879170&amp;cu=undefined" target="_blank" rel="noopener">black hole</a> (refer to 果壳 on WeChat) picture as an example. It is a great progress that scientists took a “photo” of the black hole for the first time. To tell the truth, the real black hole cannot be seen by humans because it has such a big mass that light cannot escape from it and reaches our eyes. What was token as a photo is indeed the event horizon. This is a border because no light will go out inside this horizon, which is a critical case.</p>
<img src="/2019/04/14/A%20Brief%20Studying%20of%20Computational%20Sustainalility/BH1.jpg" title="A Black Hole">
<p>So what does it relate to sustainability? We know that a huge amount of data, up to 2 PB each day were produced when observing the black hole, but how could scientists send these data to analytic institute and how to deal with it? The former part concerns about store hardware and transportation, which are economic part. They used a lot of disks and also network transportation. They even waited for the summer of the observing plot for the sake of convenience. The later part concerns with programing and algorithms. To tell the truth, scientists took about 2 years to generate just one photo. So they need to build a fast algorithm with less complexity and deal with the data on a certain service platform, which is a kind of powerful calculation platform provided by a huge amount of money, electricity and also a large area of calculation machines.</p>
<p>With this experience, scientists can design a better system to deal with the black hole in the future. Therefore, we can proudly say that computational SUS takes a vital role to generate such a photo! An in the future, computational SUS can help more when doing researches!</p>
<h3 id="4-References"><a href="#4-References" class="headerlink" title="4. References"></a>4. References</h3><p>[1] Gomes C.P. (2011) Computational Sustainability. In: Gama J., Bradley E., Hollmén J. (eds) Advances in Intelligent Data Analysis X. IDA 2011. Lecture Notes in Computer Science, vol 7014. Springer, Berlin, Heidelberg</p>
<p>[2] Gomes, C.P.: Computational Sustainability: Computational methods for a sustainable environment, economy, and society. The Bridge, National Academy of Engineering 39(4) (Winter 2009)</p>
<p>[3]果壳.黑洞你好：第一次拍到你前，我已经无数次描绘过你的样子 热点[EB/OL]</p>
<p>Notations</p>
<ul>
<li>The SUS picture comes from GaTech.</li>
<li>The black hole picture comes from the Internet with an open source permission.</li>
<li>The blog heading image comes from Google.</li>
</ul>
<p>If there’s something wrong the copyright, do please contact me through my email!</p>
]]></content>
      <tags>
        <tag>Sustainable Service Learning</tag>
        <tag>Computational Sustainability</tag>
      </tags>
  </entry>
</search>
