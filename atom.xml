<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Max Liu&#39;s Blog</title>
  
  <subtitle>Earlier birds eat more worms</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://maxliu245.github.io/"/>
  <updated>2020-09-16T11:28:45.888Z</updated>
  <id>http://maxliu245.github.io/</id>
  
  <author>
    <name>Max Liu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【论文阅读19】Do We Need Zero Training Loss After Achieving Zero Training Error</title>
    <link href="http://maxliu245.github.io/2020/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB19%E3%80%91Do-We-Need-Zero-Training-Loss-After-Achieving-Zero-Training-Error/"/>
    <id>http://maxliu245.github.io/2020/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB19%E3%80%91Do-We-Need-Zero-Training-Loss-After-Achieving-Zero-Training-Error/</id>
    <published>2020-09-16T11:05:40.000Z</published>
    <updated>2020-09-16T11:28:45.888Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>一种新的正则方法flooding，相比以往直接考虑控制模型复杂度，flooding考虑的是控制训练loss的水平</p></blockquote><a id="more"></a><img src="/2020/09/16/【论文阅读19】Do-We-Need-Zero-Training-Loss-After-Achieving-Zero-Training-Error/Zero_Training_Loss_No_1.png" title="Title"><h2 id="为什么读"><a href="#为什么读" class="headerlink" title="为什么读"></a>为什么读</h2><p>文献及相关介绍的链接：</p><ol><li><p><a href="https://arxiv.org/abs/2002.08709" target="_blank" rel="noopener">https://arxiv.org/abs/2002.08709</a></p></li><li><p><a href="https://blog.csdn.net/oYeZhou" target="_blank" rel="noopener">叶舟</a>的CSDN博客<a href="https://blog.csdn.net/oYeZhou/article/details/107667317" target="_blank" rel="noopener">一种新的正则化方法——flooding，只需一行代码即可使用</a></p></li><li><p>新智元的知乎文章<a href="https://zhuanlan.zhihu.com/p/203417649" target="_blank" rel="noopener">【论文】一行代码发一篇ICML？</a></p></li></ol><p>至于为什么读这篇文章？🤭，因为看起来简单啊，一行代码写出来的，思想上一定是很有意思的，事实也是如此。只是这个有关训练loss和训练error的讨论让我有点迷糊，放在这里，若干纪元以后再来探访~</p><h2 id="从标题到文章主题"><a href="#从标题到文章主题" class="headerlink" title="从标题到文章主题"></a>从标题到文章主题</h2><p>标题点明了文章讨论的两个概念：<strong>训练loss</strong>和<strong>训练error</strong>之间的关系</p><p>为什么会注意到这两个概念呢？从我个人的观点，<strong>前者有一点泛化</strong>的意思在里面，因为loss大了可能欠拟合，小了可能过拟合，合适的loss水平确实对各种训练方法有点影响（个人经验）；后者其实也有一点泛化的味道，但我更喜欢说它<strong>代表拟合</strong>，理由差不多，error太小了，说明模型把训练数据给背下来了，是大概率过拟合的，毕竟极其强大的网络可以拟合数据，甚至“记住”所有训练数据，达到<strong>训练error=0</strong>，但这显然有点问题，大概率会过拟合。</p><blockquote><p>文章里有一句是别人工作的结论，我觉得不太对劲：</p><p>“learning until zero training error is meaningful to achieve a lower generalization error”</p><p>对此我保留意见，文章略过了这一点，直接开始思考🤔…</p></blockquote><p>这就引出了文章标题的思考：</p><blockquote><p>Q: Do we need zero training loss after achieving zero training error?</p></blockquote><h2 id="从传统正则到针对loss水平"><a href="#从传统正则到针对loss水平" class="headerlink" title="从传统正则到针对loss水平"></a>从传统正则到针对loss水平</h2><p>一般我们会用正则控制模型复杂度（但不是完全控制loss，只是加了一项），但它的出发点不是避免0<strong>训练loss</strong>，所以它其实也控制不了<strong>训练loss</strong>的水平，即往往最后的<strong>训练loss</strong>也会有偏大或偏小的现象，达不到合适的水平。</p><p>另外，一般来说<strong>训练error</strong> $\rightarrow$ 0是可以的，但是当前者达到时，<strong>训练loss</strong>还需要继续下降么，还需要继续训练模型么？</p><blockquote><p>even if we add fooding, we can still memorize the training data</p><p>flooding剑指当<strong>训练error</strong>-&gt;0时，<strong>训练loss</strong>维持在比较小的值而不是0</p><p>假设之前希望<strong>训练error</strong>-&gt;0的那句结论的说法是对的，那么控制<strong>训练loss</strong>的水平确实有点意思，但还要确定这个水平是多少</p></blockquote><h2 id="Flooding正则"><a href="#Flooding正则" class="headerlink" title="Flooding正则"></a>Flooding正则</h2><img src="/2020/09/16/【论文阅读19】Do-We-Need-Zero-Training-Loss-After-Achieving-Zero-Training-Error/Zero_Training_Loss_No_2.png" title="形象的模型描述"><p>针对这一点，提出flooding的方法，如上图很直观啦，这个洪水的水面高度就相当于最终<strong>训练loss</strong>的水平，称为fooding level。思想很简单，就是控制<strong>训练loss</strong>的水平，当loss较高就正常SGD，loss偏低就<strong>反向进行梯度上升</strong>。思路简单的结果是实现极其简单，如下式，变量flood就是新的loss，变量b就是fooding level。一行代码即可且可应用于<strong>一切</strong>以往基于loss梯度下降的方法。</p><script type="math/tex; mode=display">flood = (loss-b).abs()+b</script><p>这样就有个麻烦，这个fooding level<strong>到底是多少</strong>？我们把<strong>训练loss</strong>控制到这个level<strong>为什么能有效</strong>？</p><blockquote><p>In Appendix F, we show that during this period of random walk, there is an increase in fatness of the loss function</p><p>附录F并没有提及理论上为何有效，似乎在附录A有定理证明</p><p>附录F的图看了几遍也没有看明白，可能需要阅读flatness的相关知识吧</p><p>附录A的证明很清晰，表明这种操作是有效的√</p></blockquote><p>那么现在只差fooding level<strong>到底是多少</strong>这个问题了。文章里表示这个可以手动设置，也可以设置验证（文中的实验就是这么做的），或者当作超参数进行超参优化；个人以为还是最后的舒服。</p><h2 id="注"><a href="#注" class="headerlink" title="注"></a>注</h2><p>这篇文章有个<strong>有意思的知识点</strong>表一，是许多正则方法的比较，以后需要的话可以看看 √</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;一种新的正则方法flooding，相比以往直接考虑控制模型复杂度，flooding考虑的是控制训练loss的水平&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="Machine Learning" scheme="http://maxliu245.github.io/tags/Machine-Learning/"/>
    
      <category term="Regularization" scheme="http://maxliu245.github.io/tags/Regularization/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读18】Meta-Learning Symmetries by Reparameterization——利用元学习学习保持运算等变性的结构</title>
    <link href="http://maxliu245.github.io/2020/08/31/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB18%E3%80%91Meta-Learning-Symmetries-by-Reparameterization%E2%80%94%E2%80%94%E5%88%A9%E7%94%A8%E5%85%83%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E4%BF%9D%E6%8C%81%E8%BF%90%E7%AE%97%E7%AD%89%E5%8F%98%E6%80%A7%E7%9A%84%E7%BB%93%E6%9E%84/"/>
    <id>http://maxliu245.github.io/2020/08/31/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB18%E3%80%91Meta-Learning-Symmetries-by-Reparameterization%E2%80%94%E2%80%94%E5%88%A9%E7%94%A8%E5%85%83%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E4%BF%9D%E6%8C%81%E8%BF%90%E7%AE%97%E7%AD%89%E5%8F%98%E6%80%A7%E7%9A%84%E7%BB%93%E6%9E%84/</id>
    <published>2020-08-31T15:25:27.000Z</published>
    <updated>2020-08-31T16:28:29.773Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>这个文章可是难到我了，谁让年轻的时候抽🐘代数学得菜呢？</p><p>好在可以举例子明白其中的道理┭┮﹏┭┮</p></blockquote><a id="more"></a><img src="/2020/08/31/【论文阅读18】Meta-Learning-Symmetries-by-Reparameterization——利用元学习学习保持运算等变性的结构/MSR.png" title="MSR"><h1 id="为什么读"><a href="#为什么读" class="headerlink" title="为什么读"></a>为什么读</h1><ul><li>元学习新作</li><li>作者之一是Finn啊doge</li></ul><p>文献链接：<a href="https://arxiv.org/abs/2007.02933" target="_blank" rel="noopener">Meta-Learning Symmetries by Reparameterization</a>，原作者提交于2020/07，挂出来了。</p><h1 id="文献总结"><a href="#文献总结" class="headerlink" title="文献总结"></a>文献总结</h1><p>用强大的神经网络去学习一些参数，它们表示某种变换（如卷积），能对特定的任务（如图像相关）满足某种等变性。</p><p>其实就是<font color="#0000FF">重参数化网络的隐层，学习像卷积这样的运算的模式pattern，可以满足像对图像平移变换的等变性，并利用元学习进一步学习此pattern</font></p><h1 id="文献内容"><a href="#文献内容" class="headerlink" title="文献内容"></a>文献内容</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>和文献总结一致！</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>背景一方面是CNN，可以认为卷积满足对图像平移变换的等变性，如果像卷积这样的运算可以推广，并满足各种特定运算的等变性，那一定是很🐂🍺。</p><p>另一方面是文章里的related work，不过我暂时不感兴趣没看。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>说实话我感觉不好总结，姑且分两个部分，一部分讲等变性对应的代数基础，另一部分讲网络参数设置的形式（以使用元学习训练）。</p><h3 id="等变性的代数理论"><a href="#等变性的代数理论" class="headerlink" title="等变性的代数理论"></a>等变性的代数理论</h3><p>3.2节提到了群卷积是什么，我不（代）太（数）明（太）白（差），举个例子吧。</p><p>网络隐层的输入输出都是空间 $X$ 的函数。比如是 $R^n$ 中一个向量，它相当于一个映射，从指标集 $X={1, \cdots, n}$ 映射到对应的 $n$ 个实数。</p><p>$X$ 上的实值函数集合定义为 $F_X$ ，群 $G$ 作用在 $F_X$ 上，通过 $\pi:G\rightarrow GL(F_X)$，这个 $\pi$ 称为representation，把群 $G$ 中的元素 $g$ 映为 $F_X$ 中的一个可逆线性变换。</p><p>注意 $G$ 对 $X$ 的作用是一个 $g\rightarrow F_X$ 的映射，</p><p>即 $F_X$ 中的可逆线性变换 $f$，当然 $f$ 作用的元素是 $X$ 中的元素 $x$ ，因为 $f$ 是 $X$ 上的实值函数嘛，现在 $\pi(g)$ 是一个可逆线性变换，假设就是 $f$ 的逆变换，所以 $\pi(g)$ 复合 $f$ 是一个函数且是恒等映射，即 $(\pi(g)f)(x) = I(x) = x$</p><p>文章写的符号我没看懂，但是看例子懂了， $G=X$ 时，定义作用为 $R^2$ 上的加法平移，则 $g^{-1}x$ 是 $x-g$ ， $gx$ 是 $x+g$ ，</p><img src="/2020/08/31/【论文阅读18】Meta-Learning-Symmetries-by-Reparameterization——利用元学习学习保持运算等变性的结构/MSR2.png" title="举个例子我就懂啦！"><p>3.3节表示，定义网络隐层为一个函数 $\phi:F_X\rightarrow F_Y$， $G$ 有等变性，举个简单例子， $X$ ， $Y$ 是输入图片网格和输出特征图的网格，所以， $F_X$ 中的元素就是网格大小的图片， $F_Y$ 中的元素就是特征图大小的图片；3式左边就是先对隐层输入做一个平移变换，再输出特征图，右边相当于先输出特征图，再做输出空间上对应的平移变换。</p><p>一个特例是，如果 $\pi$ 是id恒等映射，即 $X$ 、 $Y$ 空间上平移是一致，一样的，那么就叫不变性，先平移再映射和先映射再平移是一样的；注意这里的两个平移现在是数值上相等，但空间是其实分别是 $X$ 和 $Y$ 。</p><p>由于等变性对于函数复合可以保持，那么如果所有隐层都有等变性，那么整个网络也有。</p><p>线性层是基本的层，希望用它学习参数的共同pattern，达到使之有等变性（像群卷积）。</p><p>最后重述一下MSR方法吧，即学习和编码等变性，这个学习就是学习隐层参数的共享pattern，编码是指在代数上找到上文所述的一个灵活的representation。</p><h3 id="网络参数形式"><a href="#网络参数形式" class="headerlink" title="网络参数形式"></a>网络参数形式</h3><p>其实具体的例子参考文中的图1即可。</p><p>4.1节表示，隐层权重矩阵是图1那样形式叠加的矩阵，这就是一种constraint，就是一种pattern，结果是对平移有等变性。现在希望推广到其它的变换，旋转，镜像什么的，但是咱们没有思路怎么去做，所以就用简单的FC层模拟，设置 $y=\phi(x)=Wx, W\in \mathbb{R}^{m\times n}$ ，然后学习 $W$ 就是在学习这个pattern了。</p><p>接着阐释了这个pattern不是直接去学习 $W$ ，这样也看不出有什么pattern。我猜是参考了矩阵分解的概念， $vec(W)=Uv, v\in\mathbb{R}^k, U\in\mathbb{R}^{mn\times k}$ ， $W$ 分解为伪对称矩阵 $U$ 和滤波参数向量 $v$ 的积， $U$ 的维度就是 $W$ 完全展开，再复制个 $k$ 份， $k$ 就是参量 $v$ 的维度，应该可以自行设置。所以pattern实际上就是 $U$ ， $U$ 可以限制最后 $W$ 的形式，滤波参数 $v$也要根据实际任务训练。</p><blockquote><p>For example, the sharing pattern of standard convolution guarantees that the weight matrix is constant along any diagonal, reducing the number of per-task parameters</p></blockquote><p>显然 $U$ 的维度会很高，怎么办？Kronecker factorization，就是参数化的方式，和我之前看的文章和做的实验都是<strong>一致</strong>的想法！</p><p>4.2节继续，那么 $U$ 具体怎么设置呢？和输入的形式有一些关系…举例子吧，从基本的网格空间 $X$ 上选位置，得到输入 $f$ ，即 $f:X\rightarrow R^c$ ，这个输入是向量（矩阵或者矩阵展开应该也行）。假定 $f$ 是 $s$ 维的， $\bar{f}\in\mathbb{R}^s$ ，其中 $\bar{f}_i=f(x_i)$  。引理就是保证了可以由这个 $U$ 保持群的作用，即保持某种运算的等变性；引理分两部分，一部分是给定群后，群的作用就定了，即运算定了，该运算的等变性存在 $U$ 来保持；第二部分是此 $U$ 存在时，任何 $G$ 卷积都可以由 $vec(W)$ 表示出来。所以 $U$ 就设置成一个固定长度的参数向量，把它参数化为矩阵或者向量本身都可以，进一步得到pattern的形式，这样就是能训练的。证明估计我也看不懂了…</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>优点：</p><ul><li>优点：学习保持等变量性质的参数pattern—&gt;泛化，减少参数用量，适用小样本</li><li>从卷积的应用开始，等变性很重要—&gt;是未来发展的方向之一</li><li>未来自动寻找潜在的特殊结构的算子？像卷积这样的带有等变性的</li><li>quickly discovering symmetries的应用</li></ul><p>缺点：</p><ul><li>我也不知道，可能要根据等变性做监督，训练得到 $U$ 具体的形式，这算不算一种手动设置呢。好吧我其实在杠，还是暂时挑不出毛病</li></ul><h1 id="附录——“快乐”的阅读体验"><a href="#附录——“快乐”的阅读体验" class="headerlink" title="附录——“快乐”的阅读体验"></a>附录——“快乐”的阅读体验</h1><p>我的阅读历程😭😥😪😵，仅个人记录于此，不建议看官阅读…应该看不懂的，只是一些个人阅读思路。<strong>和上文基（是）本（水）重（字）叠（数）</strong>。</p><ul><li><p>标题中什么叫元学习的对称性？</p><p>这里应该<strong>不是</strong>指“元学习有对称性”，而是一个广泛的概念，是要学习<strong>某种</strong>对称性，包括数据本身性质、任务之间关联性，以及某些对数据运算的性质（如旋转镜像这样的运算算子）</p></li><li><p>摘要中：</p><ul><li><p>等变量equivariances是什么意思？</p><p>参考链接<a href="https://zhuanlan.zhihu.com/p/34288976" target="_blank" rel="noopener">CNN 中等变性（equivariant）与不变性（invariant）的关系</a>即可，确实是，经过某些特定运算，特定的输入<strong>模式</strong>能够保持不变</p></li><li><p>感觉文章的<strong>卖点</strong>是从数据（含某种对称性）中（元）学习这个所谓的等变量，学习的方式是用网络<strong>学习等变量对应的parameter sharing patterns</strong>，用参数化形式学习表达出这种模式！</p></li></ul></li><li><p>引言中：</p><ul><li><strong>等变量的起源——CNN</strong>，利用等变量做了某种对称变换，来保护参数，提高泛化能力。什么意思？大概就是卷积（层）这个操作，这个变换允许输入输出的对应变化（平移）有不变性，叫做translation equivariant，即输入（图像）经过特定的平移变换后输出也会经过类似的变换。</li><li>第一段提到了translation equivariant，这是个名词，专指<strong>平移不变性</strong>，不是混淆为广泛的变化！知乎<a href="https://zhuanlan.zhihu.com/p/34288976" target="_blank" rel="noopener">CNN 中等变性（equivariant）与不变性（invariant）的关系</a>讲得很好！</li><li>上面的指CNN利用的卷积，其本身有这种平移不变性，因此在特定任务上（图像相关）表现很好。现在本文试图找其它的内在的等变量。</li><li><strong>目的</strong>出现！引言第3段，在神经网络中<strong>自动学习等变量</strong>。有利于迁移，不需手动设计像卷积这样有平移等变性的算子，直接去寻找等变量。怎么自动学习？<font color="#0000FF">重参数化网络的隐层，以学习表示共享的pattern，并利用元学习进一步学习共同的pattern</font>。</li><li>引言最后一段讲的什么？网络怎么等价于有限对称群？这对称性和拓扑怎么关系上了？抽🐘代数🐕表示很淦…</li></ul></li><li><p>相关工作看不懂，现在懒得看</p></li><li><p>先修知识章节表示3.2、3.3节会给出等变量和群卷积的定义…<strong>早说啊</strong></p><ul><li><p>3.1回顾MAML，这是典型的元学习，MAML也是这样么，和我记的不太一样啊？我要回去重看MAML了，我就记得它是学一个好的初始参数，具体怎么学的忘了</p></li><li><p>3.2，对称性和等变性是群对集合作用的性质…复习一堆拓扑代数知识hahaha…群的定义，加法群…同态，从群 $G$ 映到 $X$ 的自同构群…自同构群，是群 $X$ 到自己的同构变换，且双射…</p></li><li><p>3.2，群卷积是什么，隐层的输入输出都是空间 $X$ 的函数。比如说 $R^n$ 中一个向量相当于一个映射，从指标集 $X={1, \cdots, n}$ 映射到对应的 $n$ 个实数。</p><img src="/2020/08/31/【论文阅读18】Meta-Learning-Symmetries-by-Reparameterization——利用元学习学习保持运算等变性的结构/MSR2.png" title="举个例子我就懂啦！"><p>$X$ 上的实值函数集合定义为 $F_X$ ，群 $G$ 作用在 $F_X$ 上，通过 $\pi:G\rightarrow GL(F_X)$，这个 $\pi$ 称为representation，把群 $G$ 中的元素 $g$ 映为 $F_X$ 中的一个可逆线性变换。</p><p>注意 $G$ 对 $X$ 的作用是一个 $g\rightarrow F_X$ 的映射，</p><p>即 $F_X$ 中的可逆线性变换 $f$，当然 $f$ 作用的元素是 $X$ 中的元素 $x$ ，因为 $f$ 是 $X$ 上的实值函数嘛，现在 $\pi(g)$ 是一个可逆线性变换，假设就是 $f$ 的逆变换，所以 $\pi(g)$ 复合 $f$ 是一个函数且是恒等映射，即 $(\pi(g)f)(x) = I(x) = x$</p><p>文章写的符号我没看懂，但是看例子懂了， $G=X$ 时，定义作用为 $R^2$ 上的加法平移，则 $g^{-1}x$ 是 $x-g$ ， $gx$ 是 $x+g$ ，</p></li><li><p>3.3，定义网络隐层为一个函数 $\phi:F_X\rightarrow F_Y$， $G$ 等变性，举个简单例子， $X$ ， $Y$ 是输入图片网格和输出特征图的网格，所以， $F_X$ 中的元素就是网格大小的图片， $F_Y$ 中的元素就是特征图大小的图片；3式左边就是先对隐层输入做一个平移变换，再输出特征图，右边相当于先输出特征图，再做输出空间上对应的平移变换。</p><p>如果pi_2是id恒等映射，即 $X$ 、 $Y$ 空间上平移是一致一样的，那么就叫不变性，先平移再映射和先映射再平移是一样的；注意这里的两个平移现在是数值上相等，但空间是其实分别是 $X$ 和 $Y$ 。</p></li><li><p>3.3，等变性对于函数复合可以保持，所以如果所有隐层都有等变性，那么整个网络也有。</p></li><li><p>3.3，线性层，研究基本的层，怎么学习参数的共同pattern，达到使之有等变性（群卷积）。</p></li><li><p>3.3，4式什么意思？没细看了，和之前写的3.2的式子应该是一致的</p></li><li><p>MSR方法干什么的：学习和编码等变性，这个学习就是学习隐层参数的共享pattern，编码是指在代数上找到一个灵活的representation</p></li></ul></li><li><p>讲方法了：</p><ul><li><p>4.1，FC层，权重矩阵是那样叠加的矩阵，这就是一种constraint，就是一种pattern，结果是对平移有等变性。现在希望推广到其它的变换，旋转，镜像什么的，但是咱们没有思路怎么去做，所以就用FC层模拟，设置 $y=\phi(x)=Wx, W\in \mathbb{R}^{m\times n}$ ，然后学习 $W$ 就是在学习这个pattern了。</p></li><li><p>接着阐释了这个pattern不是直接去学习 $W$ ，这样也看不出有什么pattern，我猜是参考矩阵分解的概念， $vec(W)=Uv, v\in\mathbb{R}^k, U\in\mathbb{R}^{mn\times k}$ ， $W$ 分解为伪对称矩阵 $U$ 和滤波参数向量 $v$ 的积， $U$ 的维度就是 $W$ 完全展开，再复制个 $k$ 份， $k$ 就是参量 $v$ 的维度，应该可以自行设置。所以pattern实际上就是 $U$ ， $U$ 可以限制最后 $W$ 的形式，滤波参数 $v$也要根据实际任务训练。</p><blockquote><p>For example, the sharing pattern of standard convolution guarantees that the weight matrix is constant along any diagonal, reducing the number of per-task parameters</p></blockquote></li><li><p>显然 $U$ 的维度会很高，怎么办？Kronecker factorization，就是参数化的方式，和之前看的文章和做的实验都是<strong>一致</strong>的想法！</p></li><li><p>4.2，那么 $U$ 具体怎么设置呢？输入的形式…举例子吧，从基本的网格空间 $X$ 上选位置，得到输入 $f$ ，即 $f:X\rightarrow R^c$ ，这个输入是向量（矩阵或者矩阵展开应该也行）。假定 $f$ 是 $s$ 维的， $\bar{f}\in\mathbb{R}^s$ ，其中 $\bar{f}_i=f(x_i)$  。引理就是保证了可以由这个 $U$ 保持群的作用，即保持某种运算的等变性；引理分两部分，一部分是给定群后，群的作用就定了，即运算定了，该运算的等变性存在 $U$ 来保持；第二部分是此 $U$ 存在时，任何 $G$ 卷积都可以由 $vec(W)$ 表示出来。反正就是能训练的。证明估计我也看不懂</p></li></ul></li><li><p>实验日常不想看，但是说实话还是得康康代码的</p></li><li><p>结论：</p><ul><li>优点：学习保持等变量性质的参数pattern—&gt;泛化，减少参数用量，适用小样本</li><li>从卷积的应用开始，等变性很重要—&gt;是未来发展的方向之一</li><li>未来自动寻找潜在的特殊结构的算子？像卷积这样的带有等变性的</li><li>quickly discovering symmetries的应用</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这个文章可是难到我了，谁让年轻的时候抽🐘代数学得菜呢？&lt;/p&gt;
&lt;p&gt;好在可以举例子明白其中的道理┭┮﹏┭┮&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="Meta Learning" scheme="http://maxliu245.github.io/tags/Meta-Learning/"/>
    
  </entry>
  
  <entry>
    <title>【NBA2K13】暑期修改历程暂结</title>
    <link href="http://maxliu245.github.io/2020/08/30/%E3%80%90NBA2K13%E3%80%91%E6%9A%91%E6%9C%9F%E4%BF%AE%E6%94%B9%E5%8E%86%E7%A8%8B%E6%9A%82%E7%BB%93/"/>
    <id>http://maxliu245.github.io/2020/08/30/%E3%80%90NBA2K13%E3%80%91%E6%9A%91%E6%9C%9F%E4%BF%AE%E6%94%B9%E5%8E%86%E7%A8%8B%E6%9A%82%E7%BB%93/</id>
    <published>2020-08-30T02:28:33.000Z</published>
    <updated>2020-08-30T02:31:09.169Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>暑期真的花了很多时间在整理更新数据上，修改的过程真的快乐，但也不能老是沉迷其中啦😂</p></blockquote><a id="more"></a><h1 id="个人感想"><a href="#个人感想" class="headerlink" title="个人感想"></a>个人感想</h1><p>最近相当长的一段时间内，我很癫狂地玩起了NBA2K13（Android），并大量修改其中的球员数据。不得不说，这款老游戏深得我的喜爱，大概是单纯由于我喜欢模拟类、角色扮演类的游戏，以及这款游戏的引擎恰巧很符合我的口味吧。临近开学季，我恐怕不能再抽出这么多时间来体验<font color="#0000FF">修改</font>（其实是无root移植）的乐趣了，因此打算稍微总结这近2个月来的游戏体验。</p><p>总的来说，这2个月以来，基于KM2BY的2.20数据包和4.05存档，在无root条件下，我先后一共完成了独行侠、鹰、鹈鹕、太阳、掘金、篮网、76人、快船、湖人、森林狼和猛龙共11支球队绝大多数球员的更新修改，以及魔术和开拓者24球队少部分核心球员的数据更新。</p><p>修改完每支球队后，我都会自己开一个赛季模式动手打几场比赛（均为全明星难度），来测试修改后的可玩性（不包括模拟的真实性），然而直到现在，只有5支球队的游戏体验良好，分别是76人、独行侠、湖人、开拓者和猛龙，这其中，76人和开拓者完全是核心球员刷数据带飞；独行侠和湖人由于角色球员较强，相对来说可玩性还是稍强一些；最后单独拎出猛龙是因为目前我只上手玩了一把，这个球队防守强度太高了，导致游戏比较轻松😂，有点“兄弟篮球，人人🐂🐸”那味儿了，后续我会多测试几次来确认可玩性。</p><p>一般来说，修改后的阵容有这样的优缺点（对比）：</p><div class="table-container"><table><thead><tr><th>自己操作</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td></td><td>核心球员在自己家比较给力，一般能稳住局势</td><td>对面球队有且存在不只1个核心并无限单人打爆自己…手段包括但不限于无限轻松上篮得分（或造犯规）、高频率抢断+盖帽…</td></tr><tr><td></td><td>拥有极强核心的强队游戏难度低</td><td>这样强队玩几局就腻了</td></tr><tr><td></td><td>有3分强球员的球队为所欲为</td><td>这些球员在自己手里容易n中0，在对面容易超神</td></tr><tr><td></td><td>大量角色球员会增强真实感</td><td>角色球员在自己家大部分都拉跨，在对面就很无敌…</td></tr><tr><td>模拟</td><td>真实性可以大大增强</td><td>需要手动调整球员倾向，比较麻烦</td></tr></tbody></table></div><p>一时间只想出来这么些体验方面的对比，就先放在这里。玩几个强队的时候都还行，只要雪球滚起来，分差拉开，后面就会轻松，但也经常会被对面打出各种小高潮以及末节究极被追分；玩弱队的时候体验极差，各种无限被抢被掏被帽被造犯规被轻松突破，自己进攻总是进攻犯规或者常常强打被按下来，或者干脆就攻不进去。玩了好几个这样的球队，甚至包括部分所谓的强队也有这样的感受，我觉得重要的原因是这些球队球员修改的进攻防守意识数值都不够，另外是普遍移动速度太慢，最后是没有稳定射手或稳定内线大闸。没有第一点，玩起来就总是核心刷数据；没有第二点，玩起来就是根本突不进去，尤其包括快攻的进攻效率低下；没有第三点，进攻端或者防守端必有一端拉跨，降低游戏体验。</p><h1 id="实战演示"><a href="#实战演示" class="headerlink" title="实战演示"></a>实战演示</h1><p>由于测试的次数不多且时间间隔有点长了，所以只列两个最近修改的球队，森林狼（弱队代表）和猛龙（强队代表）的比赛感受。</p><p>首先是森林狼，核心包括唐斯、拉塞尔、马利克比斯利和詹姆斯约翰逊，其他人要拉跨很多；主要弱点有2，一是射手太少，只有比斯利能顶一下，二是球队球员移动速度普遍较低（掘金、鹰、太阳也是这样）。自己上手比赛，和13年的球队比较，目前只赢过一把，其它全部头被🔨爆…</p><img src="/2020/08/30/【NBA2K13】暑期修改历程暂结/1.jpg" title="13版尼克斯VS20版森林狼——尼克斯数据"><img src="/2020/08/30/【NBA2K13】暑期修改历程暂结/2.jpg" title="13版尼克斯VS20版森林狼——森林狼数据"><p>只有这一把打赢了，赢的因素（对比后来输的比赛可以看看被打得多惨😭）主要在于：</p><ul><li>自己的球队因素：<ul><li>绝对核心唐斯在攻防两端表现稳定，拉塞尔没有受到犯规困扰，组织上没有被频繁断球</li><li>角色球员有所发挥，前期约翰逊强打内线稳住了比分，中后期比斯利抓住了几次快攻机会没有被帽</li><li>底层的替补竟然得分能超过5分了😱（气抖冷doge！我替补板凳何时在游戏里站起来！）</li></ul></li><li>对面球队因素：<ul><li>甜瓜由于在开拓者，能力值已经根据存档削弱，这次得分不算很高了</li><li>基德前期犯了好几次规，第一节让我滚了点雪球</li><li>其它球员都算是角色球员，没有那么恐怖…</li></ul></li></ul><p>输的局反正就是要么被对面n核心直接🔨爆，要么就是被无限突破+抢断+盖帽…被玩坏了…</p><p>———————————————- 手动分割线 ———————————————-</p><p>最后是刚弄的猛龙，虽然核心球员的能力值都不算顶尖，但是有不少能高于80了；角色球员的防守能力、意识都相对很不错，球队整体防守目前来看非常好；球队有一堆射手啦，虽然实战投射前期雪崩，后面才慢慢能投一些…</p><img src="/2020/08/30/【NBA2K13】暑期修改历程暂结/3.jpg" title="20版开拓者（只有利拉德）VS20版猛龙——前3节猛龙部分数据"><img src="/2020/08/30/【NBA2K13】暑期修改历程暂结/4.jpg" title="20版开拓者（只有利拉德）VS20版猛龙——前3节猛龙射手数据"><img src="/2020/08/30/【NBA2K13】暑期修改历程暂结/4.jpg" title="20版开拓者（只有利拉德）VS20版猛龙——全场统计"><p>这场测试比赛的感受就是，哇塞，防守太舒服了！球队的篮板球实在是舒服，从半场的篮板看就…是不是猛得过头了…当然前几节射手的效率实在是…有空位竟然也投不进…可能是和身高有点关系吧，游戏里范弗里特的这三分是惊到我了…</p><p>哎呀呀，总之就是修改游戏的过程很快乐，但是手打的体验没有那么好😂。接下来要收手了，再着迷在这上面就没有工作学习的时间啦🤾‍♂️</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;暑期真的花了很多时间在整理更新数据上，修改的过程真的快乐，但也不能老是沉迷其中啦😂&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="NBA2K13" scheme="http://maxliu245.github.io/tags/NBA2K13/"/>
    
      <category term="DIY" scheme="http://maxliu245.github.io/tags/DIY/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读17】ODE-Net——从离散化的ResNet到连续化ODE的模拟</title>
    <link href="http://maxliu245.github.io/2020/08/21/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB17%E3%80%91ODE-Net%E2%80%94%E2%80%94%E4%BB%8E%E7%A6%BB%E6%95%A3%E5%8C%96%E7%9A%84ResNet%E5%88%B0%E8%BF%9E%E7%BB%AD%E5%8C%96ODE%E7%9A%84%E6%A8%A1%E6%8B%9F/"/>
    <id>http://maxliu245.github.io/2020/08/21/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB17%E3%80%91ODE-Net%E2%80%94%E2%80%94%E4%BB%8E%E7%A6%BB%E6%95%A3%E5%8C%96%E7%9A%84ResNet%E5%88%B0%E8%BF%9E%E7%BB%AD%E5%8C%96ODE%E7%9A%84%E6%A8%A1%E6%8B%9F/</id>
    <published>2020-08-21T11:12:27.000Z</published>
    <updated>2020-08-21T11:16:21.590Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>读不懂PDE-Net，所以想先操作下ODE-Net</p><p>结果是瞎读文章真心难读也读不明白😭😭😭</p></blockquote><a id="more"></a><img src="/2020/08/21/【论文阅读17】ODE-Net——从离散化的ResNet到连续化ODE的模拟/ODE-Net.png" title="ODE-Net"><h1 id="为什么读"><a href="#为什么读" class="headerlink" title="为什么读"></a>为什么读</h1><ul><li>在阅读PDE-Net的时候被卡了，读得比较不明白，在搜资料的时候发现了知乎上ODE-Net的<a href="https://www.zhihu.com/question/313064079" target="_blank" rel="noopener">讨论</a>，好介绍哇，觉得现在先看ODE-Net是个不错的选择</li><li>搜资料的时候才发现这是NIP2018最佳论文</li></ul><p>文献链接：<a href="https://arxiv.org/abs/1806.07366" target="_blank" rel="noopener">Neural Ordinary Differential Equations</a>，原作者提交于2018/01，最后修改于2019/12，可见打磨的功夫还是下了不少的。</p><h1 id="文献总结"><a href="#文献总结" class="headerlink" title="文献总结"></a>文献总结</h1><p>将ODE与神经网络结合，用神经网络来模拟ODE的动态性质。</p><p>思想很简单，联想神经网络的隐层嵌套性，联系ResNet的残差（差分）形式并推广，就得到了利用ResNet模拟ODE的思想。细节就麻烦多了看不动看不动…</p><h1 id="文献内容"><a href="#文献内容" class="headerlink" title="文献内容"></a>文献内容</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>将ODE与神经网络结合，用神经网络来模拟ODE的动态性质，和总结一致。</p><p>另外如果能有办法用网络模拟ODE系统，那一定是应用超级友好的！</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>好像没啥背景，我理解的背景就是这个思想的来源，就够了。文章里的related work我暂时不感兴趣。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>这要从ODE与神经网络的一个共性讲起：</p><p>神经网络的嵌套隐层结构可以表示为$h<em>{t+1} = f(h_t, \theta_t)$，每一层都由前一层前向传播得到，现在用ResNet的结构则有意思起来了：$h</em>{t+1} = h_t + f(h_t, \theta_t)$，这种残差的形式与ODE的差分形式很“相似”：</p><script type="math/tex; mode=display">\Delta h(t) = h_{t+1} - h_t = f(h(t), t, \theta) \approx \dfrac{dh(t)}{dt}</script><p>这样ResNet的每一层$f$都相当于当前时刻$t$的一个差分，如果这个$t$不再离散，而是连续化，那么就可以近似为导数，残差神经网络也可以近似为ODE了。当然<strong>前提</strong>是（1）连续化可行；（2）连续化后能保持ODE的基本衍化性质；（3）不知道能不能证明其它性质诸如求解等等。</p><p>那么现在基本的模型其实已经构思好了，下面是实现的技术细节。要使得ResNet成为ODE的近似，首先要考虑可训练的性质，其监督信息由已知的ODEsolver求解，这些solver目前一般都是精度可控的，网络本身作为一个黑箱ODEsolver来训练；前向传播应该比较容易，关键是反向传播会比较麻烦，因为损失的回传需要对参数求导，这些势必要涉及大量参数，会引起高昂的计算代价。</p><p>那么该文怎么进行反向传播的呢，直接算？不是，它绕过去了。</p><p>先看看损失函数，直接定义为：</p><script type="math/tex; mode=display">L(z(t_1)) \approx L\left(z(t_0) + \int_{t_0}^{t_1} f(z(t), t, \theta)dt \right) = L(ODESolve(z(t_0), f, t_0, t_1, \theta))</script><p>其中把ODE最后的target函数记为$z(t)$，ODE初值记为$z(t_0)$，待预测的时刻$t_1$时的值为$z(t_1)$。左边相当于监督信息，用标准的值，右边两个式子都用黑箱网络来近似就可以了。</p><p>由于现在考虑把ResNet变成连续化的模型，因此此时不好用差分的形式，而要采用积分的形式，所以现在预测值是和时刻$t_0$和$t_1$中每个值都要有关系，这提示我们采用链式法则寻找这些值之间的关系。当然这些值太多乃至无穷，我们也只能尽可能多地进行离散近似，其实就是网络的深度（层数），有几层，就意味着我们考虑了时刻$t_0$和$t_1$之间几个中间状态。</p><p>这种近似考虑的方式称为<strong>adjoint sensitivity method</strong>（1962老文章🐂🍺+🐂🐸！），如下的原文图2所示，这里为了表示方便，记号与上文不同，图中的$z(t_N)$相当于上文的$z(t_1)$，意思大家一定能明白，就是多考虑上文中的中间时刻状态。</p><img src="/2020/08/21/【论文阅读17】ODE-Net——从离散化的ResNet到连续化ODE的模拟/ODE-Net2.png" title="Adjoint Sensitivity Method"><p>感觉是这样推导的，我们想要的目标是损失$L$对网络参数$\theta$的导数：</p><script type="math/tex; mode=display">\dfrac{dL}{d\theta} = \dfrac{dL}{dz(t_i)}\dfrac{dz(t_i)}{d\theta} = \dfrac{dL}{dz(t_i)}\dfrac{df(z(t_i), t_i, \theta)}{d\theta} \triangleq a(t_i)\dfrac{df(z(t_i), t_i, \theta)}{d\theta}</script><p>其中第一个等号用链式法则和爱因斯坦求和约定表示；第二个等号是因为$z(t<em>i) \approx z(t_0) + \int</em>{t_0}^{t_i} f(z(t), t, \theta)dt$，然后直接求导就得了，其中积分从哪里开始都可以，不一定要$t_0$；第三个等号搞了新定义，把$a(t)$叫做伴随adjoint，它怎么求呢，文章给了$\dfrac{da(t)}{dt} = -a(t)^T\dfrac{\partial f(z(t), t, \theta)}{\partial z}$，但我没看懂，我猜是1962那篇文章的结论。</p><p>上面分析了主要的思路，应该还可以了，下面开始水😂，日常写烂尾文章/(ㄒoㄒ)/~~</p><p>上面的推导就是原文的(5)式，具体计算不是直接BP，因为考虑连续性，和中间状态都有关系了，中间层现在是可以很多的，BP压力很大。现在直接看完整版的算法2，直接想办法计一个积分算梯度（参考了机器之心的文章加深理解），不过很可惜我<strong>没太看明白</strong>具体计算的过程，可能是直接自动求导，只是不再一层一层计算梯度了？这样真的能减少计算量么？</p><p>后面的流模型和生成式模型没有看了，留着看苏剑林的博客吧，感觉和那个关系比较大。</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>我理解的优点：</p><ul><li>应用必定是真正有用的应用，且范围很广，涉及ODE的领域实在是太多啦</li><li>思路很棒！从ResNet的差分形式过渡到ODE的模拟，我很喜欢</li><li>文章提到的内存占用小的优点，不过有人杠这一点，我目前没看明白算法2，只好持保留态度了</li></ul><p>缺点：</p><ul><li>有人杠他们的代码和文章不一致（知乎），我先吃瓜瓜</li><li>算法写的有一点不明白，起码我看了几遍没明白具体是怎么算法（不考虑伪代码）</li><li>原作者之一自己提的缺点，见参考资料<a href="https://www.sohu.com/a/405309910_129720" target="_blank" rel="noopener">「神经常微分方程」提出者之一David Duvenaud：如何利用深度微分方程模型处理连续时间动态</a></li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>看不动，和参考资料里的是一样的啊，而且有评论说作者放出来的代码和文章不一致，那么轻松复现是不存在的了…</p><p>扫了一眼，文章的实验主要是模拟了一些不那么复杂，但也不是很简单的ODE系统。</p><h1 id="补充资料——参考链接阅读"><a href="#补充资料——参考链接阅读" class="headerlink" title="补充资料——参考链接阅读"></a>补充资料——参考链接阅读</h1><p>在略读了文章第一遍之后有些疑惑，因此搜了一些相关的网页看一看，觉得其中大部分讲的都不错，因此记录于此。</p><ul><li>看了一遍作者<a href="https://www.jiqizhixin.com/users/b0254ca0-165f-4fd3-a041-232d39a848d2" target="_blank" rel="noopener">思源</a>发布在机器之心的文章<a href="https://www.jiqizhixin.com/articles/122302" target="_blank" rel="noopener">硬核NeruIPS 2018最佳论文，一个神经了的常微分方程</a>，前面的解释比较足，有了大致的了解，还有不少细节是不太明白的</li><li>看了知乎上的<a href="https://www.zhihu.com/question/306937011/answer/561576636" target="_blank" rel="noopener">如何评价ODENet？</a>，反对的意见要注意，我看文章的时候觉得它说避免了存储中间计算的值，减少内存使用，而算法中反向计算的时候其实还要算一遍，这个说法很奇怪。目前我感觉确实是代码中，这部分计算没有交给网络，而是交给scipy了。</li><li>看了机器之心在搜狐上的文章<a href="https://www.sohu.com/a/405309910_129720" target="_blank" rel="noopener">「神经常微分方程」提出者之一David Duvenaud：如何利用深度微分方程模型处理连续时间动态</a>，了解了一些该团队最新进展，包括可逆残差网络、设计廉价的可微算子计算梯度、随机微分方程的应用；并看到了有趣的“内幕”哈哈，担心侵权问题就不放原文了，大概意思是他们这篇文章动机很现实，而且有一些数学上比较严肃的问题：</li><li><a href="https://www.toutiao.com/a6703302712311677452/" target="_blank" rel="noopener">一个深度学习突破的方向：神经常微分方程ODE</a>这个文章<strong>内容很多</strong>啊，有不少自己尝试的实验，只简单介绍了ODE-Net的思想，然后自己做了不少拟合的实验，最后给了自己的结论太棒了！它的结论是目前该方法只在小任务上表现良好，实际应用或者稍复杂的例子就会gg😀</li><li>本来想读<a href="https://blog.csdn.net/hanss2/article/details/81301343" target="_blank" rel="noopener">【随感】ODE的欧拉解法实际上就是RNN的一个特例</a>这篇文章，结果发现是转载苏剑林的博客<a href="https://kexue.fm/archives/5643" target="_blank" rel="noopener">貌离神合的RNN与ODE：花式RNN简介</a>，除了开头的时候提了一小句来源，其它是一模一样的，不过我发现这个作者其它内容写得很有趣，很有想法啊👍。<strong>这个RNN的很有意思，有空要读一读在这里做个笔记</strong>。</li></ul><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt and David Duvenaud. Neural Ordinary Differential Equations[EB/OL]. <a href="https://arxiv.org/abs/1806.07366" target="_blank" rel="noopener">https://arxiv.org/abs/1806.07366</a>, 2018.</p><p>[2] 思源.硬核NeruIPS 2018最佳论文，一个神经了的常微分方程[EB/OL]. <a href="https://www.jiqizhixin.com/articles/122302" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/122302</a>, 2018/12/23.</p><p>[3] 匿名用户. 如何评价ODENet？[EB/OL]. <a href="https://www.zhihu.com/question/306937011/answer/561576636" target="_blank" rel="noopener">https://www.zhihu.com/question/306937011/answer/561576636</a>, 2018-12-30.</p><p>[4] 机器之心Pro. 「神经常微分方程」提出者之一David Duvenaud：如何利用深度微分方程模型处理连续时间动态[EB/OL]. <a href="https://www.sohu.com/a/405309910_129720" target="_blank" rel="noopener">https://www.sohu.com/a/405309910_129720</a>, 2020-07-02.</p><p>[5] AI火箭营. 一个深度学习突破的方向：神经常微分方程ODE[EB/OL]. <a href="https://www.toutiao.com/a6703302712311677452/" target="_blank" rel="noopener">https://www.toutiao.com/a6703302712311677452/</a>, 2019-06-17.</p><p>[6] 苏剑林. (2018, Jun 23). 《貌离神合的RNN与ODE：花式RNN简介 》[Blog post]. Retrieved from <a href="https://kexue.fm/archives/5643" target="_blank" rel="noopener">https://kexue.fm/archives/5643</a></p><p>最后一条按照原文提供的参考文献格式了~</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;读不懂PDE-Net，所以想先操作下ODE-Net&lt;/p&gt;
&lt;p&gt;结果是瞎读文章真心难读也读不明白😭😭😭&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="Machine Learning" scheme="http://maxliu245.github.io/tags/Machine-Learning/"/>
    
      <category term="PDE" scheme="http://maxliu245.github.io/tags/PDE/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读16】Isolation Forest——孤立的异常检测方式</title>
    <link href="http://maxliu245.github.io/2020/08/19/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB16%E3%80%91Isolation-Forest%E2%80%94%E2%80%94%E5%AD%A4%E7%AB%8B%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%96%B9%E5%BC%8F/"/>
    <id>http://maxliu245.github.io/2020/08/19/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB16%E3%80%91Isolation-Forest%E2%80%94%E2%80%94%E5%AD%A4%E7%AB%8B%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%96%B9%E5%BC%8F/</id>
    <published>2020-08-19T01:55:26.000Z</published>
    <updated>2020-08-19T02:07:45.693Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>这次在网上学习别人记笔记的方式，感觉很有意思，不知道我能从中学到多少。可惜markdown做富文本表格太麻烦了…但是用别的，比如word、pdf，都不方便即时打开。。。算了吧，选择了markdown还是按标题目录来整理笔记</p></blockquote><a id="more"></a><img src="/2020/08/19/【论文阅读16】Isolation-Forest——孤立的异常检测方式/1.png" title="文章简介"><h1 id="为什么读此文章"><a href="#为什么读此文章" class="headerlink" title="为什么读此文章"></a>为什么读此文章</h1><p>原因有二：</p><ul><li>是我最早接触的论文之一，当年没看，现在拿来读一下，本来想略读，然后就忍不住多看了几遍…</li><li>周志华是作者之一，应该很顶</li></ul><p>文献链接：<a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest" target="_blank" rel="noopener">Isolation Forest</a></p><h1 id="文献总结"><a href="#文献总结" class="headerlink" title="文献总结"></a>文献总结</h1><p>孤立森林是一种异常检测方法，侧重以孤立的方式直接寻找异常而非比对正常样本。</p><p>这个孤立的思想很棒，少了很多计算的步骤，但同时有很棒的准确性，毕竟基于的是异常本身的重要性质。</p><h1 id="文献内容"><a href="#文献内容" class="headerlink" title="文献内容"></a>文献内容</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>给异常检测领域提供一种新方法——孤立森林。</p><p>该方法来源于以往没人用过的孤立的思想来做异常检测，且可以发现由此提出的孤立森林有诸多优点。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul><li>大方向是<strong>异常检测</strong>，看着就知道很重要</li><li>传统的（模型驱动的）异常检测方法把测试样本与已知正常样本对比，不符即为异常。这个想法很基础很实在√</li><li>上面方法的缺点有2，1是<strong>泛化不好</strong>，因为方法效果与正常样本库相关；2是<strong>计算代价大</strong>，往往只能用于低维小规模数据，因为这些方法常常用到距离等度量方式</li><li>在此提出并使用<strong>孤立</strong>的概念，把异常点孤立于其它正常样本。思想是利用了异常的2个性质，1是它们真的只是少数；2是特征与正常样本必是不同的。由此思想，用树来孤立异常数据，异常数据会在树根部被孤立，正常样本只会在深处被孤立。</li></ul><h2 id="方法——从孤立树到孤立森林"><a href="#方法——从孤立树到孤立森林" class="headerlink" title="方法——从孤立树到孤立森林"></a>方法——从孤立树到孤立森林</h2><ul><li>孤立森林是什么：<ul><li>背景提到了孤立树的概念，集成起来就是孤立森林。孤立森林方法只有2个变量，1是树的个数，2是sub-sampling size，一开始我猜这个size翻译为二次采样的size，做一次全分割用的样本数；后来我发现我猜错了，在文章第3章中说明了这个是可以设置多个树，相当于多个异常检测专家，分别随机取一部分数据来检测异常，这些数据的size叫sub-sampling size，至于为什么将在优缺点部分介绍。</li><li>异常的2个性质体现在孤立分割上就是，异常越少，所需分割次数就越少；以及异常会早早被割出来，即异常的平均路径长度average path lengths短，图1很形象。</li></ul></li><li>孤立树是什么：<ul><li>先给出基本数据假设：数据集$X={x_1, \cdots, x_n}$，$n$个样本，每个样本是$d$维的，有$d$个分量。</li><li>首先是一棵树，然后在这里，是特定的一种树。用节点来描述，它的节点$T$只有两种，一种是外部节点，就是接下来没有分支了；第二种是内部节点，且分支有且只有两个子节点$(T_l, T_r)$，即左右子节点。</li><li>那么数据集X的孤立方式是，每次随机孤立一个分量，如第$q$个，孤立的分割阈值是$p$，这样把数据分为两部分。多次孤立的停止条件有：<ul><li>树达到临界高度</li><li>最后只有一个点了，即$|X|=1$</li><li>所有数据都只有一个值，没法孤立</li></ul></li></ul></li><li><p>异常的判定——异常score：</p><ul><li>有了方法的概念和方法的构成单元，下面说方法的度量，即怎么样算是异常。基本的想法是path（路径）长度：样本$x$的路径长度记为$h(x)$，在树上，$x$被孤立出来需要的分割次数。但是这个$h(x)$不好，因为这个$h(x)$算不算异常也要看整个树的深度、样本量什么的；因此需要一般性的异常score作为度量。</li><li>这个score来源于BST，二元搜索树，因为发现孤立树和BST有相似的性质。借BST文章的结论，其不成功搜索的路径长度$c(n)$与这里的$h(x)$类似，于是采用(1, 2)式的形式计算了此异常score $s(x, n)$。个人感觉就是$h(x)$不general，引用$c(n)$给它做了某种标准化。图2和图3解释得非常好，一目了然，分别介绍了定性的$s(x, n)$合理性和类似等高线的异常判断方式。</li></ul></li></ul><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><ul><li>孤立森林与已有方法主要区别：<ul><li>它可以利用sub-sampling，<strong>这个sub-sampling指什么？</strong>，是只用一部分数据就可以了，体现在孤立森林的局部：孤立树的使用上；如果数据多，往往树越深，正常样本也多，往往难以检测出异常。具体有两种情形——swamping and masking，分别指错把正常样本识别为异常，和异常点太多，比如扎堆出现，一看还以为都是正常的，其异常性被数量掩盖了。针对它们，只要取一部分数据就可以得到更好的效果，文章第3章画了两张图，非常形象地阐述了机理👍！</li><li>不需要计算什么距离、密度，大大降低计算复杂度</li><li>由集成的优点，大的森林可以处理大量数据了，包括高维情形</li></ul></li><li>孤立森林的优点：<ul><li>线性时间的复杂度，对运算内存的要求为很低的常数。1个原因是不需要计算什么距离、密度，<strong>另一个原因</strong>是按照方法中孤立的方式，即使树是完全树，最多（每次只孤立1个点）的内部节点有$n-1$个。最多的全部节点有$n-1+n=2n-1$个（自己画个图就知道了），所以对变量的内存要求是有界的，与样本量呈线性关系。</li><li>首次把孤立应用于异常检测，似乎用了二次采样，其它已有方法不行</li><li>它的2个变量都比较小，就可以表现优异</li></ul></li></ul><h2 id="实验——如何训练孤立森林"><a href="#实验——如何训练孤立森林" class="headerlink" title="实验——如何训练孤立森林"></a>实验——如何训练孤立森林</h2><p>和一般的机器学习方法类似，先训练出一个孤立森林模型，再拿测试数据测试。</p><ul><li><p>训练总体的算法是孤立树的集成——孤立森林，为算法1，递归式地分割数据集</p><ul><li>算法1中包括算法2，单颗孤立树的训练，这些都已经说过了。</li><li>算法1、2中有3个参数要提前设置：子采样数、树的最大深度和树的棵树，这些都根据实验经验确定，子采样数默认$\psi=256$，深度默认$l=ceiling(\log_2 \psi)$，棵树默认$t=100$。</li></ul></li><li><p>最后测试的时候计算的score要一直到被孤立出去，且使用递归的方式一点点计算。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这次在网上学习别人记笔记的方式，感觉很有意思，不知道我能从中学到多少。可惜markdown做富文本表格太麻烦了…但是用别的，比如word、pdf，都不方便即时打开。。。算了吧，选择了markdown还是按标题目录来整理笔记&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="Machine Learning" scheme="http://maxliu245.github.io/tags/Machine-Learning/"/>
    
      <category term="Abnormality Detection" scheme="http://maxliu245.github.io/tags/Abnormality-Detection/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读15】Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation</title>
    <link href="http://maxliu245.github.io/2020/07/31/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB15%E3%80%91Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation/"/>
    <id>http://maxliu245.github.io/2020/07/31/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB15%E3%80%91Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation/</id>
    <published>2020-07-31T10:01:04.000Z</published>
    <updated>2020-08-02T17:01:16.804Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>论文阅读</p></blockquote><a id="more"></a><h2 id="文章简介"><a href="#文章简介" class="headerlink" title="文章简介"></a>文章简介</h2><p>这篇文章[3]是在ECCV2020的日程[1, 2]中发现的，拿来一读。</p><p>原作者的部分讲解视频在B站BAAI发布的<a href="https://www.bilibili.com/video/BV1wZ4y1M7qw?p=4" target="_blank" rel="noopener">ECCV 2020中国预会议回放丨计算机视觉、目标检测、图像分类、特征提取…55场前沿学术报告集萃</a>P4的181:40-192:00。</p><p>从标题上看是提出了一个对抗网络的变种，<font color="#0000FF">双重对抗网络DANet</font>，针对真实场景，<strong>同时建模（生成）噪声以及去噪</strong>。</p><img src="/2020/07/31/【论文阅读15】Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation/1.png" title="文章简介"><h2 id="主要内容——DANet"><a href="#主要内容——DANet" class="headerlink" title="主要内容——DANet"></a>主要内容——DANet</h2><p>ps:</p><blockquote><p>主要内容直接参考B站视频或者原文即可。</p><p>个人不太想看相关工作的部分，只看了噪声生成的介绍，模拟噪声分布的目的之一是解决数据对的成本问题。其中传统方法GAN的难度在于如何找到好的噪声生成器。</p></blockquote><h3 id="模型提出"><a href="#模型提出" class="headerlink" title="模型提出"></a>模型提出</h3><p>本文的主要思想与之前的<a href="[https://maxliu245.github.io/2020/06/27/10-VDN-Variational-Denoising-Network-%E9%98%85%E8%AF%BB/#more](https://maxliu245.github.io/2020/06/27/10-VDN-Variational-Denoising-Network-阅读/#more">VDN</a>)[4]如出一辙，感觉是顺着VDN在贝叶斯框架下同时进行图像噪声估计和图像降噪的思路继续做的。主要内容是提出了双重对抗网络<strong>DANet</strong>，下面具体介绍一下。</p><p>相比VDN直接贝叶斯框架完成两种噪声相关任务，这次是间接地、从另一个角度地对干净图$x$和带噪图$y$的联合分布$p(x,y)$进行估计，并分两种分解形式。因此从两个方面分别建模，从$y$开始建模，生成$x$的过程就是训练了降噪器$R$，反之就是训练了噪声生成器$G$；两种方式逼近联合分布后通过判别器$D$做监督。这种方式称为标题的“双重对抗”，$R$和$G$相互影响，共同逼近联合分布。</p><p>$x$，$y$联合分布的两种分解很自然，从联合分布定义即可，实际意义参考视频中的图即可理解：</p><img src="/2020/07/31/【论文阅读15】Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation/2.png" title="联合分布$p(x,y)$的两种分解意义"><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>接下来确定对抗网络的结构：</p><img src="/2020/07/31/【论文阅读15】Dual-Adversarial-Network-Toward-Real-world-Noise-Removal-and-Noise-Generation/3.png" title="网络结构"><p>其中降噪器denoiser记为$R$，噪声生成器generator记为$G$，都用UNet，并加了残差策略；最后判别器discriminator记为$D$，采用5个卷积层和一个FC。</p><h3 id="部分训练细节"><a href="#部分训练细节" class="headerlink" title="部分训练细节"></a>部分训练细节</h3><p>训练的目标函数是Triple-GAN中提出的损失，加上两个正则项（对应于两种分解）。前者是干净图的$L_1$损失，可以优化$R$；后者由于噪声随机性，采用高斯滤波后的某种$L_1$，优化$G$，具体形式参考原文(6)式。</p><p>网络训练的过程参考原文算法1即可，其训练的思路在于向后传播分两步，交替优化$R$和$G$。</p><h3 id="部分亮点"><a href="#部分亮点" class="headerlink" title="部分亮点"></a>部分亮点</h3><p>首先是继承VDN的思路，同时完成两个噪声相关任务。</p><p>其次是在概率框架下从两种联合分布分解形式的角度设计了DANet。</p><p>再者是设计了两种噪声生成质量的度量，比较了真实带噪图和生成带噪图的相似性：</p><ul><li><p><strong>PGap </strong>(PSNR Gap)：借用降噪指标PSNR设计。已知训练集$\mathcal{D}$和测试集$\mathcal{T}$，分别是一堆成对干净图和带噪图，用准备好的噪声生成器$G$，对训练集进行加噪，得到生成的新训练集$\mathcal{D}_G$，准备原始训练数据上训练得到的降噪器$R_1$和在新训练集$\mathcal{D}_G$上得到的降噪器$R_2$，分别计算测试集上的PSNR，其差值小，代表噪声生成得越好，越接近真是噪声分布。</p><script type="math/tex; mode=display">PGap = PSNR(R_1(\mathcal{T})) - PSNR(R_2(\mathcal{T}))</script></li><li><p><strong>AKLD</strong> (Average KL Divergence)：这个比较直接，希望生成噪声的分布与其真实分布接近。真实的分布用了VDN的结论，然后采样；这里的分布是由生成器$G$对应的新分布，都通过采样近似。</p></li></ul><h2 id="DANet-V-S-VDN"><a href="#DANet-V-S-VDN" class="headerlink" title="DANet V.S. VDN"></a>DANet V.S. VDN</h2><p>与VDN的区别？难道是VDN传统贝叶斯框架条件后验，这个直接联合分布？主要区别好像是的耶~</p><blockquote><p> 参考摘要</p><p>Instead of only inferring the posteriori distribution of the latent clean image conditioned on the observed noisy image in traditional MAP framework, our proposed method learns the joint distribution of the clean-noisy image pairs.</p></blockquote><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>[1] BAAIBeijing. 计算机视觉顶会ECCV 2020中国预会议：日程公开，注册有奖[EB/OL]. <a href="https://blog.csdn.net/BAAIBeijing/article/details/107588080" target="_blank" rel="noopener">https://blog.csdn.net/BAAIBeijing/article/details/107588080</a>, 2020-07-25.</p><p>[2] 智源社区. 智源社区活动[EB/OL]. <a href="https://event.baai.ac.cn/event/53#section-four" target="_blank" rel="noopener">https://event.baai.ac.cn/event/53#section-four</a>, 2020.</p><p>[3] Zongsheng Yue, Qian Zhao, Lei Zhang, and Deyu Meng. Dual adversarial network: Toward real-world noise removal and noise generation, 2020.</p><p>[4] Yue, Zongsheng, et al. Variational denoising network: Toward blind noise modeling and removal, <em>Advances in neural information processing systems</em>, 2019.</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;论文阅读&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="Denoising" scheme="http://maxliu245.github.io/tags/Denoising/"/>
    
      <category term="Machine Learning" scheme="http://maxliu245.github.io/tags/Machine-Learning/"/>
    
      <category term="Computer Vision" scheme="http://maxliu245.github.io/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>【Windows技巧2】用画图编辑图片并保存时出现【发生了共享冲突】</title>
    <link href="http://maxliu245.github.io/2020/07/30/%E3%80%90Windows%E6%8A%80%E5%B7%A72%E3%80%91%E7%94%A8%E7%94%BB%E5%9B%BE%E7%BC%96%E8%BE%91%E5%9B%BE%E7%89%87%E5%B9%B6%E4%BF%9D%E5%AD%98%E6%97%B6%E5%87%BA%E7%8E%B0%E3%80%90%E5%8F%91%E7%94%9F%E4%BA%86%E5%85%B1%E4%BA%AB%E5%86%B2%E7%AA%81%E3%80%91/"/>
    <id>http://maxliu245.github.io/2020/07/30/%E3%80%90Windows%E6%8A%80%E5%B7%A72%E3%80%91%E7%94%A8%E7%94%BB%E5%9B%BE%E7%BC%96%E8%BE%91%E5%9B%BE%E7%89%87%E5%B9%B6%E4%BF%9D%E5%AD%98%E6%97%B6%E5%87%BA%E7%8E%B0%E3%80%90%E5%8F%91%E7%94%9F%E4%BA%86%E5%85%B1%E4%BA%AB%E5%86%B2%E7%AA%81%E3%80%91/</id>
    <published>2020-07-30T15:15:50.000Z</published>
    <updated>2020-07-30T15:31:27.820Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>用画图软件编辑图片并保存时出现bug（背景是正在用<code>python</code>导入图片）</p><p><code>发生了共享冲突</code></p></blockquote><a id="more"></a><h2 id="bug简介"><a href="#bug简介" class="headerlink" title="bug简介"></a>bug简介</h2><p>不多说，标题已经很明确了，直接上图：</p><img src="/2020/07/30/【Windows技巧2】用画图编辑图片并保存时出现【发生了共享冲突】/1.png" title="发生了共享冲突（请忽略卑微马赛克🐕）"><h2 id="经典解法"><a href="#经典解法" class="headerlink" title="经典解法"></a>经典解法</h2><p>网上有诸多解决方法，其原理都是认为原始图片和自己编辑过的图片，后者并没有在内存中覆盖前者，二者在存储的时候会冲突。</p><p>本文<strong>唯一引用文献</strong>，这篇文章讲得就还好<a href="https://www.beihaiting.com/a/YNJD/XTGZ/2016/0620/8922.html" target="_blank" rel="noopener">Win10画图保存编辑的图片时提示发生了共享冲突怎么办?</a></p><h2 id="我的问题与解法"><a href="#我的问题与解法" class="headerlink" title="我的问题与解法"></a>我的问题与解法</h2><p>但是！这个方法对我不管用，因为我删除原始图片时会显示<strong>画图软件占用</strong>：</p><img src="/2020/07/30/【Windows技巧2】用画图编辑图片并保存时出现【发生了共享冲突】/2.png" title="画图软件占用"><p>这时哥发现了端倪，平时编辑图片也没这问题啊？问题指向了<code>jupyter notebook</code>界面的<code>python</code>代码，它导入图片失败，我正调试bug……</p><p>于是我直接关闭编辑中的画图软件，直接删除图片，果然，<strong>python软件也在占用</strong>：</p><img src="/2020/07/30/【Windows技巧2】用画图编辑图片并保存时出现【发生了共享冲突】/3.png" title="`python`软件占用"><p>所以！我的解决方法是关闭当前<code>jupyter notebook</code>的<code>kernel</code>，🆗了</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>咸鱼生活需要bug滋润……</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;用画图软件编辑图片并保存时出现bug（背景是正在用&lt;code&gt;python&lt;/code&gt;导入图片）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;发生了共享冲突&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Windows" scheme="http://maxliu245.github.io/tags/Windows/"/>
    
      <category term="Bug" scheme="http://maxliu245.github.io/tags/Bug/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读14】MFM——针对重尾分布的元特征调节器</title>
    <link href="http://maxliu245.github.io/2020/07/23/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB14%E3%80%91MFM%E2%80%94%E2%80%94%E9%92%88%E5%AF%B9%E9%87%8D%E5%B0%BE%E5%88%86%E5%B8%83%E7%9A%84%E5%85%83%E7%89%B9%E5%BE%81%E8%B0%83%E8%8A%82%E5%99%A8/"/>
    <id>http://maxliu245.github.io/2020/07/23/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB14%E3%80%91MFM%E2%80%94%E2%80%94%E9%92%88%E5%AF%B9%E9%87%8D%E5%B0%BE%E5%88%86%E5%B8%83%E7%9A%84%E5%85%83%E7%89%B9%E5%BE%81%E8%B0%83%E8%8A%82%E5%99%A8/</id>
    <published>2020-07-23T09:07:56.000Z</published>
    <updated>2020-07-24T16:00:18.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Please input password to continue...</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="2d7b5b9cef4a173e9dc29c2fda836be7d45b8bd304f6cfd43c1d5b58bebd2d0c">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</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      有缘自会开放该博客~
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="Meta Learner" scheme="http://maxliu245.github.io/tags/Meta-Learner/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读13】PDE-Net: Learning PDEs from Data</title>
    <link href="http://maxliu245.github.io/2020/07/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB13%E3%80%91PDE-Net-Learning-PDEs-from-Data/"/>
    <id>http://maxliu245.github.io/2020/07/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB13%E3%80%91PDE-Net-Learning-PDEs-from-Data/</id>
    <published>2020-07-22T04:46:12.000Z</published>
    <updated>2020-07-22T16:14:40.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>早就想了解PDE相关的方法啦，来凎！</p></blockquote><a id="more"></a><h2 id="我的早年想法"><a href="#我的早年想法" class="headerlink" title="我的早年想法"></a>我的早年想法</h2><p>其实很早我就有过这种想法，从一堆奇怪的数字序列中自动得到背后的数学规律，这个规律就是PDE啦。</p><blockquote><p>ps：记得当时是看蚂蚁活动突然有这样的想法？？？</p></blockquote><p>但是当时对神经网络了解不深，不知道怎么去模拟PDE中的算子。n年后开始磕盐才发现已经有不少这样的研究了，今天就来正式看一看！</p><h2 id="PDE-Net"><a href="#PDE-Net" class="headerlink" title="PDE-Net"></a>PDE-Net</h2><p>元学习器，即<code>meta learner</code>，在我理解是机器学习从传统的<code>训练-验证-测试</code>向元学习<code>训练-元数据-泛化</code>转变的一种产物。</p><p>以MAML为例，它的核心在于能够找到一个通用的初始参数，能用于一系列存在联系的任务上，使得相关联的新任务使用该初始参数后就能快速收敛，达到较优的性能。但是我对此有</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>大背景是PDE应用广泛$\checkmark$，但是现代应用场景下的复杂系统遵循什么样的PDE规律我们并不清楚。我们清楚的是可以收集到很多场景下的数据，因此以数据驱动方式，设计前馈神经网络PDE-Net，能同时预测复杂系统行为，并揭示潜在的PDE模型。</p><p>其设计方式是用卷积核拟合微分算子，其实就是离散地逼近非线性可微算子。逼近的时候，会通过微分算子的阶和卷积核加和规则的阶去合理地限制卷积核。</p><p>PDE-Net的优点：</p><p>预测系统行为的时间长，即便数据有noise</p><p>对卷积核的限制合理，既保持逼近的微分算子性质又保持相当的向后预测能力；很灵活，即又逼近潜在PDE，又能预测。</p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>复杂系统的PDE不知道啊，只能通过数据瞎jb学了。</p><p>列举的前人工作最早09年，多集中在16，17年。</p><p>启发之一，ResNet与PDE有关。</p><p>The main objective那一段的思想很棒！</p><p>我tm看不懂了</p><ul><li><a href="https://www.zhihu.com/pin/974609149418479616" target="_blank" rel="noopener">https://www.zhihu.com/pin/974609149418479616</a></li><li><a href="https://blog.csdn.net/qq_30883339/article/details/86022111" target="_blank" rel="noopener">https://blog.csdn.net/qq_30883339/article/details/86022111</a></li><li><a href="https://www.zhihu.com/question/266075899/answer/363848527" target="_blank" rel="noopener">https://www.zhihu.com/question/266075899/answer/363848527</a></li><li><a href="https://www.bilibili.com/video/av65293637/" target="_blank" rel="noopener">https://www.bilibili.com/video/av65293637/</a> 40min</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;早就想了解PDE相关的方法啦，来凎！&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="PDE" scheme="http://maxliu245.github.io/tags/PDE/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读12】Meta Label Corrector——设计元学习器完成错误标签修改</title>
    <link href="http://maxliu245.github.io/2020/07/21/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB12%E3%80%91Meta-Label-Corrector%E2%80%94%E2%80%94%E8%AE%BE%E8%AE%A1%E5%85%83%E5%AD%A6%E4%B9%A0%E5%99%A8%E5%AE%8C%E6%88%90%E9%94%99%E8%AF%AF%E6%A0%87%E7%AD%BE%E4%BF%AE%E6%94%B9/"/>
    <id>http://maxliu245.github.io/2020/07/21/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB12%E3%80%91Meta-Label-Corrector%E2%80%94%E2%80%94%E8%AE%BE%E8%AE%A1%E5%85%83%E5%AD%A6%E4%B9%A0%E5%99%A8%E5%AE%8C%E6%88%90%E9%94%99%E8%AF%AF%E6%A0%87%E7%AD%BE%E4%BF%AE%E6%94%B9/</id>
    <published>2020-07-20T16:15:59.000Z</published>
    <updated>2020-07-21T02:22:46.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Please input password to continue...</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="3594e7eaa513efc4c7dc9235bd2667257d90bda111adaf0df9729ac6cac61fd5">acd06079641cef667d5ebc5c77e664bd808d11cf554c712b0e8004a28cdb8f9d713d42c8e461b0dc25a08f6a56176980487b88911efbc1b8926e6c062c2fa7bf21fc648a0f7c54d63b5f695ac06bf5e34a9eb61ccd90d99c38aa3b95a386d2359b2937dfe48b428e5a4f65a08b67fc1a7a3b75af7a7e649431d5dedb6bb74bee6f7ecf6004d46739e848bd8b6c4f3439f8a81bca0c3887e95163e289b4f16280baf480e012265bfc2391b21605b28ed0ead9ff32bd6ff61328b4cf77bb9f495894f354f87a8a38970fe0f724f0697a3a8375c2cd4d3bddee598a27844db649eff228d60617f6ecc7a69d7e1d41f9d473ae598de35133f6aa3f485a952cc6dc3173cb619824fb27025a67541eea7cde77ed9269034a9aa2c8c70b8b7997f60e833d530b1cefa1ae1a1a1a8a66939b3061768f1872bbf3530ad077b17274f19cd472dfa19d9ec87fc9595741e8a4afeb43b543e4d06d33ce0980dee0d6c8889e693dbcdc120cdc9221e74eec8a928e81820ae536d61bc1aa2a2fa1f1acaf9592b69f231df2701339729f20257f7332c244ea139e003136e7dd1c9fe3619c516b6616c6912aa65f0df6b9611518296252f881d50637794f7d6daee41623540c4416a767e47d9f64a8567d68527e7a8d4d02530d958808b9cd3b56003b40e8d252fa3f9ba40b9e5d89241d38f45d7dd1b3d697cbdf473824cebc71b971be05f443b94c428c87f526cb1bca0392ff9fbfd265664bf4e0b09571e73cad2359d8bb2fea97dfc0432890ea83778771c6d78ec8291bc7485ee0fa5643c1a38d74e69a9d260f618a8d908ecf578dc8e87e712dc94c8ee6112200cae7058eeb38e382b79bb2766f9a28b3df87c2e22e5c5f5d46e359539ecf247d23a303f9f0db1d939d875fd5e68b9ea5e6824af69715b0ee234446ac3e1c4162f9f6f5852ae22a3a1f887adbefbf3c1b93311fa50368ecc7af526f547b334793eaf2d82c61a93087d51041f34f80ff4368d35900afe5bfd0bdf06364b03ff07d195aa776d8bcb28ed56f84bf9dda735a59684e0463ebe8c6a424cdcf23bcb8cfba49fa9edb86ba1bf8b7ea5eb6f7c001cbd2b26b22a78b07eeb297322d2338b39e6901278a07ddc440290124d887825f936b4f13a3631a0bef1e5ef922402ddbb70cb2a051e55273b71ccc315033cb2dcb9d1d1451846de425c1a7e3e12151f4c9f75b3657bd0f4e779d588955956eb2b246dabf0ae34cc628a70548c288edbdb90b0a6f2f544357ed16f690ae3d887bcb48adf54a054dcea5e9e3f4cc543fb3a586e762a27c3f84cf3c37cc803f16e847d371f00137a0947815fad659855decb2bc33031daa2f7a92311649930c2f5da49dedcc9ee1464f829f81911cfc29d4998d92b2e81fb79e6e3dcda2b37ab90679fb2666845dc826f37a0d77f76ce68cfc89cea25d4a15cc7fbdd3a54e7c2b0936dced14db39b7445f3998beebd5b85c15b2bdb4da43a49a4ffc3f725200274b2ad46f36c8e9be374911c7abcfbb75a8f089ab4f61b45d50485e6708460eaa347f33f1d25920f1bd3097328125c8609113d17b3197d313298a82a5b03a92df1bc3cfe740f1147e34c40cfcd752af8b8a86e78425177ae5a1f745ea6c768cb17511eca72d052b467c2ea68f3de839f22a0ffb4c57b3f9b847739240e9677c526f405e1b61af75da9ee9f743a7493de0bd6775b8b133937527479c90922b7e1c84c143a5d4fd1d831da3a658d4f0da87b6316c2bb29a3ccadbc24be1113472283adcd7269bc1988637437939faf8b6e5c83fd00a6af09ef1f5c64bf832fbad0ddd7aed4724c437c9d26d85cc3e7af9fd6577db6f3d885a9a4e451e8a2db323af591f786285a2400db69ef4f58b395d16309b56488c2276f464001a9d20a734793c27f79d956521e3d8ac28971bb67f19bdd8d1b4abdc4c752a112fc94bec1cb3e0939afd58c26b02e84931e834cc32148b0838791a97313f02dcc1b28a32b2078f34507d0c2c19b55fd74efd1f26573bcb6647091c92be8275fa189e81dd966a51ce8a88ac8d6ea651484845ab85a07c2eaec2f6ae14248faa3e99b4e98aaf7c1d5880caa0a12943c05c39296c736d89f2e5eba49d8c38d220ba7e9d427d40f5dbb7ccefceb792e9ea5a9e79bf460d1f0d584dac386c3f73ace31b1918438bf1b3b24c0a630e446280a5f9e19c4aa59c7187484871e6fd928d2eef5b75c8ae438c9ec4e0fd599e2b6b5298938227cdb593ae0e84b442101554ba1ab77ec45f97d771bf6dd16ba30aee368ffa8692fc22d233f4d442042a3e5ba12e2b102ca37cb6eae8c01154a8e62bb939e88d17fed9086c5bb25a6eb4f9fa0700598c72bd3bfec08b2a73f655c0c2b36e292277724bb4b1455c84c4af9daae0e76e0ec045516cea86a63ab1ea6ac4f3e7237e5bc82fad6ce9d645baa8f3b470277f43f7e51e155ac09c4a3752d1f7f43cdf3bd707389251241aab1f4e3d7fa0f4925027c7b525ada33d3a6c124d5b028ca0853a3be673a7fbb4fb6e66a3d3e15fe6c7aa86fb3fde9ae69b33388149888d4dfa677c2e439f5a2daec5ba7f6b08294b6fdcf095ef74b40197ad46bff0fe0a439f2a5b97c957ce336e6283842b1badca1e292e5685cb3b1add38e2252658ea768e48eaa8629c454344b53822b9b79602710daa64d9d4e39f14590d8ae451ae22878ad3d3ad29fe220cf3a8ddc51a98c63499b0624c707da3184bc691d306d6cf0f422048587358362bdd332000d4811b3cae7ced740ef77a34b6e5b19c34ed5b162aecc2fdfe2f96fa27fe8ef7b2921378059ca6693aa6272b6124421b872a907a1b418c34078c3b76181a0644aaf407f750e4edafab1329f7edff81fb8ae465fadeb80f1218effe9dec21996ca95a71292e621b70b57a7f000d6d106fa62587e850dad84ab3399963fead6785b15471a2ee79b14c527b8c39095aa91e11ddf9bbb2f124c6fc1333f2ac055328029eaa94b2d52ff22ff1e9f1ff023d97ca5d4009c10292716bddc3d99f99580fd502c6a23b92a6cd58e3eed5c8242f7dd91717368e66fbc3b634c0bc1a1d5e3032dec62629ba101c730c36b0e9113d9d389fa2edfa03cdbe96a3371021fa22fb3ba1811ac918a3b5484f1753346da2a2320f383c660c6deacee2981e37294c9a2732997af9e1a4622584b496d5ec18ca85f9e3d78350d0a61f0ab2f72796d22d58ddfdc39645d203da887a2bc423d7fec5779dcabc527b70d65a0d90bb7a73b5680ea917164b2629206c83cd7f8807a02790be90fae84c579b8bb5d40e54700f56486baddb26c54e3328b1c65886fcc475d40d6d5f0db8a98abc9b71f94429d5c64f7e192a0d6193987e243a158b759ee990de34240942ae954a3841b094a6f881ade12b82aa30080d1445ce84a59a663d4bccd2c3505adec0f10116e61bed819b36eb38ab0a8477fae3e957e49eecfec9ae54a7f20dd96afcb99c3da712b04c8b139638ad3d0d68b14abc7726216a79f22d53a1988ca5925d4dd0cb8a5981bed2bde025675cf037004f02aa5aa0e6c9112d4c4822815770205c6f2f37328a11174d3729e386fa830459eb20808404d2c415d641c069f4d7f583398577200f13332d66c00a8b79504ce616a24f732ada836a4c1dd25c63a0492ab4fd002e68604802da33c48ef596972da837fc6477ae01ef2bdf67c1575d12c4530062f8178eadd5463951852891c0a2b440b16e9f5929a84b03f1cc0d01edda78acd866be7a416b392ee467e68a8d9d387a2ad3a3435fecbfe2912ffff878f2a0cdd84250a1030a73b36fb925a91378c7e0f238c128f942294cc837eeea33774fa76c88b1a247ddf39504de959991b898880b2cc5a47da8c44c91dce5589b4a8ac709a52bc3a993995a3d628517d70c40681387269a16d659bce01bc84b6e7562e20f4b2a9b174358f7906f399f2bee8c43f9177c9af246dc2e9d348842e5867178c63e61ed5fa29931a3d36d7e02ab0f129e78ed20a2668065995659feebaa6c4ecc420d871a4ff26c1cfb0037b7146234f24b656bac91c44283da6dd9350405ee5485b33b4edab27ac5bbdce713e299ec75477ea781fe43c4fbd04b9e527de06e25ec32963b1ac377d0bd7f2449f0feba12753cf668d13c54ca43080fece859d27d8781ee0c597b44147055e53700012c9607e2ba085e96dde8e104b2d795975f19881fa3cc037bd6f3798cb32528900dcdd62c44322199322d9a2046530edfdcfb6ae8b81741094d7af8a198e12bb8ba59a0c13e9017b74232f33af10b0785af482f876d72201f0dd6c53835f7104accd27814c3a2a0cc84d0d1b2c5875c4f9014213d906473e6e01c5f43223f0dd904167d4a4fa59ed32c376ca48d2d78a4d4a11bb1e1b4f3462a1adbb2e49d3e0d26652dbb1aa810ebe4174739300191d4dd339f8183f3eec6cefb4acb33ab859ce7c5854b688aafeafc6158191cb871f9e47bf7e0d440b8251a1ae9dc43f5ebd071ad11285ea1d731ef67d68bd11e23549746d45d6c08407d25b2de9fcc7ab8aae911d1d20acec9733037f16f80274e5bec694d1a41ec8e430ac4bf5b9a19191a3103791296455d387c2792779ecb79eb5aaf831c6956a559ad1cb592c86c07e13e177583e8fdb90bbbc2eed7311b01d730d4b097e4b3741d3e240bc382c5a43b77d18bcb7be89b1dd0b84f330a3960c1a5ccb5dc56c37bf32f5d5bcff4f5d7bcb32ea6cd35a32bdf612133adb02fbf4f6d3bf747b7e50ecd3ba7f3f604df21584d04697797066c66a00f831127739b73bfaa3532d9797ea19165ac9223f5f489ad6bbce75d5358c33186046a83a2093f6908c8926e0a50c5c2f4063505b0803f4864e85c3c4ba665dad5ffcd192a302408b783cc95ecac9bfcd2cfcc501ac0fc6aecabf4b7f0789919c96b37744405dc5d03bb935eda4df70302a00538f25a82f6663bbad0f9052137940d40781a4a0d107107066cc1f824722718e59348a0363565da6311ac003eb42c1b7c6b543020354322c96951c39a8a6284c63264a80973ece6d19396d7befc92f7369c2cab39b9f5c3e1732f8b87b84a8ed390fa5c1ebd2cbb92affb1f3fd72f30288f8b4393bdc0c664a7084cbb121e9475a881f9fd5ddd124c816a09dee9e8f08ab6e95b4c21558e991a3631c83127b7d2924372cabbbc22ccc9cf26988457051f7daeb456ab224b5da9c1dbac0b71abae5263ae9f7533256a4bbef2f4373757bea922194f3ccd9dd3d239f59f203b390c0900656242cd874fadad16243fe52a8a5f504acb2cc32fca2e41536f910e1d09c686274e4223d7140518fbb561dbb8d0a4a926abfd308fbae0faa9580ff43d263247e2049a5ebf7f39cf11d640749c47256138286f260828dc08f11ad18cfb174544756d070aa59210c87ef06ebe0396ec892b80bbfd112337c63ed08c7896c8f9436de2a5ea2b80b29aa415fdece62be5dfdb9f54cc9de668cb9d942e88d9411e42e85599caa151a419b1e9f21ff80d5f453b720d647104a9964ad6e7a7ae2f3e0da774517f38ea0a7247a91352c511bd63bf8c47519d8f1c27b2ac55657e480298582a605ee783d54cc503baf88b0b62ae76bd334c486a6070fe4bcdcc92c8b3d1acb6c9911f1ba0f1f6381b726dd4eb54cbb9c49c7bdc5da62b1d30d9981c96c2e4741638ec2827e0eb157aa1c0b51200a2f98ece2a4adae2afc94dd810894967eae7487c9927a8ac1dffb1a5f41238ed05a082dfa35ab3e93537170bb11945ab341facd2e31ab8250e5f223d9ad8d430c54330b94f2fab6c9540fba02f3bf48bc8fdb223e46543af239cde71777e878857c7688cea9cdab6cf2b19aa6b540832971132228c69fb271bb47bea2945d0cd65d4df377345f822a033e916c5bba37def3b65ea660680980b912bfcf59e5bd2919bc5cdd9943471688a8f3c7efd20d34ecb887b67aee45d8cfe060cd961822aa8f4e4c72ced4c3c037080f98de401c012fda57026a460c4d9ca00986054b32b259f4d7cb67e50805ac8a1f65bf75adac263ac7798fe8d21a2010777d68ba740368b8a72c99eb9d5f3a819ffa172c440534b583e129ff3ead6440664fc16dd60f1b24046001da929b21618cf73faae57c76fb115d39f76bd7cbb5260142a3cc85c34b4a1618c5d02bd4a61e5134e88c048176a4c6fac1b05c710c386c130f9d39509360286eb66e387604c7d46432a915b105ea1649cde6b5eb3adba1c5d8d72f0d3544de00def892fec6a2eea3cfab632c30c6088300cfc36c08619d41a3e4589d4320e41c144899efc95bf155c6e0739c4caf7479db1f7e987fb84b830fd21a289277a52eba7d9dcf91e7705ef442f101b4e9fcd56ce7a5e4ad4c0f3ade83a78ec09d1e0ae128ce5bbd4b0044b73cd35efa8a59a2abf4dc6ce647defe1311257ef13b5ab27e9651afcef7c153a96f4e6322458919caea6b90496778af339cfd067db171489dcf1df0f54eec77facc6abd81f931084717a5ee1dd24a37cf8a2fe613116f4a5d605f6edcf056e78b20a24d28e5f5e6aa58da484c05c8be51a8a73093cc26aad14fdc6476425f2a884ed7535201a1a8a4c256c3102beff49358550c05ef69bee2c5666b2e4630d23bdfc8d6ecc65118683d1a2dbaddf5d27f06115a55d52c5ca73316d8fafb239e3427acb6e6151ddc1c505e1815f52bfa41c15d93f2e8d957e0fc189f7763eb4860db677a07607c7e516f6a0b4ca6799e20fd46e81daf9430d3bca5045bfa048aab468ba3aba5a9f6f64b5119bfaccac8342a846637b858752bceaa421f27c1633a83d7e552478bd61da137729bb1f2bef552529ea2a27e644fe14530f1e60a41ae3ead6cab0abb0ff844ecf1f0b652b71a72b89b3155aac6154791dbae380fcf415be616ef97e19bd3430b80bf36ac2d83ece03a3bca69cc4fb3e58cc856cbcfc8be49ae64ebcdbbe40618f37f70a3854ea8442df666925e4bd5e7b6fd9906c8c7dbc30568a975c6df7890b351260314038b56e241eb77e6cf18279152e3ffa004db212114be88558e2495f81455a794aa46027c1b285206a6222a4ce0a3fd306bd47eba16266c77701c5dd67d22cb2f3de1a227c8264dfe3adad9dc311ca5a5f7f66d08dad6ad43f42fd0c516a2c7af124aa303737e9050595ec3fab055240964bb64cb3b046b1e6f3016c16239860b35dfef6cdaf8fb9aa7a2abcbd600ad381123e8b797bf0f3d6785702e8c9dac8dad95d3756c283b4f2c6ddf1e4f1ecd22b861ce19399a166ff2a9c766cf229b3b80818da8b60ddb65269fee7fde144f6d37a65088d7107fa4f611873d32f122257db4cfe4e1f0cbe278c29ffa308374bd618c7b8b189ab44f70c6da48c0468ed35ea3b3fbbbe8341924c861f41a5f0a50f226fd3616ebb888480d24b6a57486b433d633ce0be5a0d509c151a9d7c8d763c15c0c584698d11a378b7c7868f1fa9f824915681b9d4b6ac3a4b36315657c242d0d43a17dbffc2df1701d28510f41d372009acad66cbc74b5830b3a0d2b340d164e9de2e6725133ab0d2cf931e48aec6939fa82de56366d436c418b1aafc254cc21557d9c3bd8b9184ee242dd32f7cf63a4add382b5f83b6b7eec94358844914474ad3652171345d9cc4cfe16f9ac15b45e995e2d773a1f06af150dc900a8439f7ebeb4518f60904b7490813ed5679536ea6ddd949bd3669ca0ee18674beb097a2e7d97dcbe02336dea495b185c677bf90367d5959743310fdab8b01e556d3064981bd093c6651eebfd76e702be5fbd0325edd3ed54deb40a3c86f2252167709ce342cb808e6ec193e7842d6ab74892f1beb08ca56d5c8bf7c8cc1fd529cbda88c0f0f2d0ddd134db4a26e5747dba42749b4766a02ef8aaac967c3d8a968c361a35d5ff12909230cd85a2de6ea0ea3c1828e4f38706c7b10922e566a01a9fd70701d2a0bd29c520af11b6af397246bdb683feaad3500f82a2ba7886392ec808b14ef1cbad0cc83dede6b5b39371441863e4a71037908efd6576566722b0a9f3bb7c1035485807a83b339c29f0ad5b292692532158a22d097699ba41d</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      有缘自会开放该博客~
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
  </entry>
  
  <entry>
    <title>11 Automated Identification of Chromosome Segments 利用染色体核型图像和条带信息分析易位</title>
    <link href="http://maxliu245.github.io/2020/07/05/11-Automated-Identification-of-Chromosome-Segments-%E5%88%A9%E7%94%A8%E6%9F%93%E8%89%B2%E4%BD%93%E6%A0%B8%E5%9E%8B%E5%9B%BE%E5%83%8F%E5%92%8C%E6%9D%A1%E5%B8%A6%E4%BF%A1%E6%81%AF%E5%88%86%E6%9E%90%E6%98%93%E4%BD%8D/"/>
    <id>http://maxliu245.github.io/2020/07/05/11-Automated-Identification-of-Chromosome-Segments-%E5%88%A9%E7%94%A8%E6%9F%93%E8%89%B2%E4%BD%93%E6%A0%B8%E5%9E%8B%E5%9B%BE%E5%83%8F%E5%92%8C%E6%9D%A1%E5%B8%A6%E4%BF%A1%E6%81%AF%E5%88%86%E6%9E%90%E6%98%93%E4%BD%8D/</id>
    <published>2020-07-05T02:03:30.000Z</published>
    <updated>2020-07-07T04:05:54.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>该文利用DTW算法以及染色体密度谱曲线判断易位信息</p></blockquote><a id="more"></a><img src="/2020/07/05/11-Automated-Identification-of-Chromosome-Segments-利用染色体核型图像和条带信息分析易位/11.png" title="DTW and Chromosome density profile"><p>这次读论文尝试了新的记笔记方式，然而现实是残酷的。$\LaTeX{}$写着是真心慢呜呜呜…</p><p>附上笔记么么哒~，参考文献和链接一并记录在pdf文件中。此外，笔记仅供参考，<strong>请勿修改此pdf</strong>。</p><div class="row">    <embed src="20200707-DTW-blog-pdf.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;该文利用DTW算法以及染色体密度谱曲线判断易位信息&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="DTW" scheme="http://maxliu245.github.io/tags/DTW/"/>
    
  </entry>
  
  <entry>
    <title>10 VDN-Variational Denoising Network 阅读</title>
    <link href="http://maxliu245.github.io/2020/06/27/10-VDN-Variational-Denoising-Network-%E9%98%85%E8%AF%BB/"/>
    <id>http://maxliu245.github.io/2020/06/27/10-VDN-Variational-Denoising-Network-%E9%98%85%E8%AF%BB/</id>
    <published>2020-06-27T06:58:57.000Z</published>
    <updated>2020-06-29T10:32:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>VDN阅读笔记</p></blockquote><a id="more"></a><h2 id="文献简介"><a href="#文献简介" class="headerlink" title="文献简介"></a>文献简介</h2><img src="/2020/06/27/10-VDN-Variational-Denoising-Network-阅读/10.png" title="VDN"><ul><li>这是一篇2019年<a href="http://papers.nips.cc/paper/8446-variational-denoising-network-toward-blind-noise-modeling-and-removal" target="_blank" rel="noopener">NIPS</a>的文章</li><li>截至目前，2020/06/29谷歌上的引用量11</li></ul><h2 id="概括VDN"><a href="#概括VDN" class="headerlink" title="概括VDN"></a>概括VDN</h2><p>VDN方法概括：是VI的方法，在贝叶斯框架下同时进行图像噪声估计和图像降噪。同时基于数据驱动，使用DNN近似图像后验，隐变量为假设的真实干净图像与噪声，进而生成带噪图像。这个图像后验是由DNN输出的参数显示表达的。</p><blockquote><p>当然你要问，那一般有了大量有监督的带噪图和干净图对，我们可以直接用DNN，或者GAN暴力做一个降噪器，VDN相比于此有何区别呢？除了贝叶斯框架外，最主要的是噪声假设更加general，其先验假设不再是简化的iid噪声，而是与图像上像素位置相关的非iid噪声，是spatial的。</p></blockquote><p>VDN的优点：</p><ul><li>使用数据驱动的DNN，其生成模式带来泛化性</li><li>是传统DNN方法的general版本，可退化回传统DNN方法（揭示了其机理与缺点）</li><li>两种隐变量，干净图和噪声的建模很general，可解释性好</li><li>spatial的非iid噪声先验假设</li><li>降噪和噪声估计同时进行，且均可显示表达出来</li></ul><h2 id="文献内容阅读（部分翻译）"><a href="#文献内容阅读（部分翻译）" class="headerlink" title="文献内容阅读（部分翻译）"></a>文献内容阅读（部分翻译）</h2><ul><li><p>摘要</p><p>介绍个背景<code>blind image denoising</code>，基本概念是收集图像的时候噪声很复杂以至于我们其实真的不清楚噪声具体结构，所以这种降噪称为blind</p></li><li><p>引言</p><ul><li>两种传统图像降噪方法：<ul><li>贝叶斯框架下的MAP方法，通过稀疏、低秩等先验，得到合理的后验，引出保真的损失和正则。但是缺点是噪声的先验往往与实际复杂且非iid的空间变化的噪声不符；且传统的MAP似乎只能作用于一张图像，对新图像要重新计算。</li><li>方法二是DNN端到端模型，利用大量收集的干净带噪图像对训练降噪器。优点是能利用大量图像信息，测试速度快；缺点是易对特定类型的噪声过拟合，且实际噪声复杂导致难以泛化√</li></ul></li><li>本文基于blind image denoising提出了新的VDN<ul><li>VDN概念：仍然在贝叶斯框架下，一般性的blind降噪方法。目的是同时估计噪声并降噪，把真实干净图像和噪声估计视为隐变量，从脏图生成干净的后验图像。使用DNN，后验成为参数的显式表示。</li><li>对一般方法的改进：<ul><li>噪声的估计是非iid的，更真实，与图像的空间位置相关；</li><li>生成式模型泛化更好</li><li>噪声的估计是生成式的，解释性</li><li>DNN框架，<strong>可以退化</strong>为一般的DNN降噪方法</li><li>一般方法过拟合的解释：本质上过分强调了拟合干净图的先验，忽略了噪声的空间非iid性，导致测试的时候易被不同类型的噪声影响</li></ul></li></ul></li></ul></li><li><p>相关工作</p><p>图像降噪的两种方法Model-driven MAP based Methods和Data-driven Deep Learning based Methods。</p></li><li><p>VDN理论推导过程</p><ul><li><p>贝叶斯模型建立</p><p>数据对集合$D={\mathbf{y}<em>j, \mathbf{x}_j}</em>{j=1}^n$，前者是带噪图，后者是对应的真实干净图，下标$j$是样本的index。贝叶斯框架下，隐变量是带噪图条件下的干净图和噪声。假定图像的维度为$d = weight \times height$，单个样本对$(\mathbf{y}, \mathbf{x})=\left(\left[y_1, \cdots, y_d\right]^T, \left[x_1, \cdots, x_d\right]^T\right)$，维度的index则统一用下标$i$表示。那么一张带噪图$\mathbf{y}$的一个分量</p><script type="math/tex; mode=display">y_i \sim \mathcal{N}(y_i | z_i, \sigma_i^2), i = 1, 2, \cdots, d \tag{1}</script><p>其中$z_i\in \mathbb{R}^d$，是隐变量，潜在的干净图，以数据对中的干净图为先验，</p><script type="math/tex; mode=display">z_i \sim \mathcal{N}(z_i | x_i, \epsilon_0^2), i = 1, 2, \cdots, d \tag{2}</script><p>为共轭的高斯先验。后面的方差项就是指图像上每个像素处的噪声是非iid的，不知道为什么取为对什么共轭的逆高斯分布IG，即</p><script type="math/tex; mode=display">\sigma_i^2 \sim IG\left(\sigma_i^2 | \dfrac{p^2}{2} - 1, \dfrac{p^2\xi_i}{2}\right), i = 1, 2, \cdots, d \tag{3}</script><p>其中$\mathbf{\xi} =\mathcal{G}\left(\left(\hat{\mathbf{y}} - \hat{\mathbf{x}} \right)^2; p\right)$，其中$\left(\hat{\mathbf{y}} - \hat{\mathbf{x}} \right)^2$称为variance map，中文可能叫图像的方差映射；$\mathcal{G}(\cdot, p)$指用$p\times p$的卷积核进行高斯滤波。具体有啥用我没看懂鸭…</p></li><li><p>后验的变分表示</p><p>首先明确后验分布是对于隐变量来说的，即$p(z, \sigma^2 | \mathbf{y})$。</p><p>那么有自然的后验近似分布$q(z, \sigma^2 | \mathbf{y})$</p><p>由平均场假设，近似分布可以分解</p><script type="math/tex; mode=display">q(z, \sigma^2 | \mathbf{y}) = q(z | \mathbf{y})q(\sigma^2 | \mathbf{y}) \tag{4}</script><p>由于前面提到“$z$服从共轭的高斯先验”和“方差项为共轭的逆高斯分布IG”，我们用下面两个式子表示$(3)$式中RHS的两项：</p><script type="math/tex; mode=display">\displaystyle q(z|\mathbf{y}) = \prod_{i}^{d} \mathcal{N}(z_i | \mu_i (\mathbf{y};W_D), m_i^2 (\mathbf{y}; W_D)) \tag{5}</script><script type="math/tex; mode=display">\displaystyle q(\sigma^2|\mathbf{y}) = \prod_{i}^{d} IG(\sigma_i^2 | \alpha_i (\mathbf{y};W_S), \beta_i^2 (\mathbf{y}; W_S)) \tag{6}</script><p>其中$\mu_i, m_i, \alpha_i, \beta_i$都是预测表示从脏图$\mathbf{y}$到对应参数的映射函数，用NN就可以做到。式$(4)$和式$(5)$分别对应降噪网络D-Net和参数网络Sigma-Net。</p></li><li><p>变分下界推导：</p><p>详细给了后验的分解形式后，来推下变分下界，下面是数据边际似然的对数：</p><script type="math/tex; mode=display">\log p(\mathbf{y}; z, \sigma^2) = \mathcal{L}(z,\sigma^2; \mathbf{y}) + D_{KL}(q(z, \sigma^2 | \mathbf{y}) || p(z, \sigma^2 | \mathbf{y})) \tag{7}</script><p>那么其中的$\mathcal{L}$就是变分下界：</p><script type="math/tex; mode=display">\mathcal{L}(z,\sigma^2; \mathbf{y}) = \mathbb{E}_{q(z, \sigma^2|y)} [\log p(y | z, \sigma^2)] - D_{KL}(q(z|y)||p(z)) - D_{KL}(q(\sigma^2|y)||p(\sigma^2)) \tag{8}</script><p>至于$(7)(8)$式怎么推导的，那就是一般变分下界的推导方式，对隐变量积分、似然项分解、差分出KL散度。式$(7)$中的每一项<strong>都有解析表示</strong>！下面是第一项</p><script type="math/tex; mode=display">\begin{align}\displaystyle \mathbb{E}_{q(z, \sigma^2|y)} [\log p(y | z, \sigma^2)] &\triangleq \int q(z, \sigma^2|y) \log p(y | z, \sigma^2) dzd\sigma^2 \tag{9}\\ &= \sum_{i}^n \int q(z_i, \sigma_i^2|y) \log p(y_i | z_i, \sigma_i^2) dz_id\sigma_i^2 \tag{10}\\ &= \sum_{i}^n \int q(z_i|y) q(\sigma_i^2|y) \left\{-\dfrac{1}{2}\log (2\pi) - \dfrac{1}{2}\log (\sigma_i^2) - \dfrac{(y_i - z_i)^2}{2\sigma_i^2}\right\} dz_id\sigma_i^2 \tag{11}\\ &= \sum_{i}^n \left\{ -\dfrac{1}{2}\log (2\pi) - \dfrac{1}{2} \int q(\sigma_i^2|y) \log (\sigma_i^2)d\sigma_i^2 \int q(z_i^2|y)dz_i - \dfrac{1}{2}\int q(z_i^2|y)(y_i - z_i)^2 dz_i \int q(\sigma_i^2|y)\dfrac{1}{\sigma_i^2}d\sigma_i^2\right\}  \tag{12}\\ &= \sum_{i}^n \left\{ -\dfrac{1}{2}\log (2\pi) - \dfrac{1}{2} \mathbb{E} \left[\log (\sigma_i^2)\right] - \dfrac{1}{2}\mathbb{E}\left[(y_i - z_i)^2\right]\mathbb{E}\left[\dfrac{1}{\sigma_i^2}\right]\right\}  \tag{13}\\ &= \sum_{i}^n \left\{ -\dfrac{1}{2}\log (2\pi) - \dfrac{1}{2} \left(\log \beta_i - \psi (\alpha_i)\right) - \dfrac{\alpha_i}{2\beta_i} \left[(y_i - \mu_i)^2 + m_i^2\right]\right\} \tag{14}\end{align}</script><p>后面两项我着实捉虾了，以及从$(13)$到$(14)$还是令我费解的。补充材料里都推导了，但可惜我这各种分布不太熟悉，这些期望具体怎么算的我还要多学…✊</p><p>然后就可以把变分下界当成目标函数，或者说loss来训练了。</p></li><li><p>网络，D-Net和Sigma-Net的训练似乎比较轻松，不需要重参数化什么的，直接BP开干就完事了。注意这里网络的结构怎么设计先不管了，有需要再进行阅读</p><blockquote><p>留一段原文结构机理在这里</p><p>During the BP training process, the gradient information from the likelihood term of<br>Eq. (10) is used for updating both the parameters of D-Net and S-Net simultaneously, implying that<br>the inference for the latent clean image z and sigma^2 is guided to be learned from each other.</p></blockquote><p>训练好两个网络之后，把测试脏图输入降噪的D-Net，得到的均值$\mu$就是降噪图；输入Sigma-Net，就得到该图噪声的分布参数。</p></li></ul></li><li><p>最后给出一个VDN退化到传统DNN的理解：</p><p>$(2)$式中如果方差项趋于0，则上面变分下界中的前面似然项期望退化为MSELoss，即传统的方法。传统的方法容易对特定的噪声过拟合就是因为只有这样的MSELoss项控制，缺少噪声随图像上位置变化的描述（往往非iid）</p></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] Yue, Zongsheng, et al. “Variational denoising network: Toward blind noise modeling and removal.” <em>Advances in neural information processing systems</em>. 2019.</p><p>[2] NIPS Proceedings 2019. Variational Denoising Network: Toward Blind Noise Modeling and Removal[EB/OL]. <a href="http://papers.nips.cc/paper/8446-variational-denoising-network-toward-blind-noise-modeling-and-removal" target="_blank" rel="noopener">http://papers.nips.cc/paper/8446-variational-denoising-network-toward-blind-noise-modeling-and-removal</a>, 2019.</p><p>详细推导细节参考[2]提供的补充材料。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;VDN阅读笔记&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="Denoising" scheme="http://maxliu245.github.io/tags/Denoising/"/>
    
  </entry>
  
  <entry>
    <title>[Hexo] 2 加密博客尝试</title>
    <link href="http://maxliu245.github.io/2020/06/27/Hexo-2-%E5%8A%A0%E5%AF%86%E5%8D%9A%E5%AE%A2%E5%B0%9D%E8%AF%95/"/>
    <id>http://maxliu245.github.io/2020/06/27/Hexo-2-%E5%8A%A0%E5%AF%86%E5%8D%9A%E5%AE%A2%E5%B0%9D%E8%AF%95/</id>
    <published>2020-06-27T05:23:42.000Z</published>
    <updated>2020-06-27T05:53:50.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Please input password to continue...</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="edb5e20c23b715968b8477bf0a982bcb9cc3393bedd954396a4ed8bfe91a9d49">7a0f6d1610bd85d142dfca6c113a63ebea2c61e81192ee6fa750c7e682a0a2911fce232fa4035290a3e2f7ea67a62944cc76e5960303612967119da3eae60fd5ed4266dfd401f8ddf7b61461f0237d50c9e17f91905b801f4969e4b4babbae66928617dc7c0292c6644ea66571079f17f014acc9d8607be61822b73e91bd2c1584b62a84b2f3f32996411b4426b42d10a44e1b48d2d7c7cae41481752e9cc36a05954d05080414658d40c3a2e63d30c7ee10cbb8e962e33f3b93c25f065a21c5d6d26520404b19d5651698a6d56012cc70ba407744879b4c9722b8ecc035fa17484a4909cdc321351647c84c4ca77ca12ab4112e7da5ceff16252d319e64dbd9741f0ac0fb44ac1fa0abbc95f158d8f7403f93971ba49b5678b1b02679d537f0c73e083ce27402de8c35d21073663b91db37bdc03d6d3ce4da4459a3cd72081907d6672b77c3f4e040d031ce6464e9d93fffe6cbc122fe021b56df85ba886b83a02a11cb99f7142d5c67f045186330b253c1eaae422acafba5adae35e6ba31182fdd54c35921945c1f0bf667cc720eab517297228cd091054ff054cfbc4939e9f5f3773cb55173b63425ea67889fe3723bba65297caee4859be60efd7c74969741fd8d96b29c105fe66273f5cb4f0dd8a3f8a501dedff57a38fec186ecb5bbebc4be93f9b3b07f5133f39941e26415e18e75bc30f32baf67151ae5e9852b37407d9522a59ea81b7d13efcb33156a30408d934a4d31e1018d46e8acf997db7ba43fe8b33367995b5a60227eda49ab28f3af35536a4cedd6db039d057c23e52fd34c3037fd905ba8990c954f3e948bcbc8f39e0e93569e9fcb37c796301ef2814836e73a007b7ba2a427f85c45d264e6279f890072d5e71c9cf4e5a861c3e1d288c9ac52a608475ab4a4cac2483f8b9afd14b839ee8161299490684d4bef654b6e65e44e6a004cc6fc089cc93ef914738e6f64838d3fd53381e39387821b485b9093c2a7873e83fab0f002db6df169cb49a923e5630a511bdc8ff2ee49795607459d11dcab9ad648d0ef9f9afd58d4d732e7c60b93cb00e6225b892a5b0e29291c6b09870456db1c0cc0da1c90acac2353ab86f8a4625316e02a715b387a3a6097ba7894d094ff09ea2a77a9c720b6b83e2b949e0510aa9dcf21a0ff2a6bc26784c22e6ad575ae01e0003ce9bc5b8fb2456b8ef81d445ce017201f4ff974b1db427cc985c78ccce194b38c6afb165daa8b3b8ed3a2bbd9fcfce18f82a48c92259a1528d72f4d0b531cee2fadcb17b087f73987d675fa6d299874781a9aa8ffc349ff0600266ec2c589d72c6fc4d7b12092e9419c59993ae0aa4fd8e0ec15033edcd0b732bf543e7af3cb1ef2a1dc791938b5fef4bc5f72425f20cb5bc9241ecac4bf55dcd48ab84ba905223a96aea31a2c870a8b54f8e1989fc2915afdec0f15ae271a54284f0b6ae078f5308f8df1dd68644e596c181e294fc53cbc5b53be3ba0c12088ba3385df2068554e3022248a3c94412d56615a093d8642d8fdcd82e5ecb82fc348938391b2bfc6b7948099385e200a84dc877889c1a6cbb4045e9ef893bc3851b084f5428b3a2bd34748f91c296cf3e826e956aa6e9caf06472079e21a6d3c7be329629251ff7639e1ba9d84bb94c25fe3b4a85c8dc2cd8faa0874f23f7d9c7ccf064b95caf4e281312e139f7f53982a210c86f53e2db0891c3ea919f55e4f072d467b2d0b61810938fa6b123268b5b7cf2bf169863aa7d7ef96b2319f1a9418209bbab315eea21c50cbe82348889e2ee84987a9f14baa4038ffcdc7f5854e97e33426f9e4460b3fd2fecd8ba5308082ea15c1769ed6d46a6e415be1e613b8016c4b872772836d884394b45e16bb5ae418ae70d171fd5db3b6ce64b2e99df7e372064f9bdcc40fcf6e5eed18a7df7c9e31fdf664e0f88ea06bdbb7d4e6abbe692ce7e1ab9c184bad79f091253859f442989f9e9dfb46bee567f4481b054914542e1f647da581e92f7fe53dc9da2ff9242595db52c15dc96dcba7ef3793c60a7f5ed22bf1759b2584597f6bc9947c0a36b272577ec8bbd18d4691d1c7eb0c405529d4a94ab377e3f6ba3722e1c0ad966c57d76662f8cdb51f664c932ba86c6f03381beb303a39872e76bc65e33ab3978b7eec0cbf26517242af80caeb37ce959d0078455025ae34391848c206430ca68a6a68d95547d1d75110fdf761ee6300ba5a6559d771cc25dc1b6f71c60ac2ddc37d71667113a3fa66d64e31715dc69560a5572c0784ac281db7a4bc594f424d77a26031c022a71ae438160dc2f4f448d275abfd80173c388cc82e2b4705d12801e4de18a3f019341ffbd4a12382ad326dc86016a9eb478bab68019968a784c593632636e81e74df4f850a46f8330d4aa2ee084e3ca2f12db5ed3b27255be18cadb815406bd89849f9a93b16565fe17b6ac2867d9400efe38e65837a0828adc84f4318cf6429408279a0ce7d48aefe978f3dbd9e31affc8d7f9ed78ebfae4227fd519cad22f93bf619da6ed857f08aa5af866bf735e164435207c4f139906dbb9f28837d2f8b8e7bfc5e73885171f74e0f5ee219d71038606487a06de71a56009d63b22563f07b4307528a84d206d3638ebfad6ee4c56afedc808a7a87549a6979315ecb3cbf0d6593bb8f38767198815a5107ee5b45191ff7d2d8872356e47f1744b0eda30b7416dd3cff4b186d29befa7a9552116759ad6afbacf0d06bd1ab59b9d4a4fdde84a38382b5108c491e34bd30a3917db95188e010bf9e87a46044745b459685512138f51614b7f1dc53c0bdc7b6caf902930029f39aebf3c7873840b410aaa361480e749e043728d779ce547fde42c576c7df2664a2d0dfce4016450a37289445fb94646cf94eaf5a220520892433712066307bc4347a299841b2a0c0bc7d229c3d7d367668841fca6bf092cc78b03f646a660ea57f921b3d843fcb901d913199667b4cd9e7b002a59573c86e489402273ff11a5a64a66aec354640ef705144e76f8a0b4a740eda50af4e21d185abe7e7ee593624c79af49611a72c7b750803e53ca5ab8b229720e4b22c2e1844c9b759e2ce260fc25942175e648c18025d9f662476a82a167f4a873d1ff848c45533b7396300787db61f91ae3ce6f2907d922204b57c958e7d523ee050c21ae43f4d3c08fcccdde37ce7190afa46ff11457ae21fc0d2e5a22bfddec0044722278d54f2bbd6e12c32076e6996b67fe68debc9cd3836f957e7b0a45def7a5291eb04672de01b4bc26ff6920b4cb0b50b1af6de762efd971e84233a9be7dba9e0baca3ff881fbc0217643db5c0e2cf5b3a7880c1f6932ac3b9ad1528d6d2e18408c51bc734ba693f0e297fc29863d3c025770abb74de8d0ac90c3e0847e79e586aa0d35375a803e018ff51697a579ca04e0ecbcb62636018792a81b42cb8558045ba3245d4622fa9ea26ede75066fc6fd0fb155fac03b667db13bda6e9fe840a388afa931ffda0082bd16ba3b1c646cd2db9f310d9d10925e2d3e9f6d2960224c9a205a67a93380373967dfbad512440d714c652397ce24deca947e43fb7513d8b0c53e91189acb4787a02739b1a7ce3a05c7140ed6700500e26c2ae0618721b08227d951399c1d370ef02b97e2861ef376f255b304c3cc3898bad04fe357bcc8a2dab84bb15684c955948c789845486f9b10d8a2ea20507ac28a51544e5898acd875a9e41b83c754b43bf83d2bece114b037b21cf19b4140345843fafd6bf65229e0a0ab6b0be651ef2e5ec00c96420c004d7b151506e7f0d782cb1ed28de8a86a27ffc1b332e17cdfbc05eafeca48652557d79f4f91c0c7263fb3de392e56f84527c3d86fcfc914bc4109a77863be261e0de8d1e03877e6596adba684da65dd23afc0b762b52a2936ba663c4c166f99d2bb7d91192df14feac184df441e1b98effb02ff9fc16e2e22ae7c2a30edfe99451ad9ba1d793a3a71e2fda730c145912f220654551b71cbd029daf40346c04e5718fe9fb973f993e50ee6f951ad3eef2908e7d8628ec1cbc4b32ef622c1b214d0b76f5580cb3b4f5a1647a1804d4717e7005d05085d8f2b2d871d70661ab945409239f9cb95cb214f85b293e42937ae53911b15c2ddcbbc5faa6e2699a04aa978996791c86e826e8b85a632ab81b2a8bf4d912ccbfb3266d3afae2a7b628dba3345792fd22f7b103651d42faf2bbc92781760a17004da33145d38a20562fc8a7fd0785df3be67031d121e44a18f9fb97b2294d52e00c2b21188287ee842fc745f294d1fdecc289358e704e37a4e76f09ed0043276dd070da7f0ad3d50c1cb7dcf3f7a07c0208eb57fe47acdb5f8d5bc980c02ae8a0f41840afcbecb56d356055443ed721175be849de7332762d17356b4f3a1692fdf60754c225f08af205907ce5e51ca37ff5ae32f9cc9d1e107aaf6caaa1656bf82d281e87576afd97250f3f3ac80b4c744108057e10b4ef7af57fa0270548393742b478ef43def59cc2a337c4db16657be1a9098814c35af92ee0fd97e146ee5d577156772125691430c48138986847f13ff2e8184af05d719552769e6b3a75ed9aad3a2ef78178c4312826cad51952080bf54365cd9379a34bff4c317b1595b56cdf127121f7994b61e2805cb0b494d7ba83e0de06699618e3a79647cf8bd3450e847cd56e24ae20e1cc30834b3245eee0178eca33cbf7e54803f4923e8c26ec89e4a6006a0a52e4fdd768334d046155ba8847bc46b94425d3b377865935eb327c750c2e519d235122fb54c04538d895249089d0eb59f845204be04ac1aed814de669689847ad53a3ea146d5f26d476198f6e21f3c3350e9990b86e983a87283efc10f73c8ff9e4b655a0b194f4aaf1d300960c1b3b8ba5e2adf5f230d1686c27e08d9c8e323f31dbd77f684fa2e6b2e29d1849cfeb494248253c21c2152e74adf893be564b60db7693c6818d240f2704ce83a996e343942d6d74523c31f587ab4766794dfea9df9ca2f5899e4fc34f4982d0b559cafebdf214151a4986915bbea1ebd05d6a841b8e3a254f983fb6e40e1ad9a9f4998ae76c374d33a09f10e075d8a32c9c9e97131393e0cfdc4ef157ecdcb11ca6bba6c6491a8a394afa6e91ded20ebf9e06f7ae8fc46e0c6e4d9d8ac1af9ff46a7ef2687ddd11e4c3f978974a9ee87857195e</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Need password to continue... Here it&#39;s Hahaha
    
    </summary>
    
    
    
      <category term="Hexo" scheme="http://maxliu245.github.io/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>多元高斯分布之间的KL散度</title>
    <link href="http://maxliu245.github.io/2020/06/14/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E4%B9%8B%E9%97%B4%E7%9A%84KL%E6%95%A3%E5%BA%A6/"/>
    <id>http://maxliu245.github.io/2020/06/14/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E4%B9%8B%E9%97%B4%E7%9A%84KL%E6%95%A3%E5%BA%A6/</id>
    <published>2020-06-14T11:46:22.000Z</published>
    <updated>2020-06-14T14:01:48.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>推导一下两个多元高斯分布之间的KL散度公式</p></blockquote><a id="more"></a><h3 id="完整推导过程"><a href="#完整推导过程" class="headerlink" title="完整推导过程"></a>完整推导过程</h3><p>哎，其实我是傻乎乎地想推导两个多元高斯分布之间的$\beta$和$\gamma$散度，后来发现不可行。在这个过程中顺手推导了一般的KL散度，谨记过程如下。</p><p>假定两个多元高斯分布分别为$\mathcal{N}(\mu_1, \Sigma_1)$和$\mathcal{N}(\mu_2, \Sigma_2)$，其概率分布分别表示为$p(x)$和$q(x)$。其中$\mu_1$和$\mu_2$为长度$n$的向量，$\Sigma_1$和$\Sigma_2$为维度$n\times n$的协方差阵。那么$p(x)$和$q(x)$之间的差异可以用KL散度表示，定义为：</p><script type="math/tex; mode=display">\displaystyle D_{KL}(p(x)||q(x)) = \int p(x)\log \dfrac{p(x)}{q(x)}dx.\tag{1}</script><p>$(1)$式的完整计算过程如下：</p><script type="math/tex; mode=display">\begin{align} \displaystyle D_{KL}(p(x)||q(x)) &= \int p(x)\log \dfrac{p(x)}{q(x)}dx\\ &= \int p(x)\log \dfrac{\dfrac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_1|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1)}}{\dfrac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_2|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2)}}dx \tag{2}\\ &= \int p(x)\log \dfrac{|\Sigma_2|^{\frac{1}{2}}}{|\Sigma_1|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) + \frac{1}{2}(x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2)}dx \tag{3} \\&= \dfrac{1}{2}\int p(x)\log \dfrac{|\Sigma_2|}{|\Sigma_1|}dx + \dfrac{1}{2}\int p(x)\left[ -(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) + (x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2)\right]dx \tag{4} \\ &= \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} + \dfrac{1}{2}\int p(x)\left[ -tr\left(\Sigma_1^{-1}(x-\mu_1)(x-\mu_1)^T\right) + tr\left(\Sigma_2^{-1}(x-\mu_2)(x-\mu_2)^T\right)\right]dx \tag{5} \\ &= \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} - \dfrac{1}{2}tr\left[\int p(x)\left[\Sigma_1^{-1}(x-\mu_1)(x-\mu_1)^T\right]dx\right] + \dfrac{1}{2}tr\left[\int p(x)\left[\Sigma_2^{-1}(x-\mu_2)(x-\mu_2)^T\right]dx\right] \tag{6}\\ &= \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} - \dfrac{1}{2}tr\left(\Sigma_1^{-1}\int p(x)(x-\mu_1)(x-\mu_1)^T dx\right) + \dfrac{1}{2}tr\left(\Sigma_2^{-1}\int p(x)(x-\mu_2)(x-\mu_2)^T dx\right) \tag{7} \\ &= \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} - \dfrac{1}{2}tr\left(\Sigma_1^{-1}\mathbb{E}_p\left(xx^T -x\mu_1^T - \mu_1 x^T + \mu_1\mu_1^T\right)\right) + \dfrac{1}{2}tr\left(\Sigma_2^{-1}\mathbb{E}_p\left(xx^T -x\mu_2^T - \mu_2 x^T + \mu_2\mu_2^T\right)\right) \tag{8} \\ &=  \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} - \dfrac{1}{2}tr\left(\Sigma_1^{-1}\Sigma_1\right) + \dfrac{1}{2}tr\left(\Sigma_2^{-1}\left(\Sigma_1 - \mu_1\mu_2^T - \mu_2 \mu_1^T + \mu_2\mu_2^T\right)\right) \tag{9} \end{align}</script><p>Emmm，式子太长，编辑器渲染都卡爆了，所以接着上面再推导：</p><script type="math/tex; mode=display">\begin{align} \displaystyle D_{KL}(p(x)||q(x)) &= \dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|} - \dfrac{1}{2}tr\left(I\right) + \dfrac{1}{2}tr\left(\Sigma_2^{-1}\Sigma_1\right) + \dfrac{1}{2}tr\left[\Sigma_2^{-1}\left(\mu_1\mu_1^T -\mu_1\mu_2^T - \mu_2 \mu_1^T + \mu_2\mu_2^T\right)\right] \tag{10} \\ &= \dfrac{1}{2}\left[ \log \dfrac{|\Sigma_2|}{|\Sigma_1|} - n + tr\left(\Sigma_2^{-1}\Sigma_1\right) + tr\left[\Sigma_2^{-1}\left(\mu_1\mu_1^T -\mu_1\mu_2^T - \mu_2 \mu_1^T + \mu_2\mu_2^T\right)\right]\right] \tag{11}\\ &= \dfrac{1}{2}\left[ \log \dfrac{|\Sigma_2|}{|\Sigma_1|} - n + tr\left(\Sigma_2^{-1}\Sigma_1\right) + tr\left(\mu_1^T\Sigma_2^{-1}\mu_1^T\right) - tr\left(\mu_1^T\Sigma_2^{-1}\mu_2^T\right) - tr\left(\mu_2^T\Sigma_2^{-1}\mu_1^T\right)+ tr\left(\mu_2^T\Sigma_2^{-1}\mu_2^T\right)\right] \tag{12} \\ &= \dfrac{1}{2}\left[ \log \dfrac{|\Sigma_2|}{|\Sigma_1|} - n + tr\left(\Sigma_2^{-1}\Sigma_1\right) + tr\left(\mu_1^T\Sigma_2^{-1}\mu_1^T - 2\mu_1^T\Sigma_2^{-1}\mu_2^T + \mu_2^T\Sigma_2^{-1}\mu_2^T\right)\right] \tag{13} \\ &= \dfrac{1}{2}\left[ \log \dfrac{|\Sigma_2|}{|\Sigma_1|} - n + tr\left(\Sigma_2^{-1}\Sigma_1\right) + tr\left[\left(\mu_2-\mu_1\right)^T\Sigma_2^{-1}\left(\mu_2-\mu_1\right)\right]\right] \tag{14} \\ &= \dfrac{1}{2}\left[ \log \dfrac{|\Sigma_2|}{|\Sigma_1|} - n + tr\left(\Sigma_2^{-1}\Sigma_1\right) + \left(\mu_2-\mu_1\right)^T\Sigma_2^{-1}\left(\mu_2-\mu_1\right)\right]. \tag{15}\end{align}</script><h3 id="部分推导细节"><a href="#部分推导细节" class="headerlink" title="部分推导细节"></a>部分推导细节</h3><p>在上述推导中有几步需要额外注意，我自己已经在纸上推导好了，这里简要提示一下（具体参见参考文献）：</p><ul><li>$(4)$式到$(5)$式：利用标量$\Leftrightarrow$标量的迹和迹的交换性；</li><li>$(5)$式到$(6)$式：举个例子展开写就明白了；</li><li>$(8)$式到$(9)$式：其实是$(7)$式到$(9)$式，注意对谁求期望和协方差的定义；</li><li>$(14)$式到$(15)$式：标量$\Leftrightarrow$标量的迹。</li></ul><h3 id="PyTorch官方函数的写法"><a href="#PyTorch官方函数的写法" class="headerlink" title="PyTorch官方函数的写法"></a><code>PyTorch</code>官方函数的写法</h3><p>最后$(15)$式中的结果与<code>PyTorch</code>中多元高斯分布KL散度的表达是一致的，<code>half_term1</code>就是$\dfrac{1}{2}\log \dfrac{|\Sigma_2|}{|\Sigma_1|}$，<code>term2</code>就是$tr\left(\Sigma_2^{-1}\Sigma_1\right)$，<code>term3</code>就是$\left(\mu_2-\mu_1\right)^T\Sigma_2^{-1}\left(\mu_2-\mu_1\right)$：</p><img src="/2020/06/14/多元高斯分布之间的KL散度/KL.png" title="PyTorch中多元高斯分布KL散度函数"><h3 id="beta-散度的想法"><a href="#beta-散度的想法" class="headerlink" title="$\beta$散度的想法"></a>$\beta$散度的想法</h3><p>其实本来是想计算$\beta$和$\gamma$散度的类似表达的…只是我真的算不出来…估计只能近似了。列个$\beta$散度的定义，有待研究。</p><script type="math/tex; mode=display">\displaystyle D_{\beta}(p(x)||q(x)) = \dfrac{1}{\beta}\int p(x)^{1+\beta}dx - \dfrac{\beta + 1}{\beta}\int p(x)q(x)^{beta}dx + \int q(x)^{1+\beta}dx. \tag{16}</script><h3 id="参考链接列表"><a href="#参考链接列表" class="headerlink" title="参考链接列表"></a>参考链接列表</h3><ul><li><a href="https://github.com/pytorch/pytorch/blob/master/torch/distributions/kl.py" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/blob/master/torch/distributions/kl.py</a></li><li><a href="https://blog.csdn.net/sinat_33598258/article/details/103866549" target="_blank" rel="noopener">https://blog.csdn.net/sinat_33598258/article/details/103866549</a></li><li><a href="https://arxiv.org/pdf/1905.09961.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.09961.pdf</a></li><li><a href="https://zhuanlan.zhihu.com/p/143124676" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/143124676</a></li><li><a href="https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;推导一下两个多元高斯分布之间的KL散度公式&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="KL Divergence" scheme="http://maxliu245.github.io/tags/KL-Divergence/"/>
    
  </entry>
  
  <entry>
    <title>NBA2K13鹰重整理</title>
    <link href="http://maxliu245.github.io/2020/06/13/NBA2K13%E9%B9%B0%E9%87%8D%E6%95%B4%E7%90%86/"/>
    <id>http://maxliu245.github.io/2020/06/13/NBA2K13%E9%B9%B0%E9%87%8D%E6%95%B4%E7%90%86/</id>
    <published>2020-06-13T14:10:01.000Z</published>
    <updated>2020-06-14T14:03:22.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><a id="more"></a><h2 id="鹰🦅修改"><a href="#鹰🦅修改" class="headerlink" title="鹰🦅修改"></a>鹰🦅修改</h2><h3 id="Trae-Young"><a href="#Trae-Young" class="headerlink" title="Trae Young"></a>Trae Young</h3><p>用雄鹿的Beno Udo…。名字54726165 596f756e67。号码11=22=16，肤色03，位置PG/SG=00/01，年龄21岁7e 7c，身高6‘2’=188，体重180，原头99 04=1177，时间35=140=8c。</p><div class="table-container"><table><thead><tr><th>/</th><th>KM最新</th><th>90</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td>1799b0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>08e80</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close   92 5c</td><td>2 med  91 5b</td><td>3 3pt 90 5a</td><td>4 ft 85 55</td><td>5 layup  87 57</td><td>6 dunk  58 3a</td><td>28 stdnk  50 1e</td><td><strong>23  sit</strong>  55 37</td><td><strong>22  sod</strong>  69 45</td><td>25 hus  72 48</td></tr><tr><td><strong>7 hndl</strong>  92 5c</td><td><strong>8  pass</strong>  92 5c</td><td>39</td><td>32</td><td>9 opost  58 3a</td><td>10 dpost  60 3c</td><td>11 block   60 3c</td><td>26 hnd  64 40</td><td>12 steal  50 32</td><td>15 speed  86 56</td><td>16 stam  91 5b</td><td>19</td><td>21 vert 90 5a</td><td>13 oreb  50 21</td><td>14 dreb  50 24</td><td>17 dur  70 46</td></tr><tr><td><strong>18 dawr</strong>  72 48</td><td><strong>19  oawr</strong>  98 62</td><td>32</td><td>54</td><td>27 def 88 58</td><td>24 qui 88 58</td><td>潜力 89 59</td><td>20 str  60 3c</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h3 id="Clint-Capela"><a href="#Clint-Capela" class="headerlink" title="Clint Capela"></a>Clint Capela</h3><p>用Alexis Ajinca。身高6‘11’=208，体重240，年龄26=2d 7c，号码17=34=22，肤色01，位置C/05=04/05，原头25 06=1573，名字436c696e74 436170656c61，时间33=132=84。</p><div class="table-container"><table><thead><tr><th>/</th><th>KM最新</th><th>86</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td>181960</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>71700</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close   73 49</td><td>2 med  60 3c</td><td>3 3pt 50 32</td><td>4 ft 54 36</td><td>5 layup  76 4c</td><td>6 dunk  82 52</td><td>28 stdnk  96 60</td><td><strong>23  sit</strong>  88 58</td><td><strong>22  sod</strong>  70 46</td><td>25 hus  82 52</td></tr><tr><td><strong>7 hndl</strong>  70 46</td><td><strong>8  pass</strong>  64 40</td><td>32</td><td>32</td><td>9 opost  79 4f</td><td>10 dpost  84 54</td><td>11 block   81 51</td><td>26 hnd  67 43</td><td>12 steal  52 34</td><td>15 speed  68 44</td><td>16 stam  83 53</td><td>19</td><td>21 vert 71 47</td><td>13 oreb  86 56</td><td>14 dreb  94 5e</td><td>17 dur  80 50</td></tr><tr><td><strong>18 dawr</strong>  84 54</td><td><strong>19  oawr</strong>  79 4f</td><td>19</td><td>4c</td><td>27 def 69 45</td><td>24 qui 59 3b</td><td>潜力 55</td><td>20 str  80 50</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h3 id="Qi-Zhou🤭"><a href="#Qi-Zhou🤭" class="headerlink" title="Qi Zhou🤭"></a>Qi Zhou🤭</h3><p>用凯尔特人Fab Melo改。名字5169 5a686f75，身高7‘1’=217，体重210，年龄24=43 7c，号码9=18=12，肤色04，位置C/PF=04/03，原头a1 09=2465，时间15=60=3c。</p><div class="table-container"><table><thead><tr><th>/</th><th>KM之前</th><th>69</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td>179dd0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>0d980</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close   87 57</td><td>2 med  87 57</td><td>3 3pt 82 52</td><td>4 ft 88 58</td><td>5 layup  75 4b</td><td>6 dunk  75 4b</td><td>28 stdnk  99 63</td><td><strong>23  sit</strong>  67 43</td><td><strong>22  sod</strong>  67 43</td><td>25 hus  89 59</td></tr><tr><td><strong>7 hndl</strong>  61 3d</td><td><strong>8  pass</strong>  50 1d</td><td>32</td><td>32</td><td>9 opost  64 40</td><td>10 dpost  61 3d</td><td>11 block   90 5a</td><td>26 hnd  82 52</td><td>12 steal  68 44</td><td>15 speed  79 4f</td><td>16 stam  90 5a</td><td>28</td><td>21 vert 80 50</td><td>13 oreb  84 54</td><td>14 dreb  87 57</td><td>17 dur  80 50</td></tr><tr><td><strong>18 dawr</strong>  64 40</td><td><strong>19  oawr</strong>  64 40</td><td>23</td><td>50</td><td>27 def 77 4d</td><td>24 qui 77 4d</td><td>潜力 83 53</td><td>20 str  72 48</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h3 id="Kevin-Huerter"><a href="#Kevin-Huerter" class="headerlink" title="Kevin Huerter"></a>Kevin Huerter</h3><p>位置SG/SF=01/02，</p><div class="table-container"><table><thead><tr><th>/</th><th>KM</th><th>74</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close   87 57</td><td>2 med  87 57</td><td>3 3pt 82 52</td><td>4 ft 88 58</td><td>5 layup  75 4b</td><td>6 dunk  75 4b</td><td>28 stdnk  99 63</td><td><strong>23  sit</strong>  67 43</td><td><strong>22  sod</strong>  67 43</td><td>25 hus  89 59</td></tr><tr><td><strong>7 hndl</strong>  61 3d</td><td><strong>8  pass</strong>  50 1d</td><td>32</td><td>32</td><td>9 opost  64 40</td><td>10 dpost  61 3d</td><td>11 block   90 5a</td><td>26 hnd  82 52</td><td>12 steal  68 44</td><td>15 speed  79 4f</td><td>16 stam  90 5a</td><td>28</td><td>21 vert 80 50</td><td>13 oreb  84 54</td><td>14 dreb  87 57</td><td>17 dur  80 50</td></tr><tr><td><strong>18 dawr</strong>  64 40</td><td><strong>19  oawr</strong>  64 40</td><td>23</td><td>50</td><td>27 def 77 4d</td><td>24 qui 77 4d</td><td>潜力 83 53</td><td>20 str  72 48</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
      <category term="NBA2K13" scheme="http://maxliu245.github.io/tags/NBA2K13/"/>
    
      <category term="DIY修改" scheme="http://maxliu245.github.io/tags/DIY%E4%BF%AE%E6%94%B9/"/>
    
  </entry>
  
  <entry>
    <title>NBA2K13太阳重整理</title>
    <link href="http://maxliu245.github.io/2020/06/12/hexo/"/>
    <id>http://maxliu245.github.io/2020/06/12/hexo/</id>
    <published>2020-06-12T15:58:04.000Z</published>
    <updated>2020-06-12T16:56:32.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="太阳修改"><a href="#太阳修改" class="headerlink" title="太阳修改"></a>太阳修改</h2><h3 id="Devin-Booker"><a href="#Devin-Booker" class="headerlink" title="Devin Booker"></a>Devin Booker</h3><p>网队Keith Bogans。446576696e 426f6f6b6572。</p><p>号码01；位置SG/SF=01/02；身高6‘6’=196，体重210；潜力89=59；时间36=144=90；白人肤色05；面补19 04=25+1024=1049。KM3542</p><div class="table-container"><table><thead><tr><th>/</th><th>KM最新</th><th>86</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td>17aa30</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>1f860</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close  90 5a</td><td>2 med  85 55</td><td>3 3pt  82 52</td><td>4 ft 91 5b</td><td>5 layup  87 57</td><td>6 dunk  60 3c</td><td>28 stdnk  69 45</td><td><strong>23  sit</strong> 66 42</td><td><strong>22  sod</strong>  67 43</td><td>25 hus  68 44</td></tr><tr><td><strong>7 hndl</strong>  86 56</td><td><strong>8  pass</strong>  79 4f</td><td>40</td><td>40</td><td>9 opost  60 3c</td><td>10 dpost  60 3c</td><td>11 block   50 1c</td><td>26 hnd  76 4c</td><td>12 steal  50 32</td><td>15 speed  78 4e</td><td>16 stam  79 4f</td><td>19</td><td>21 vert 54 36</td><td>13 oreb  78 4e</td><td>14 dreb  63 3f</td><td>17 dur  86 56</td></tr><tr><td><strong>18 dawr</strong>  60 3c</td><td><strong>19  oawr</strong>  98 62</td><td>3c</td><td>41</td><td>27 def 58 3a</td><td>24 qui 75 4b</td><td>潜力 59</td><td>20 str 50 32</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td><td>/</td></tr></tbody></table></div><h3 id="Deandre-Ayton"><a href="#Deandre-Ayton" class="headerlink" title="Deandre Ayton"></a>Deandre Ayton</h3><p>76人Spencer Hawes。4465616e647265 4179746f6e。</p><p>号码22=44=2c；位置C/PF=04/03；身高7‘1’=216.5，体重250；潜力92=5c；时间33=132=84；肤色01；面补8b 05=139+1280=1419；</p><div class="table-container"><table><thead><tr><th>/</th><th>KM最新</th><th>85</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td>179660</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>05640</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close  91 5b</td><td>2 med  71 47</td><td>3 3pt  60 3c</td><td>4 ft 75 4b</td><td>5 layup  79 4f</td><td>6 dunk  75 4b</td><td>28 stdnk  90 5a</td><td><strong>23  sit</strong> 65 41</td><td><strong>22  sod</strong>  50 2d</td><td>25 hus  82 52</td></tr><tr><td><strong>7 hndl</strong>  50 25</td><td><strong>8  pass</strong>  50 2a</td><td>40</td><td>40</td><td>9 opost  82 52</td><td>10 dpost  85 55</td><td>11 block   81 51</td><td>26 hnd  66 42</td><td>12 steal  50 2a</td><td>15 speed  77 4d</td><td>16 stam  78 4e</td><td>3c</td><td>21 vert 52 34</td><td>13 oreb  85 55</td><td>14 dreb  89 59</td><td>17 dur  70 46</td></tr><tr><td><strong>18 dawr</strong>  65 41</td><td><strong>19  oawr</strong>  90 5a</td><td>41</td><td>3b</td><td>27 def 62 3e</td><td>24 qui 55 37</td><td>潜力</td><td>20 str 75 4b</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td><td>/</td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
      <category term="NBA2K13" scheme="http://maxliu245.github.io/tags/NBA2K13/"/>
    
      <category term="DIY" scheme="http://maxliu245.github.io/tags/DIY/"/>
    
  </entry>
  
  <entry>
    <title>NBA2K13森林狼重整理</title>
    <link href="http://maxliu245.github.io/2020/06/12/NBA2K13%E6%A3%AE%E6%9E%97%E7%8B%BC%E9%87%8D%E6%95%B4%E7%90%86/"/>
    <id>http://maxliu245.github.io/2020/06/12/NBA2K13%E6%A3%AE%E6%9E%97%E7%8B%BC%E9%87%8D%E6%95%B4%E7%90%86/</id>
    <published>2020-06-12T12:04:16.000Z</published>
    <updated>2020-09-15T16:33:11.942Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p> 此文仅整理记录安卓原版2K13非root情况下可用于修改DIY的球员名单😜。</p><p> 之前的大名单整理内容实在是太多了，整理的时候很容易弄混，因此开始分解名单更新。</p><p> 如有不当的地方，请联系我删除。</p></blockquote><a id="more"></a><h2 id="森林狼修改"><a href="#森林狼修改" class="headerlink" title="森林狼修改"></a>森林狼修改</h2><p>全队的时间按照stat-nba2020常规赛修改</p><h3 id="Karl-Anthony-Towns"><a href="#Karl-Anthony-Towns" class="headerlink" title="Karl-Anthony Towns"></a>Karl-Anthony Towns</h3><div class="table-container"><table><thead><tr><th>/</th><th>KM405</th><th>90</th><th>88</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td>179620</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>05280</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close   91 5b</td><td>2 med  87 57</td><td>3 3pt 79 4f</td><td>4 ft 83 53</td><td>5 layup  87 57</td><td>6 dunk  85 55</td><td>28 stdnk  99 63</td><td><strong>23  sit</strong>  55 37</td><td><strong>22  sod</strong>  68 44</td><td>25 hus  60 3c</td></tr><tr><td><strong>7 hndl</strong>  71 47</td><td><strong>8  pass</strong>  62 3e</td><td>32</td><td>40</td><td>9 opost  86 56</td><td>10 dpost  78 4e</td><td>11 block   72 48</td><td>26 hnd  72 48</td><td>12 steal  66 42</td><td>15 speed  80 50</td><td>16 stam  90 5a</td><td>3c</td><td>21 vert 72 48</td><td>13 oreb  76 4c</td><td>14 dreb  89 59</td><td>17 dur  78 4e</td></tr><tr><td><strong>18 dawr</strong>  81 51</td><td><strong>19  oawr</strong>  82 52</td><td>50</td><td>5b</td><td>27 def 73 49</td><td>24 qui 90 5a</td><td>潜力 93 5d</td><td>20 str  79 4f</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td></tr></tbody></table></div><p>目前考虑用原76人的Jason改。</p><p>名字4b61726c2d416e74686f6e79 546f776e73；位置C，04/05；身高6‘11’，211；体重248；32号-&gt;64-&gt;40；上场时间34min-&gt;136-&gt;88；面补用KM的，原面补0805原来整理好了；年龄24=4c 7c；原肤色00，太黑了，改成03。</p><p>注：身高6‘6=198.12，年龄31=da 7b</p><h3 id="D’Angelo-Russell"><a href="#D’Angelo-Russell" class="headerlink" title="D’Angelo Russell"></a>D’Angelo Russell</h3><div class="table-container"><table><thead><tr><th>/</th><th>KM405</th><th>84</th><th>83</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td>17a980</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>1e3c0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close  81 51</td><td>2 med  93 5d</td><td>3 3pt  82 52</td><td>4 ft 77 4d</td><td>5 layup  89 59</td><td>6 dunk  60 3c</td><td>28 stdnk  50 23</td><td><strong>23  sit</strong> 58 3a</td><td><strong>22  sod</strong>  55 37</td><td>25 hus  81 51</td></tr><tr><td><strong>7 hndl</strong>  86 56</td><td><strong>8  pass</strong>  82 52</td><td>49</td><td>32</td><td>9 opost  68 44</td><td>10 dpost  69 45</td><td>11 block   54 36</td><td>26 hnd  74 4a</td><td>12 steal  50 32</td><td>15 speed  83 53</td><td>16 stam  88 58</td><td>19</td><td>21 vert 70 46</td><td>13 oreb  70 46</td><td>14 dreb  67 43</td><td>17 dur  75 4b</td></tr><tr><td><strong>18 dawr</strong>  74 4a</td><td><strong>19  oawr</strong>  84 54</td><td>55</td><td>5a</td><td>27 def 68 44</td><td>24 qui 60 3c</td><td>潜力 57</td><td>20 str 53 35</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td><td>/</td></tr></tbody></table></div><p>换人改，不用比卢普斯，有两个候选，小牛的和太阳的，选了小牛的Rodrigue 526f647269677565 Beaubois。</p><p>名字4427416e67656c6f 52757373656c6c；位置PG/SG，00/01；上场时间32.6min-&gt;130-&gt;82；原肤色02，好像不行改成04；原面补65 06=1637，KM的用了即可；身高6‘4’=1.93？不行，还是6‘3’，改成194试试，好了；体重195。号码0号。</p><p>投篮动作改成鹰Harris（数据11b20）的。动作的位置在一堆ff上面的08和0c位置。Harris的2c 57。比卢普斯的是0a 17.</p><p>变向动作参考利拉德的试一试。不对啊，他的动作也是00 00 00 08 07 00 00 00 07 00 00 00，把08 07删了试试</p><p>ps：年龄36 = 8c 7b；身高6’3‘=190.5。原身高6‘1’=187.95，24=4c 7c</p><h3 id="Malik-Beasley"><a href="#Malik-Beasley" class="headerlink" title="Malik Beasley"></a>Malik Beasley</h3><p>魔术的Arron Afflalo已经被用了，换一个活塞头号种子Rodney Stuckey 537475636b6579。名字4d616c696b 426561736c6579。</p><div class="table-container"><table><thead><tr><th>/</th><th>KM405</th><th>78</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td>17ac70</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>22ec0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close  55 37</td><td>2 med  85 55</td><td>3 3pt  86 56</td><td>4 ft 85 55</td><td>5 layup  85 55</td><td>6 dunk  85 55</td><td>28 stdnk  50 23</td><td><strong>23  sit</strong> 72 48</td><td><strong>22  sod</strong>  81 51</td><td>25 hus  73 49</td></tr><tr><td><strong>7 hndl</strong>  74 4a</td><td><strong>8  pass</strong>  60 3c</td><td>32</td><td>32</td><td>9 opost  57 39</td><td>10 dpost  59 3b</td><td>11 block   50 22</td><td>26 hnd  78 4e</td><td>12 steal  59 3b</td><td>15 speed  77 4d</td><td>16 stam  96 60</td><td>19</td><td>21 vert 82 52</td><td>13 oreb  63 3f</td><td>14 dreb  63 3f</td><td>17 dur  85 55</td></tr><tr><td><strong>18 dawr</strong>  60 3c</td><td><strong>19  oawr</strong>  85 55</td><td>41</td><td>5a</td><td>27 def 77 4d</td><td>24 qui 98 62</td><td>潜力 78 4e</td><td>20 str 51 33</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td><td>/</td></tr></tbody></table></div><p>身高6‘4’=193.5，体重194，年龄27。号码5号，位置SG=01/05。面补90 05=1424。时间89-&gt;33.1min=132=84。肤色黑黑的02试试，原肤色01.</p><p>ps:6’5’=195.58</p><h3 id="旧球员-James-Johnson"><a href="#旧球员-James-Johnson" class="headerlink" title="旧球员 James Johnson"></a>旧球员 James Johnson</h3><p>在原国王队，注意要改成PF/05，03/05。号码，16=32=20号。原上场时间02，改为24min=96=60.面补49 06=1609。</p><p>名字被之前哪个collins改掉了。。。应该是🦅的Ivan Johnson 4a6f686e736f6e，给了John科林斯。。。17a120</p><div class="table-container"><table><thead><tr><th></th><th>KM最新</th><th>76</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td>17a120</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>18060</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close  82 52</td><td>2 med  60 3c</td><td>3 3pt 79 4f</td><td>4 ft 71 47</td><td>5 layup  86 56</td><td>6 dunk  80 50</td><td>28 stdnk  60 3c</td><td><strong>23  sit</strong> 76 4c</td><td><strong>22  sod</strong>  77 4d</td><td>25 hus  76 4c</td></tr><tr><td><strong>7 hndl</strong>  74 4a</td><td><strong>8  pass</strong>  70 46</td><td>32</td><td>32</td><td>9 opost  78 4e</td><td>10 dpost  75 4b</td><td>11 block   80 50</td><td>26 hnd  92 5c</td><td>12 steal  50 32</td><td>15 speed  69 45</td><td>16 stam  58 3a</td><td>19</td><td>21 vert 72 48</td><td>13 oreb  50 32</td><td>14 dreb  64 40</td><td>17 dur  90 5a</td></tr><tr><td><strong>18 dawr</strong>  78 4e</td><td><strong>19  oawr</strong>  78 4e</td><td>19</td><td>54</td><td>27 def 68 44</td><td>24 qui 84 54</td><td>潜力 75 4b</td><td>20 str 68 44</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td><td>/</td></tr></tbody></table></div><h3 id="旧球员-Evan-Turner"><a href="#旧球员-Evan-Turner" class="headerlink" title="旧球员 Evan Turner"></a>旧球员 Evan Turner</h3><p>这人好像不在球队了。。。哦还在，改一下吧</p><p>原76人队。注意位置SG/05，01/05。号码1号=02。时间由72-&gt;13min=52=34。面补0a 07=1802.</p><div class="table-container"><table><thead><tr><th>/</th><th>KM最新</th><th>73</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名地址</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>05460</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close  69 45</td><td>2 med  80 50</td><td>3 3pt 57 39</td><td>4 ft 71 47</td><td>5 layup  80 50</td><td>6 dunk  64 40</td><td>28 stdnk  50 28</td><td><strong>23  sit</strong> 68 44</td><td><strong>22  sod</strong>  75 4b</td><td>25 hus  74 4a</td></tr><tr><td><strong>7 hndl</strong>  77 4d</td><td><strong>8  pass</strong>  70 46</td><td>32</td><td>32</td><td>9 opost  67 43</td><td>10 dpost  55 37</td><td>11 block   62 3e</td><td>26 hnd  73 49</td><td>12 steal  60 3c</td><td>15 speed  76 4c</td><td>16 stam  95 5f</td><td>19</td><td>21 vert 78 4e</td><td>13 oreb  68 44</td><td>14 dreb  68 44</td><td>17 dur  85 55</td></tr><tr><td><strong>18 dawr</strong>  72 48</td><td><strong>19  oawr</strong>  72 48</td><td>32</td><td>50</td><td>27 def 71 47</td><td>24 qui 85 55</td><td>潜力 75 4b</td><td>20 str 69 45</td><td>？</td><td>？</td><td>？</td><td></td><td></td><td></td><td></td><td>/</td></tr></tbody></table></div><h3 id="Juancho-Hernangomez"><a href="#Juancho-Hernangomez" class="headerlink" title="Juancho Hernangomez"></a>Juancho Hernangomez</h3><p>真的没有合适长度的名字了，我选择用慈世平，first name的长度只能有5，怎么说？哈哈来个Juanc。4a75616e63 4865726e616e676f6d657a。</p><p>412数据包里这个面补是亚当斯</p><div class="table-container"><table><thead><tr><th>身高</th><th>体重</th><th>面补</th><th>位置</th><th>肤色</th><th>上场时间</th><th>年龄</th><th>号码</th><th></th></tr></thead><tbody><tr><td>6‘9’=206</td><td>214espn</td><td>3d 02=61+512=0573</td><td>PF</td><td>05</td><td>29.4-117.6-76</td><td>24</td><td>41-82-52</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th></th><th>KM405</th><th>74</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名</td><td>17a660</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>1a040</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close 3d</td><td>2 med  37</td><td>3 3pt  54</td><td>4 ft  4b</td><td>5 layup  4a</td><td>6 dunk  3e</td><td>28 stdnk  4e</td><td><strong>23  sit</strong>  47</td><td><strong>22  sod</strong>  37</td><td>25 hus  48</td></tr><tr><td><strong>7 hndl</strong>  37</td><td><strong>8  pass</strong>  39</td><td>32</td><td>49</td><td>9 opost  49</td><td>10 dpost  4a</td><td>11 block  4d</td><td>26 hnd  49</td><td>12 steal  32</td><td>15 speed  49</td><td>16 stam  51</td><td>63</td><td>21 vert  50</td><td>13 oreb  4e</td><td>14 dreb  4f</td><td>17 dur  32</td></tr><tr><td><strong>18 dawr</strong>  4d</td><td><strong>19  oawr</strong>  43</td><td>2d</td><td>3d</td><td>27 def  32</td><td>24 qui 33</td><td>潜力 4d</td><td>20 str  51</td><td>？</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h3 id="Josh-Okogie"><a href="#Josh-Okogie" class="headerlink" title="Josh Okogie"></a>Josh Okogie</h3><p>网队Donte Greene。</p><p>4a6f7368 4f6b6f676965.</p><div class="table-container"><table><thead><tr><th>身高</th><th>体重</th><th>面补</th><th>位置</th><th>肤色</th><th>上场时间</th><th>年龄</th><th>号码</th><th></th></tr></thead><tbody><tr><td>6‘4’=194</td><td>212</td><td>fd 05=1533</td><td>SF/SG</td><td>03</td><td>24.9-100-64</td><td></td><td>20-40-28</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th></th><th>KM405</th><th>75</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名</td><td>17aa40</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>1fa40</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close 42</td><td>2 med  45</td><td>3 3pt  3e</td><td>4 ft  46</td><td>5 layup  4b</td><td>6 dunk  35</td><td>28 stdnk  32</td><td><strong>23  sit</strong>  3f</td><td><strong>22  sod</strong>  4a</td><td>25 hus  3e</td></tr><tr><td><strong>7 hndl</strong>  4c</td><td><strong>8  pass</strong>  45</td><td>32</td><td>32</td><td>9 opost  4c</td><td>10 dpost  48</td><td>11 block  35</td><td>26 hnd  4e</td><td>12 steal  42</td><td>15 speed  61</td><td>16 stam  58</td><td>63</td><td>21 vert  50</td><td>13 oreb  34</td><td>14 dreb  3b</td><td>17 dur  55</td></tr><tr><td><strong>18 dawr</strong>  50</td><td><strong>19  oawr</strong>  50</td><td>28</td><td>4b</td><td>27 def  47</td><td>24 qui 5a</td><td>潜力3f</td><td>20 str 2e</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h3 id="Omari-Spellman"><a href="#Omari-Spellman" class="headerlink" title="Omari Spellman"></a>Omari Spellman</h3><p>C位没名字了，马刺的PF Tiago 546961676f Splitter。</p><p>4f6d617269 5370656c6c6d616e。</p><div class="table-container"><table><thead><tr><th>身高</th><th>体重</th><th>面补</th><th>位置</th><th>肤色</th><th>上场时间</th><th>年龄</th><th>号码</th><th></th></tr></thead><tbody><tr><td>6‘8’=205</td><td>245</td><td>47 07=1863</td><td>C</td><td>03</td><td>2019勇士18.1-72-48</td><td></td><td>14-28-1c</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th></th><th>KM405</th><th>76</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名</td><td>17b000</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>28140</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close 4b</td><td>2 med  4a</td><td>3 3pt  53</td><td>4 ft  51</td><td>5 layup  46</td><td>6 dunk  3c</td><td>28 stdnk  46</td><td><strong>23  sit</strong>  3e</td><td><strong>22  sod</strong>  23</td><td>25 hus  4f</td></tr><tr><td><strong>7 hndl</strong>  23</td><td><strong>8  pass</strong>  1d</td><td>32</td><td>32</td><td>9 opost  48</td><td>10 dpost  4b</td><td>11 block  40</td><td>26 hnd  4b</td><td>12 steal  3f</td><td>15 speed  38</td><td>16 stam  58</td><td>1e</td><td>21 vert  34</td><td>13 oreb  4b</td><td>14 dreb  4e</td><td>17 dur  4b</td></tr><tr><td><strong>18 dawr</strong>  4c</td><td><strong>19  oawr</strong>  4e</td><td>28</td><td>50</td><td>27 def  34</td><td>24 qui 32</td><td>潜力 50</td><td>20 str 37</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h3 id="Jake-Layman"><a href="#Jake-Layman" class="headerlink" title="Jake Layman"></a>Jake Layman</h3><p>可用的人不少，字数刚好的比较鸡，是黄蜂的Jeff Taylor 5461796c6f72，这样倾向应该会比较菜。没好的面补!!!</p><p>4a616b65 4c61796d616e.</p><div class="table-container"><table><thead><tr><th>身高</th><th>体重</th><th>面补</th><th>位置</th><th>肤色</th><th>上场时间</th><th>年龄</th><th>号码</th><th></th></tr></thead><tbody><tr><td>6‘8‘=205(.8就变成6‘9’了)</td><td>214</td><td>aa 09=2474</td><td>SF=02/05</td><td>04</td><td>22-88-58</td><td>？</td><td>10-20-14</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th></th><th>KM405</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名</td><td>1798b0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>7da0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close  44</td><td>2 med  4e</td><td>3 3pt  4c</td><td>4 ft  46</td><td>5 layup  40</td><td>6 dunk  3c</td><td>28 stdnk  4a</td><td><strong>23  sit</strong>  42</td><td><strong>22  sod</strong>  43</td><td>25 hus  4e</td></tr><tr><td><strong>7 hndl</strong>  44</td><td><strong>8  pass</strong>  41</td><td>32</td><td>32</td><td>9 opost  46</td><td>10 dpost  47</td><td>11 block  3e</td><td>26 hnd  38</td><td>12 steal  32</td><td>15 speed  46</td><td>16 stam  50</td><td>28</td><td>21 vert  52</td><td>13 oreb  30</td><td>14 dreb  32</td><td>17 dur  4b</td></tr><tr><td><strong>18 dawr</strong>  4d</td><td><strong>19  oawr</strong>  4c</td><td>23</td><td>50</td><td>27 def  40</td><td>24 qui 3e</td><td>潜力 4c</td><td>20 str 37</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h3 id="Jarrett-Culver"><a href="#Jarrett-Culver" class="headerlink" title="Jarrett Culver"></a>Jarrett Culver</h3><div class="table-container"><table><thead><tr><th>身高</th><th>体重</th><th>面补</th><th>位置</th><th>肤色</th><th>上场时间</th><th>年龄</th><th>号码</th><th></th></tr></thead><tbody><tr><td>=</td><td></td><td></td><td></td><td>0</td><td></td><td>？</td><td>-20-14</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th></th><th>KM405</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close</td><td>2 med  4e</td><td>3 3pt  4c</td><td>4 ft  46</td><td>5 layup  40</td><td>6 dunk  3c</td><td>28 stdnk  4a</td><td><strong>23  sit</strong>  42</td><td><strong>22  sod</strong>  43</td><td>25 hus  4e</td></tr><tr><td><strong>7 hndl</strong>  44</td><td><strong>8  pass</strong>  41</td><td>32</td><td>32</td><td>9 opost  46</td><td>10 dpost  47</td><td>11 block  3e</td><td>26 hnd  38</td><td>12 steal  32</td><td>15 speed  46</td><td>16 stam  50</td><td>28</td><td>21 vert  52</td><td>13 oreb  30</td><td>14 dreb  32</td><td>17 dur  4b</td></tr><tr><td><strong>18 dawr</strong>  4d</td><td><strong>19  oawr</strong>  4c</td><td>23</td><td>50</td><td>27 def  40</td><td>24 qui 3e</td><td>潜力 4c</td><td>20 str 37</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h3 id="Jordan-McLaughlin"><a href="#Jordan-McLaughlin" class="headerlink" title="Jordan McLaughlin"></a><strong>Jordan McLaughlin</strong></h3><p>改的不好，但是实在没名字了，用尼克斯的PF Amar’s 416d61722773 Stoudemire。</p><p>4a6f7264616e 4d634c617567686c696e。</p><p>身高180不行是5‘10’，180.5差不多</p><div class="table-container"><table><thead><tr><th>身高</th><th>体重</th><th>面补</th><th>位置</th><th>肤色</th><th>上场时间</th><th>年龄</th><th>号码</th><th></th></tr></thead><tbody><tr><td>5‘11‘=</td><td>170</td><td>8d 03=0909</td><td>PG=00/05</td><td>03</td><td>19.7-80-50</td><td>？</td><td>6-12-0c</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th></th><th>KM405</th><th>74</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>姓名</td><td>17a5a0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>数据地址</td><td>18d80</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>00</strong></td><td><strong>01</strong></td><td><strong>02</strong></td><td><strong>03</strong></td><td><strong>04</strong></td><td><strong>05</strong></td><td><strong>06</strong></td><td><strong>07</strong></td><td><strong>08</strong></td><td><strong>09</strong></td><td><strong>0a</strong></td><td><strong>0b</strong></td><td><strong>0c</strong></td><td><strong>0d</strong></td><td><strong>0e</strong></td><td><strong>0f</strong></td></tr><tr><td><strong>0</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>?</td><td>1 close   52</td><td>2 med  4a</td><td>3 3pt  50</td><td>4 ft  42</td><td>5 layup  51</td><td>6 dunk  32</td><td>28 stdnk  42</td><td><strong>23  sit</strong>  43</td><td><strong>22  sod</strong>  4b</td><td>25 hus  4d</td></tr><tr><td><strong>7 hndl</strong>  54</td><td><strong>8  pass</strong>  50</td><td>3f</td><td>35</td><td>9 opost  3f</td><td>10 dpost  44</td><td>11 block  32</td><td>26 hnd  3e</td><td>12 steal  49</td><td>15 speed  4a</td><td>16 stam  54</td><td>63</td><td>21 vert  4d</td><td>13 oreb  32</td><td>14 dreb  32</td><td>17 dur  46</td></tr><tr><td><strong>18 dawr</strong>  43</td><td><strong>19  oawr</strong>  4f</td><td>50</td><td>44</td><td>27 def  37</td><td>24 qui 3c</td><td>潜力 5f</td><td>20 str 32</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt; 此文仅整理记录安卓原版2K13非root情况下可用于修改DIY的球员名单😜。&lt;/p&gt;
&lt;p&gt; 之前的大名单整理内容实在是太多了，整理的时候很容易弄混，因此开始分解名单更新。&lt;/p&gt;
&lt;p&gt; 如有不当的地方，请联系我删除。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="NBA2K13" scheme="http://maxliu245.github.io/tags/NBA2K13/"/>
    
      <category term="DIY" scheme="http://maxliu245.github.io/tags/DIY/"/>
    
  </entry>
  
  <entry>
    <title>9 Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks 笔记</title>
    <link href="http://maxliu245.github.io/2020/06/12/9-Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks-%E7%AC%94%E8%AE%B0/"/>
    <id>http://maxliu245.github.io/2020/06/12/9-Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks-%E7%AC%94%E8%AE%B0/</id>
    <published>2020-06-11T23:37:25.000Z</published>
    <updated>2020-06-12T00:26:46.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>MAML文献回顾</p></blockquote><a id="more"></a><h2 id="文献简介"><a href="#文献简介" class="headerlink" title="文献简介"></a>文献简介</h2><img src="/2020/06/12/9-Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks-笔记/9.png" title="MAML"><p>其实寒假的时候就读过这篇文章，<strong>当时的理解</strong>主要是：搞了个新模型去弄原始模型的参数，新模型的目标是使原始模型Loss下一次梯度下降得更多，这样似乎面对新任务新数据时只要几次迭代Loss就能降不少，需要计算迭代的次数因此可能很少。现在希望<strong>回顾</strong>一下。</p><ul><li>文献地址：<a href="https://arxiv.org/abs/1703.03400或https://dl.acm.org/doi/10.5555/3305381.3305498" target="_blank" rel="noopener">https://arxiv.org/abs/1703.03400或https://dl.acm.org/doi/10.5555/3305381.3305498</a></li><li>文献投于ICML2017</li><li>Finn作品，另有参考BAIR的博客<a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/" target="_blank" rel="noopener">Learning to Learn</a></li><li>截至2020/06/12谷歌学术引用量1664</li></ul><h2 id="阅读笔记——文章主题整理"><a href="#阅读笔记——文章主题整理" class="headerlink" title="阅读笔记——文章主题整理"></a>阅读笔记——文章主题整理</h2><p>这篇文章提出的是一种经典的元学习方法，记为<code>MAML</code>。其卖点在于<code>模型不可知</code>，对于一般使用梯度下降的模型，都可以以<code>MAML</code>的方法使之能<strong>快速适应新任务</strong>。</p><p>MAML的思想来源是元学习，假定有一系列共同或相似内在联系的任务，那么希望机器能把任务之间的关联抽象为所谓的经验，辅助新任务的训练。但是这样一种<strong>general的经验</strong>不好确定啊，而且直观地说，此经验一定是要随着新任务变化而稍有变化的，才能使之合理地成为元学习器。</p><p>其思想就是找到这样的经验并用于加强新任务的<strong>泛化</strong>能力，很自然地，这个经验抽象为了模型的初始参数。在元学习的假设下，我们有很多之前的任务$T_i$（经验）和待解决的新任务，它们都可以从任务分布$p(T)$中采样出来。目标是新模型的快速适应能力，即只要很少的迭代次数就能达到较优的效果，那么，初始参数要使得损失的变化最大，只要微调，性能快速变化。</p><p>所以考虑模型$f<em>{\theta}$，其损失$L$。先采样一个过去的任务$T_i\sim p(T)$（也可以采样多个任务，这里以一个为例），在此任务上计算$L</em>{T<em>i}(f</em>{\theta})$，从中提取经验的信息，对初始参数$\theta$求导得$\nabla<em>{\theta}L</em>{T<em>i}(f</em>{\theta})$。为了使Loss对初始参数的变化更加敏感，对当前初始参数进行一步梯度更新得到$\theta’$。再在重新采样的元任务上确认效果，如何保证Loss对初始参数更敏感？$\theta’$要让经过一次迭代后元任务上的损失就达到最小$\Leftrightarrow$</p><script type="math/tex; mode=display">\min\limits_{\theta}\sum\limits_{T_i\sim p(T)}L_{T_i}(f_{\theta'})=\min\limits_{\theta}\sum\limits_{T_i\sim p(T)}L_{T_i}(f_{\theta-\alpha\nabla_{\theta}L_{T_i}(f_{\theta})})</script><p>骚操作，实验不看了，有空研究下代码就更懂了🤭</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;MAML文献回顾&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="MAML" scheme="http://maxliu245.github.io/tags/MAML/"/>
    
      <category term="Meta Learning" scheme="http://maxliu245.github.io/tags/Meta-Learning/"/>
    
  </entry>
  
  <entry>
    <title>8 A Generic First-Order Algorithmic Framework for Bi-Level Programming Beyond Lower-Level Singleton 笔记</title>
    <link href="http://maxliu245.github.io/2020/06/10/8-A-Generic-First-Order-Algorithmic-Framework-for-Bi-Level-Programming-Beyond-Lower-Level-Singleton-%E7%AC%94%E8%AE%B0/"/>
    <id>http://maxliu245.github.io/2020/06/10/8-A-Generic-First-Order-Algorithmic-Framework-for-Bi-Level-Programming-Beyond-Lower-Level-Singleton-%E7%AC%94%E8%AE%B0/</id>
    <published>2020-06-10T07:31:17.000Z</published>
    <updated>2020-06-10T08:59:10.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>文献阅读笔记</p></blockquote><a id="more"></a><h2 id="文献简介"><a href="#文献简介" class="headerlink" title="文献简介"></a>文献简介</h2><img src="/2020/06/10/8-A-Generic-First-Order-Algorithmic-Framework-for-Bi-Level-Programming-Beyond-Lower-Level-Singleton-笔记/8.png" title="A Generic First-Order Algorithmic Framework for Bi-Level Programming Beyond Lower-Level Singleton"><ul><li>文献地址：<a href="https://arxiv.org/abs/2006.04045" target="_blank" rel="noopener">https://arxiv.org/abs/2006.04045</a></li><li>文献发布于2020年6月</li></ul><blockquote><p>ps：发现06/10的版本有两处书写错误：</p><ul><li>(9)式上面有个”We”写成了”W e”</li><li>5.2节最后一段方法名字写错了…”BAD”</li></ul></blockquote><h2 id="阅读笔记——文章主题整理"><a href="#阅读笔记——文章主题整理" class="headerlink" title="阅读笔记——文章主题整理"></a>阅读笔记——文章主题整理</h2><p>这篇文章研究的对象是<strong>双边优化</strong><code>Bi-Level</code>问题，记为<code>BLP</code>。该类问题虽然早就被提出，但最近一段时间，各种深度学习的应用、元学习中都开始使用它作为目标函数。它的一般形式如下：</p><blockquote><p> ps：我喜欢的Meta-Weight-Net的目标也属于此类问题</p></blockquote><script type="math/tex; mode=display">\min_\limits{x\in \mathcal{X}} F(x,y),\ s.t.\ y\in\mathcal{S}(x),\ where\ \mathcal{S}(x):=\arg\min_\limits{y}f(x,y).</script><p>传统的方法是交替迭代优化的方法，但是它们能针对的是上述双边优化问题中的一小类，即条件中的$y$是单一解，而一般双边优化中$y$可以有多解。当底层优化条件退化为单一解时，这个条件<strong>定义为<code>LLS</code></strong>(Lower-Level Singleton)。那么有LLS条件的双边优化问题形式为：</p><blockquote><p>ps：查阅百度、谷歌、wiki均未查到此LLS，应该是本文先这样称谓</p></blockquote><script type="math/tex; mode=display">\min_\limits{x\in \mathcal{X}} F(x,y),\ s.t.\ y=\arg\min_\limits{y}f(x,y).</script><p>定义名称，高层优化问题为upper level，底层为lower level，分别记为<strong>UL问题</strong>和<strong>LL子问题</strong>。那么传统方法不细说，就是我了解的那种交替迭代，放张Meta-Weight-Net的图自行体会吧🤭：</p><img src="/2020/06/10/8-A-Generic-First-Order-Algorithmic-Framework-for-Bi-Level-Programming-Beyond-Lower-Level-Singleton-笔记/MWN.png" title="体会交替优化方法"><p>那么传统方法不行啊，因为一般LLS条件是<strong>不满足</strong>的（文中举了反例并实验验证），研究一般的双边优化问题<strong>更有意义</strong>。传统方法不好的原因是UL和LL问题中的变量是相互依存的，直接梯度方法交替迭代不能考虑这种联系，因此理论上不能完全保证收敛正确，一旦不满足LLS条件就会螺旋升天。</p><p>基于此提出<strong><code>BDA</code></strong>方法，即Bi-level Descent Aggregation，基本思想就是集合UL和LL问题中变量的联系，力图解决一般的BLP问题。BDA方法的理论我还不完全明白，因为其中包含了很多优化领域的名词、条件，这个暂时没时间一一理解；索性思想是可以明白的，因此大致介绍BDA方法如下：</p><ul><li>一般的BLP问题可以写成<code>single-level model</code>：</li></ul><script type="math/tex; mode=display">\min_\limits{x\in \mathcal{X}} \varphi(x),\ with\ \varphi(x):=\inf_\limits{y\in \mathcal{S}(x)}F(x,y).</script><ul><li>那么当有数据观测$x$（固定$x$）时，上述问题简化为：</li></ul><script type="math/tex; mode=display">\min_\limits{y} F(x,y),\ s.t.\ y\in \mathcal{S}(x).</script><ul><li>那么LL子问题中$y$的更新要同时考虑$x$与$y$的联系：</li></ul><script type="math/tex; mode=display">y_{k+1}(x)=\tau_k (x,y_k(x)),\ k=0,\cdots,K-1.</script><p>  其中$\tau_k (x,\cdot)$是更新策略，可以自行调整，且需要初始化$y_0=\tau_0(x)$。这个更新策略要求考虑$x$与$y$的联系！</p><ul><li>BLP问题的UL更新转化为：</li></ul><script type="math/tex; mode=display">\min_\limits{x\in\mathcal{X}} \varphi_K(x):=F(x,y_K(x)).</script><ul><li><p>其实就是这么个小东西$\tau$需要很多条件支撑，证明内容也很多</p><blockquote><p>ps：文中表示这些条件不像LLS难以满足，这些条件都是正常的一般条件，易满足</p></blockquote></li></ul><p>最终的反例，数据标签重学，元学习的实验结果表示，这个BDA方法很棒：</p><ul><li>比对比用的RHG(Reverse Hyper-Gradient)方法普遍效果更好，也很稳定</li><li>关键是不受LLS条件约束，不会收敛到虚假的极小</li><li>在LL子问题的序列长度$K$增大时效果更强</li></ul><h2 id="阅读的不足之处："><a href="#阅读的不足之处：" class="headerlink" title="阅读的不足之处："></a>阅读的不足之处：</h2><p>这次笔记没有直接翻译，读了之后按自己的理解捋了一遍。但是其实和翻译差不到哪去…有个问题是只知道BDA方法的改进思想是结合UL和LL问题中变量的相关信息，但不知道理论上究竟为何$\tau$就能成功</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文献阅读笔记&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="Optimization" scheme="http://maxliu245.github.io/tags/Optimization/"/>
    
      <category term="Bi-Level" scheme="http://maxliu245.github.io/tags/Bi-Level/"/>
    
  </entry>
  
  <entry>
    <title>7 Variational Inference based on Robust Divergences</title>
    <link href="http://maxliu245.github.io/2020/06/05/%C2%967-Variational-Inference-based-on-Robust-Divergences/"/>
    <id>http://maxliu245.github.io/2020/06/05/%C2%967-Variational-Inference-based-on-Robust-Divergences/</id>
    <published>2020-06-05T07:06:08.000Z</published>
    <updated>2020-06-05T12:45:30.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>文献阅读记录，这篇稍微读全一点</p></blockquote><a id="more"></a><h2 id="文献简介"><a href="#文献简介" class="headerlink" title="文献简介"></a>文献简介</h2><img src="/2020/06/05/7-Variational-Inference-based-on-Robust-Divergences/7.png" title="Variational Inference based on Robust Divergences"><ul><li>无附录文献地址：<a href="http://proceedings.mlr.press/v84/futami18a/futami18a.pdf" target="_blank" rel="noopener">http://proceedings.mlr.press/v84/futami18a/futami18a.pdf</a></li><li>有附录文献地址：<a href="https://arxiv.org/abs/1710.06595" target="_blank" rel="noopener">https://arxiv.org/abs/1710.06595</a></li><li>文献发布于2017年</li></ul><h2 id="摘要-Intro-结论"><a href="#摘要-Intro-结论" class="headerlink" title="摘要+Intro+结论"></a>摘要+Intro+结论</h2><p>本文研究过程：</p><ul><li>背景：稳健性在机器学习中十分重要</li><li>传统标准方法0：基于模型，使用重尾分布，缺点是只能用于少数简单模型</li><li>新方法1：Zellner的贝叶斯变分推断，用KL散度，是我熟知的那个</li><li>新方法2：基于方法1，使用$\beta$散度，导出伪贝叶斯变分推断，更稳健，但只针对了简单高斯分布做数学证明</li><li>本文方法3：基于方法1，使用稳健的$\beta$或者$\gamma$散度，导出伪贝叶斯变分推断。优点如下：<ul><li>针对了更复杂的模型，证明了对ReLu激活的DNN的稳健性。对比了只做简单模型验证的新方法2</li><li>完成IF分析，在ReLu激活的DNN上对比了方法1的无界IF，因此对特征和标签扰动都是稳健的；第二个对比是我毕设主文献，其IF分析是渐进有界的，此方法则有限样本必有界</li></ul></li><li>未来工作：拓展到更多复杂模型、与其它推断估计方法结合</li></ul><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><ul><li><p>MLE与其稳健变式：</p><ul><li>原始MLE很简单，（2）可以用狄拉克函数推导出来</li><li>稳健变式即散度的变化，$\beta$和$\gamma$散度都可以退化到KL散度，（8）式有点恶心，但是自己带入就推出来了（已推）</li></ul></li><li><p>贝叶斯变分推断：</p><p>Emmm，真的就是介绍下变分推断…其中（12）式没看明白，应该问题不大…</p></li></ul><h2 id="本文方法——基于稳健散度的稳健推断"><a href="#本文方法——基于稳健散度的稳健推断" class="headerlink" title="本文方法——基于稳健散度的稳健推断"></a>本文方法——基于稳健散度的稳健推断</h2><p>总的方法就是在正常变分推断的基础上，把式（14）中的第一项期望中的KL散度项替换为稳健散度。</p><p>以$\beta$散度为例，举了例子，近似的参数后验分布有（18）式给出，具体怎么算的话…积分鬼才，本文表示用好用的分布族+重参数化采样近似去计算。</p><p>注：其中有个<strong>不对劲</strong>的地方，$\beta$交叉熵，我查阅了原始论文<a href="https://www.duo.uio.no/bitstream/handle/10852/47786/1997-7.pdf" target="_blank" rel="noopener"><em>Robust and efficient estimation by minimising a density power divergence</em></a>中的（4.1）式，本文好像少了一个常数项，虽然求导不影响啦~</p><h2 id="IF分析"><a href="#IF分析" class="headerlink" title="IF分析"></a>IF分析</h2><p>本文的IF定义介绍我不是太明白，但是意思我懂了，即数据偏差导致的变化。接下来的定理我也不懂，反正都是给好的结果。接下来的分析我基本上都明白了，<strong>结果也看懂了</strong>，即IF分析表明稳健散度在DNN（包括分类回归）上对特征和标签扰动导致的IF都是有界的即稳健起来了。除了<strong>（26）式又不懂了</strong>，</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验很正常，数据特征和标签的两种打乱方式，按照所提稳健散度引出的稳健VI，分别在人造数据和真实数据上进行了实验。</p><ul><li><p>人造数据简单验证了稳健性</p></li><li><p>用UCI数据打乱的程度与测试对数似然的关系验证了稳健性</p></li><li>用交叉验证的方法获取较优的稳健散度的参数，并以优异结果验证了稳健性</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文献阅读记录，这篇稍微读全一点&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Paper Reading" scheme="http://maxliu245.github.io/tags/Paper-Reading/"/>
    
      <category term="Robust" scheme="http://maxliu245.github.io/tags/Robust/"/>
    
      <category term="KL Divergence" scheme="http://maxliu245.github.io/tags/KL-Divergence/"/>
    
      <category term="Bayesian" scheme="http://maxliu245.github.io/tags/Bayesian/"/>
    
  </entry>
  
</feed>
